{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FLo2FDAREB_J"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 3.3 Thinking with Deep Learning: Week 3 Part 3\n",
        "# Introduction to Deep Learning for Causal Inference on Observables\n",
        "\n",
        "__Instructor:__ James Evans\n",
        "\n",
        "__Notebook Author:__ Bernard Koch (https://github.com/kochbj/Deep-Learning-for-Causal-Inference)\n",
        "\n",
        "__Notebook Editor & Teaching Assistants:__ Shiyang Lai, Avi\n"
      ],
      "metadata": {
        "id": "uU6GQAPU-ZzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double/Debiased Machine Learning (DML)\n",
        "\n",
        "Double/Debiased Machine Learning (DML) is a powerful method for causal inference that has gained significant attention in recent years. The method is proposed by Chernozhukov etc. in 2018 ([link](https://academic.oup.com/ectj/article/21/1/C1/5056401?login=true)). In this practical guide, we will briefly explore what DML is, how it works, and implement it using the [DoubleML](https://docs.doubleml.org/stable/index.html#doubleml-package) package.\n",
        "\n",
        "<font color='red'><p>Please select A100 device for this colab exercise.<p></font>"
      ],
      "metadata": {
        "id": "BWgiVMAGHHAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn==1.5.0 --quiet\n",
        "!pip install --upgrade scikeras==0.13.0 --quiet\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
        "\n",
        "# Install DoubleML package if not already installed\n",
        "try:\n",
        "    import doubleml\n",
        "except ImportError:\n",
        "    !pip install doubleml --quiet\n",
        "    import doubleml\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import necessary libraries\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Build LightGBM with GPU support\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import doubleml\n",
        "from doubleml import DoubleMLData, DoubleMLPLR\n",
        "from doubleml.datasets import make_plr_CCDDHNR2018\n",
        "\n",
        "# Load tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Set plotting styles\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "face_colors = sns.color_palette('pastel')\n",
        "edge_colors = sns.color_palette('dark')"
      ],
      "metadata": {
        "id": "Aqb8midjSaOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tensorflow:\", tf.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"doubleml:\", doubleml.__version__)\n",
        "print(\"scikeras:\", scikeras.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "metadata": {
        "id": "BhyK-OoLjphA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DML Basics\n",
        "### Data generating process\n",
        "\n",
        "To understand the spirit of DML, we can first think through a data-generating process (DGP) such that:\n",
        "\n",
        "$$\n",
        "y_i = \\theta_0 d_i + g_0(x_i) + \\zeta_i, \\quad \\zeta_i \\sim N(0, 1),\n",
        "$$\n",
        "\n",
        "$$\n",
        "d_i = m_0(x_i) + v_i, \\quad v_i \\sim N(0, 1),\n",
        "$$\n",
        "\n",
        "with covariates $x_i \\sim N(0, \\Sigma)$, where $\\Sigma$ is a matrix with entries $\\Sigma_{kj} = 0.7^{|j-k|}$. We are interested in performing valid inference on the causal parameter $\\theta_0$. The true parameter $\\theta_0$ is set to 0.5 in our simulation experiment.\n",
        "\n",
        "Here, the nuisance functions are given by\n",
        "\n",
        "$$\n",
        "m_{0}(x_{i})=x_{i,1}+\\frac{1}{4} \\frac{exp\\left( x_{i,3} \\right)}{1+exp\\left( x_{i,3} \\right)},\n",
        "$$\n",
        "\n",
        "$$\n",
        "g_{0}(x_{i})=\\frac{exp\\left( x_{i,1} \\right)}{1+exp(x_{i,1})} +\\frac{1}{4} x_{i,3}.\n",
        "$$\n",
        "\n",
        "We generate `n_rep` replications of the data generating process with sample size `n_obs` and compare the performance of different estimators."
      ],
      "metadata": {
        "id": "XgkLFSr5M1XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(60615)\n",
        "n_rep = 1000\n",
        "n_obs = 500\n",
        "n_vars = 5\n",
        "alpha = 0.5\n",
        "\n",
        "data = list()\n",
        "\n",
        "# In Python the data can be generated with doubleml.datasets.make_plr_CCDDHNR2018().\n",
        "for i_rep in range(n_rep):\n",
        "    (x, y, d) = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars, return_type='array')\n",
        "    data.append((x, y, d))"
      ],
      "metadata": {
        "id": "oEbc1eg-HGLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization Bias in Simple ML-Approaches\n",
        "\n",
        "Naive inference that is based on a direct application of machine learning methods to estimate the causal parameter $θ_0$, is generally invalid. The use of machine learning methods introduces a bias that arises due to regularization. A simple ML approach is given by randomly splitting the sample into two parts. On the auxiliary sample indexed by $i ∈ I^C$ the nuisance function $g_0(X)$ is estimated with an ML method, for example a decision tree or a MLP learner. Given the estimate $\\hat{g_0}(X)$, the final estimate of $θ_0$\n",
        " is obtained as ($n=N/2$) using the other half of observations indexed with $i \\in I$\n",
        "\n",
        " $$\n",
        " \\hat{\\theta_{0}} =\\left( \\frac{1}{n} \\sum_{i\\in I} D_{i}^{2} \\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} D_{i}\\left( Y_{i}-\\hat{g_{0}} \\left( X_{i} \\right) \\right).\n",
        " $$\n",
        "\n",
        "As this corresponds to a “non-orthogonal” score, which is not implemented in the DoubleML package, we need to define a custom callable."
      ],
      "metadata": {
        "id": "KPHVQxFNTfVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def non_orth_score(y, d, l_hat, m_hat, g_hat, smpls):\n",
        "    u_hat = y - g_hat\n",
        "    psi_a = -np.multiply(d, d)\n",
        "    psi_b = np.multiply(d, u_hat)\n",
        "    return psi_a, psi_b"
      ],
      "metadata": {
        "id": "ZBUjsWk6bfnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remark that the estimator is not able to estimate $\\hat{g_0}(X)$ directly, but has to be based on a preliminary estimate of $\\hat{m_0}(X)$. All following estimators with `score=\"IV-type\"` are based on the same preliminary procedure. Furthermore, remark that we are using external predictions to avoid cross-fitting (for demonstration purposes)."
      ],
      "metadata": {
        "id": "fd3xRxYZbhsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree model (~10min)\n",
        "np.random.seed(60615)\n",
        "\n",
        "ml_l = LGBMRegressor(n_estimators=300, learning_rate=0.1, verbose=-1)\n",
        "ml_m = LGBMRegressor(n_estimators=300, learning_rate=0.1, verbose=-1)\n",
        "\n",
        "ml_g = clone(ml_l)\n",
        "\n",
        "theta_nonorth_dt = np.full(n_rep, np.nan)\n",
        "se_nonorth_dt = np.full(n_rep, np.nan)\n",
        "\n",
        "for i_rep in tqdm(range(n_rep), total=n_rep):\n",
        "    print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
        "    (x, y, d) = data[i_rep]\n",
        "\n",
        "    # choose a random sample for training and estimation\n",
        "    i_train, i_est = train_test_split(np.arange(n_obs), test_size=0.5, random_state=42)\n",
        "\n",
        "    # fit the ML algorithms on the training sample\n",
        "    ml_l.fit(x[i_train, :], y[i_train])\n",
        "    ml_m.fit(x[i_train, :], d[i_train])\n",
        "\n",
        "    psi_a = -np.multiply(d[i_train] - ml_m.predict(x[i_train, :]), d[i_train] - ml_m.predict(x[i_train, :]))\n",
        "    psi_b = np.multiply(d[i_train] - ml_m.predict(x[i_train, :]), y[i_train] - ml_l.predict(x[i_train, :]))\n",
        "    theta_initial = -np.nanmean(psi_b) / np.nanmean(psi_a)\n",
        "    ml_g.fit(x[i_train, :], y[i_train] - theta_initial * d[i_train])\n",
        "\n",
        "    # create out-of-sample predictions\n",
        "    l_hat = ml_l.predict(x[i_est, :])\n",
        "    m_hat = ml_m.predict(x[i_est, :])\n",
        "    g_hat = ml_g.predict(x[i_est, :])\n",
        "\n",
        "    external_predictions = {\n",
        "        'd': {\n",
        "            'ml_l': l_hat.reshape(-1, 1),\n",
        "            'ml_m': m_hat.reshape(-1, 1),\n",
        "            'ml_g': g_hat.reshape(-1, 1)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    obj_dml_data = DoubleMLData.from_arrays(x[i_est, :], y[i_est], d[i_est])\n",
        "    obj_dml_plr_nonorth = DoubleMLPLR(obj_dml_data,\n",
        "                                    ml_l, ml_m, ml_g,\n",
        "                                    n_folds=2,\n",
        "                                    score=non_orth_score)\n",
        "    obj_dml_plr_nonorth.fit(external_predictions=external_predictions)\n",
        "    theta_nonorth_dt[i_rep] = obj_dml_plr_nonorth.coef[0]\n",
        "    se_nonorth_dt[i_rep] = obj_dml_plr_nonorth.se[0]\n",
        "\n",
        "fig_non_orth, ax = plt.subplots(constrained_layout=True);\n",
        "ax = sns.histplot((theta_nonorth_dt - alpha)/se_nonorth_dt,\n",
        "                color=face_colors[0], edgecolor = edge_colors[0],\n",
        "                stat='density', bins=30, label='Non-orthogonal DT');\n",
        "ax.axvline(0., color='k');\n",
        "xx = np.arange(-5, +5, 0.001)\n",
        "yy = stats.norm.pdf(xx)\n",
        "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
        "ax.set_xlim([-6., 6.]);\n",
        "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zWG-cCpdcPnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement the same process but change the model to a one-layer multi-layer perceptron. This is intended to demonstrate that DL models also suffer from the same biases as traditional ML algorithms. You can simply re-run the two code chunks below yourself. Note that we shrink the `n_rep` significantly to 1/5, as it is extremely time-consuming."
      ],
      "metadata": {
        "id": "uKtpxDgHXg5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure GPU is configured\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    for gpu in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(gpu, False)\n",
        "    print(\"GPU is available and configured.\")\n",
        "else:\n",
        "    print(\"Using CPU.\")\n",
        "\n",
        "def create_mlp_model(\n",
        "    n_features,\n",
        "    n_hidden=32,\n",
        "    learning_rate=0.01\n",
        "):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_hidden, activation='relu', input_shape=(n_features,)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        metrics=[]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Early stopping to limit unnecessary epochs once validation loss plateaus\n",
        "early_stop = EarlyStopping(patience=2, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "0E4uQC0iXoKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: single-hidden-layer MLP (~30min)\n",
        "# Build KerasRegressor pipelines using SciKeras + StandardScaler\n",
        "ml_l_mlp = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"regressor\", KerasRegressor(\n",
        "        model=create_mlp_model,\n",
        "        n_features=x.shape[1],\n",
        "        n_hidden=32,            # single hidden layer of size 32\n",
        "        learning_rate=0.01,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    ))\n",
        "])\n",
        "\n",
        "ml_m_mlp = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"regressor\", KerasRegressor(\n",
        "        model=create_mlp_model,\n",
        "        n_features=x.shape[1],\n",
        "        n_hidden=32,\n",
        "        learning_rate=0.01,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    ))\n",
        "])\n",
        "\n",
        "ml_g_mlp = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"regressor\", KerasRegressor(\n",
        "        model=create_mlp_model,\n",
        "        n_features=x.shape[1],\n",
        "        n_hidden=32,\n",
        "        learning_rate=0.01,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    ))\n",
        "])\n",
        "\n",
        "small_n_rep = int(n_rep / 5)\n",
        "theta_nonorth_mlp = np.full(small_n_rep, np.nan)\n",
        "se_nonorth_mlp    = np.full(small_n_rep, np.nan)\n",
        "\n",
        "with tf.device('/GPU:0'):  # Ensure GPU usage in Colab\n",
        "    for i_rep in tqdm(range(small_n_rep), total=small_n_rep):\n",
        "        (x_i, y_i, d_i) = data[i_rep]\n",
        "\n",
        "        i_train, i_est = train_test_split(\n",
        "            np.arange(n_obs),\n",
        "            test_size=0.5,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Fit ml_l and ml_m on the training sample\n",
        "        ml_l_mlp.fit(x_i[i_train, :], y_i[i_train])\n",
        "        ml_m_mlp.fit(x_i[i_train, :], d_i[i_train])\n",
        "\n",
        "        # Compute initial guess for partial out\n",
        "        d_hat_train = ml_m_mlp.predict(x_i[i_train, :])\n",
        "        y_hat_train = ml_l_mlp.predict(x_i[i_train, :])\n",
        "\n",
        "        psi_a = -(d_i[i_train] - d_hat_train) * (d_i[i_train] - d_hat_train)\n",
        "        psi_b =  (d_i[i_train] - d_hat_train) * (y_i[i_train] - y_hat_train)\n",
        "        theta_initial = -np.nanmean(psi_b) / np.nanmean(psi_a)\n",
        "\n",
        "        # Fit ml_g on (y_i - theta_initial * d_i)\n",
        "        residual_for_g = y_i[i_train] - theta_initial * d_i[i_train]\n",
        "        ml_g_mlp.fit(x_i[i_train, :], residual_for_g)\n",
        "\n",
        "        # Out-of-sample predictions\n",
        "        l_hat = ml_l_mlp.predict(x_i[i_est, :])\n",
        "        m_hat = ml_m_mlp.predict(x_i[i_est, :])\n",
        "        g_hat = ml_g_mlp.predict(x_i[i_est, :])\n",
        "\n",
        "        # Provide these to DoubleML\n",
        "        external_predictions = {\n",
        "            'd': {\n",
        "                'ml_l': l_hat.reshape(-1, 1),\n",
        "                'ml_m': m_hat.reshape(-1, 1),\n",
        "                'ml_g': g_hat.reshape(-1, 1)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # DoubleML data & fit\n",
        "        obj_dml_data = DoubleMLData.from_arrays(\n",
        "            x_i[i_est, :], y_i[i_est], d_i[i_est]\n",
        "        )\n",
        "        obj_dml_plr_nonorth = DoubleMLPLR(\n",
        "            obj_dml_data,\n",
        "            ml_l_mlp,\n",
        "            ml_m_mlp,\n",
        "            ml_g_mlp,\n",
        "            n_folds=2,\n",
        "            score=non_orth_score\n",
        "        )\n",
        "        obj_dml_plr_nonorth.fit(external_predictions=external_predictions)\n",
        "\n",
        "        theta_nonorth_mlp[i_rep] = obj_dml_plr_nonorth.coef[0]\n",
        "        se_nonorth_mlp[i_rep]    = obj_dml_plr_nonorth.se[0]\n",
        "\n",
        "# Visualization\n",
        "fig_non_orth_mlp, ax = plt.subplots(constrained_layout=True)\n",
        "ax = sns.histplot(\n",
        "    (theta_nonorth_mlp - alpha) / se_nonorth_mlp,\n",
        "    color = face_colors[1], edgecolor = edge_colors[1],\n",
        "    stat='density',\n",
        "    bins=30,\n",
        "    label='Non-orthogonal MLP'\n",
        ")\n",
        "ax.axvline(0., color='k')\n",
        "xx = np.arange(-5, +5, 0.001)\n",
        "yy = stats.norm.pdf(xx)\n",
        "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$')\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
        "ax.set_xlim([-6., 6.])\n",
        "ax.set_xlabel('$(\\\\hat{\\\\theta}_0 - \\\\theta_0)/\\\\hat{\\\\sigma}$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dDuIiMzWhMkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization bias in the simple ML-approach is caused by the slow convergence of $\\hat{\\theta}_0$\n",
        "\n",
        "$$\n",
        "|\\sqrt{n} (\\hat{\\theta_0} - \\theta_0) | \\rightarrow_{P} \\infty\n",
        "$$\n",
        "\n",
        "i.e., slower than $1/\\sqrt{n}$.\n",
        "The driving factor is the bias that arises by learning $g$ with a random forest or any other ML technique.\n",
        "A heuristic illustration is given by\n",
        "\n",
        "$$\n",
        "\\sqrt{n}(\\hat{\\theta_0} - \\theta_0) = \\underbrace{\\left(\\frac{1}{n} \\sum_{i\\in I} D_i^2\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} D_i \\zeta_i}_{=:a}\n",
        "+  \\underbrace{\\left(\\frac{1}{n} \\sum_{i\\in I} D_i^2\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} D_i (g_0(X_i) - \\hat{g_0}(X_i))}_{=:b}.\n",
        "$$\n",
        "\n",
        "$a$ is approximately Gaussian under mild conditions.\n",
        "However, $b$ (the regularization bias) diverges in general."
      ],
      "metadata": {
        "id": "3qT8rQzgm_CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overcoming regularization bias by orthogonalization\n",
        "\n",
        "To overcome the regularization bias we can partial out the effect of $X$ from $D$ to obtain the orthogonalized regressor $V = D - m(X)$. This can be achieved by setting `score=\"IV-type\"`. We then use the final estimate\n",
        "\n",
        "$$\n",
        "\\check{\\theta_0} = \\left(\\frac{1}{n} \\sum_{i\\in I} \\hat{V_i} D_i\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} \\hat{V_i} (Y_i - \\hat{g_0}(X_i)).\n",
        "$$\n",
        "\n",
        "The following figure shows the distribution of the resulting estimates $\\hat{\\theta}_0$ without sample-splitting. Again, we are using external predictions to avoid cross-fitting (for demonstration purposes)."
      ],
      "metadata": {
        "id": "o147-tWrnLcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(60615)\n",
        "\n",
        "theta_orth_nosplit_dt = np.full(n_rep, np.nan)\n",
        "se_orth_nosplit_dt = np.full(n_rep, np.nan)\n",
        "\n",
        "for i_rep in tqdm(range(n_rep), total=n_rep):\n",
        "    print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
        "    (x, y, d) = data[i_rep]\n",
        "\n",
        "    # fit the ML algorithms on the training sample\n",
        "    ml_l.fit(x, y)\n",
        "    ml_m.fit(x, d)\n",
        "\n",
        "    psi_a = -np.multiply(d - ml_m.predict(x), d - ml_m.predict(x))\n",
        "    psi_b = np.multiply(d - ml_m.predict(x), y - ml_l.predict(x))\n",
        "    theta_initial = -np.nanmean(psi_b) / np.nanmean(psi_a)\n",
        "    ml_g.fit(x, y - theta_initial * d)\n",
        "\n",
        "    l_hat = ml_l.predict(x)\n",
        "    m_hat = ml_m.predict(x)\n",
        "    g_hat = ml_g.predict(x)\n",
        "\n",
        "    external_predictions = {\n",
        "        'd': {\n",
        "            'ml_l': l_hat.reshape(-1, 1),\n",
        "            'ml_m': m_hat.reshape(-1, 1),\n",
        "            'ml_g': g_hat.reshape(-1, 1)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    obj_dml_data = DoubleMLData.from_arrays(x, y, d)\n",
        "\n",
        "    obj_dml_plr_orth_nosplit = DoubleMLPLR(obj_dml_data,\n",
        "                                        ml_l, ml_m, ml_g,\n",
        "                                        score='IV-type')\n",
        "    obj_dml_plr_orth_nosplit.fit(external_predictions=external_predictions)\n",
        "    theta_orth_nosplit_dt[i_rep] = obj_dml_plr_orth_nosplit.coef[0]\n",
        "    se_orth_nosplit_dt[i_rep] = obj_dml_plr_orth_nosplit.se[0]\n",
        "\n",
        "fig_orth_nosplit, ax = plt.subplots(constrained_layout=True);\n",
        "ax = sns.histplot((theta_orth_nosplit_dt - alpha)/se_orth_nosplit_dt,\n",
        "                color=face_colors[2], edgecolor = edge_colors[2],\n",
        "                stat='density', bins=30, label='Double ML (DT, no sample splitting)');\n",
        "ax.axvline(0., color='k');\n",
        "xx = np.arange(-5, +5, 0.001)\n",
        "yy = stats.norm.pdf(xx)\n",
        "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
        "ax.set_xlim([-6., 6.]);\n",
        "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0loD6E_KnVB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't do deep learning model for this case because the story is the same. If the nuisance models $\\hat{g_0}()$ and $\\hat{m}()$ are estimated on the whole dataset, which is also used for obtaining the final estimate $\\check{\\theta_0}$, another bias is observed."
      ],
      "metadata": {
        "id": "c2LZgioz_9VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample splitting to remove bias induced by overfitting\n",
        "\n",
        "Using sample splitting, i.e., estimate the nuisance models $\\hat{g_0}()$ and $\\hat{m}()$ on one part of the data (training data) and estimate $\\check{\\theta}_0$ on the other part of the data (test data), overcomes the bias induced by overfitting. We can exploit the benefits of cross-fitting by switching the role of the training and test sample. Cross-fitting performs well empirically because the entire sample can be used for estimation.\n",
        "\n",
        "The following figure shows the distribution of the resulting estimates $\\hat{\\theta_0}$ with orthogonal score and sample-splitting."
      ],
      "metadata": {
        "id": "582S9kJJ_2GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(60615)\n",
        "\n",
        "theta_dml_dt = np.full(n_rep, np.nan)\n",
        "se_dml_dt = np.full(n_rep, np.nan)\n",
        "\n",
        "for i_rep in tqdm(range(n_rep), total=n_rep):\n",
        "    print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
        "    (x, y, d) = data[i_rep]\n",
        "    obj_dml_data = DoubleMLData.from_arrays(x, y, d)\n",
        "    obj_dml_plr = DoubleMLPLR(obj_dml_data,\n",
        "                            ml_l, ml_m, ml_g,\n",
        "                            n_folds=2,\n",
        "                            score='IV-type')\n",
        "    obj_dml_plr.fit()\n",
        "    theta_dml_dt[i_rep] = obj_dml_plr.coef[0]\n",
        "    se_dml_dt[i_rep] = obj_dml_plr.se[0]\n",
        "\n",
        "fig_dml, ax = plt.subplots(constrained_layout=True);\n",
        "ax = sns.histplot((theta_dml_dt - alpha)/se_dml_dt,\n",
        "                color=face_colors[3], edgecolor = edge_colors[3],\n",
        "                stat='density', bins=30, label='Double ML (DT) with cross-fitting');\n",
        "ax.axvline(0., color='k');\n",
        "xx = np.arange(-5, +5, 0.001)\n",
        "yy = stats.norm.pdf(xx)\n",
        "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
        "ax.set_xlim([-6., 6.]);\n",
        "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E_ZGbWAeBrga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP (~45h; you may skip this code chunk to save your time)\n",
        "ml_l_mlp = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"regressor\", KerasRegressor(\n",
        "        model=create_mlp_model,\n",
        "        n_features=x.shape[1],\n",
        "        n_hidden=32,\n",
        "        learning_rate=0.01,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    ))\n",
        "])\n",
        "\n",
        "ml_m_mlp = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"regressor\", KerasRegressor(\n",
        "        model=create_mlp_model,\n",
        "        n_features=x.shape[1],\n",
        "        n_hidden=32,\n",
        "        learning_rate=0.01,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    ))\n",
        "])\n",
        "\n",
        "ml_g_mlp = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"regressor\", KerasRegressor(\n",
        "        model=create_mlp_model,\n",
        "        n_features=x.shape[1],\n",
        "        n_hidden=32,\n",
        "        learning_rate=0.01,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    ))\n",
        "])\n",
        "\n",
        "small_n_rep = int(n_rep / 5)\n",
        "theta_dml_mlp = np.full(small_n_rep, np.nan)\n",
        "se_dml_mlp = np.full(small_n_rep, np.nan)\n",
        "\n",
        "for i_rep in tqdm(range(small_n_rep), total=small_n_rep):\n",
        "    print(f'Replication {i_rep+1}/{small_n_rep}', end='\\r')\n",
        "    (x, y, d) = data[i_rep]\n",
        "    obj_dml_data = DoubleMLData.from_arrays(x, y, d)\n",
        "    obj_dml_plr = DoubleMLPLR(obj_dml_data,\n",
        "                            ml_l_mlp, ml_m_mlp, ml_g_mlp,\n",
        "                            n_folds=2,\n",
        "                            score='IV-type')\n",
        "    obj_dml_plr.fit()\n",
        "    theta_dml_mlp[i_rep] = obj_dml_plr.coef[0]\n",
        "    se_dml_mlp[i_rep] = obj_dml_plr.se[0]\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "fig_dml, ax = plt.subplots(constrained_layout=True)\n",
        "ax = sns.histplot((theta_dml_mlp - alpha)/se_dml_mlp,\n",
        "                color=face_colors[4], edgecolor = edge_colors[4],\n",
        "                stat='density', bins=30, label='Double ML (MLP) with cross-fitting')\n",
        "ax.axvline(0., color='k');\n",
        "xx = np.arange(-5, +5, 0.001)\n",
        "yy = stats.norm.pdf(xx)\n",
        "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$')\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
        "ax.set_xlim([-6., 6.])\n",
        "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uCEqBUK-B7MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double/debiased machine learning\n",
        "\n",
        "To illustrate the benefits of the auxiliary prediction step in the DML framework we write the error as\n",
        "\n",
        "$$\n",
        "\\sqrt{n}(\\check{\\theta_0} - \\theta_0) = a^* + b^* + c^*\n",
        "$$\n",
        "\n",
        "Chernozhukov et al. (2018) argues that:\n",
        "\n",
        "The first term\n",
        "\n",
        "$$\n",
        "a^* := (EV^2)^{-1} \\frac{1}{\\sqrt{n}} \\sum_{i\\in I} V_i \\zeta_i\n",
        "$$\n",
        "\n",
        "will be asymptotically normally distributed.\n",
        "\n",
        "The second term\n",
        "\n",
        "$$\n",
        "b^* := (EV^2)^{-1} \\frac{1}{\\sqrt{n}} \\sum_{i\\in I} (\\hat{m}(X_i) - m(X_i)) (\\hat{g_0}(X_i) - g_0(X_i))\n",
        "$$\n",
        "\n",
        "vanishes asymptotically for many data generating processes.\n",
        "\n",
        "The third term $c^*$ vanishes in probability if sample splitting is applied."
      ],
      "metadata": {
        "id": "ZMnUaAZKGJop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DML Example: Impact of 401(k) on Financial Wealth"
      ],
      "metadata": {
        "id": "rTKQcVV4bHmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this real-data example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the effect of 401(k) eligibility and participation on accumulated assets. The 401(k) data set has been analyzed in several studies, among others [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060).\n",
        "\n",
        "401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.\n",
        "\n",
        "One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)’s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job."
      ],
      "metadata": {
        "id": "mslNVFWqHIUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The preprocessed data can be fetched by calling [fetch_401K()](https://docs.doubleml.org/stable/api/generated/doubleml.datasets.fetch_401K.html#doubleml.datasets.fetch_401K). Note that an internet connection is required for loading the data."
      ],
      "metadata": {
        "id": "V-82-p0ma3JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from doubleml.datasets import fetch_401K\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "giBE4-axYCza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = 10., 7.5\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style('whitegrid', {'axes.spines.top': False,\n",
        "                            'axes.spines.bottom': False,\n",
        "                            'axes.spines.left': False,\n",
        "                            'axes.spines.right': False})\n",
        "\n",
        "# load the 401k dataset\n",
        "data = fetch_401K(return_type='DataFrame')"
      ],
      "metadata": {
        "id": "Hef7p05PbGZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporary fix for https://github.com/DoubleML/doubleml-docs/issues/45 / https://github.com/scikit-learn/scikit-learn/issues/21997\n",
        "# Can be removed when scikit-learn version 1.2.0 is released\n",
        "dtypes = data.dtypes\n",
        "dtypes['nifa'] = 'float64'\n",
        "dtypes['net_tfa'] = 'float64'\n",
        "dtypes['tw'] = 'float64'\n",
        "dtypes['inc'] = 'float64'\n",
        "data = data.astype(dtypes)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LPgFadFrcv6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "Pq1gagEQc2YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP).  All the variables are referred to 1990. We use net financial assets (*net\\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts.\n",
        "\n",
        "Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively."
      ],
      "metadata": {
        "id": "H0eKJcbSc7HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)  # 1 row, 2 columns\n",
        "\n",
        "# Plot the first bar plot for e401\n",
        "data['e401'].value_counts().plot(kind='bar', color=face_colors, ax=axes[0])\n",
        "axes[0].set_title('Eligibility, 401(k)')\n",
        "axes[0].set_xlabel('e401')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "# Plot the second bar plot for p401\n",
        "data['p401'].value_counts().plot(kind='bar', color=face_colors, ax=axes[1])\n",
        "axes[1].set_title('Participation, 401(k)')\n",
        "axes[1].set_xlabel('p401')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxaHOPeEdQqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
        "\n",
        "# Plot for e401 = 0\n",
        "sns.kdeplot(data=data[data[\"e401\"] == 0], x=\"net_tfa\", fill=True,\n",
        "            color=face_colors[0], ax=axes[0])\n",
        "axes[0].set_title(\"e401 = 0\", fontsize=16)\n",
        "axes[0].set_xlabel(\"Net Total Financial Assets (net_tfa)\", fontsize=14)\n",
        "axes[0].set_ylabel(\"Density\", fontsize=14)\n",
        "axes[0].tick_params(labelsize=12)\n",
        "\n",
        "# Plot for e401 = 1\n",
        "sns.kdeplot(data=data[data[\"e401\"] == 1], x=\"net_tfa\", fill=True,\n",
        "            color=face_colors[1], ax=axes[1])\n",
        "axes[1].set_title(\"e401 = 1\", fontsize=16)\n",
        "axes[1].set_xlabel(\"Net Total Financial Assets (net_tfa)\", fontsize=14)\n",
        "axes[1].tick_params(labelsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WjJCNKOKeFio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first estimate, we calculate the unconditional average predictive effect (APE) of 401(k) eligibility on accumulated assets. This effect corresponds to the average treatment effect if 401(k) eligibility would be assigned to individuals in an entirely randomized way. The unconditional APE of e401 is about $19559$:"
      ],
      "metadata": {
        "id": "0chgpweIgQtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['e401', 'net_tfa']].groupby('e401').mean().diff()"
      ],
      "metadata": {
        "id": "MIOQ7zubgLMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the $3682$ individuals that  are eligible, $2594$ decided to participate in the program. The unconditional APE of p401 is about $27372$:"
      ],
      "metadata": {
        "id": "gsTN0rX0gZmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['p401', 'net_tfa']].groupby('p401').mean().diff()"
      ],
      "metadata": {
        "id": "tt2JynJPgWBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed, these estimates are biased since they do not account for saver heterogeneity and endogeneity of participation.\n",
        "\n",
        "Let's use the package [DoubleML](https://docs.doubleml.org/stable/index.html) to estimate the average treatment effect of 401(k) eligibility, i.e. `e401`, and participation, i.e. `p401`, on net financial assets `net_tfa`."
      ],
      "metadata": {
        "id": "qDrrudsygalK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets\n",
        "\n",
        "We first look at the treatment effect of `e401` on net total financial assets. We give estimates of the ATE in the linear model\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "Y = D \\alpha + f(X)'\\beta+ \\epsilon,\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "where $f(X)$ is a dictonary applied to the raw regressors. $X$ contains variables on marital status, two-earner status, defined benefit pension status, IRA participation, home ownership, family size, education, age, and income.\n",
        "\n",
        "In the following, we will consider two different models,\n",
        "\n",
        "* a basic model specification that includes the raw regressors, i.e., $f(X) = X$, and\n",
        "\n",
        "* a flexible model specification, where $f(X)$ includes the raw regressors $X$ and the orthogonal polynomials of degree 2 for the variables family size education, age, and income.\n",
        "\n",
        "We will use the basic model specification whenever we use nonlinear methods, for example regression trees or random forests, and use the flexible model for linear methods such as the lasso. There are, of course, multiple ways how the model can be specified even more flexibly, for example including interactions of variable and higher order interaction. However, for the sake of simplicity we stick to the specification above. Users who are interested in varying the model can adapt the code below accordingly, for example to implement the orignal specification in [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060).\n",
        "\n",
        "In the first step, we report estimates of the average treatment effect (ATE) of 401(k) eligibility on net financial assets both in the partially linear regression (PLR) model and in the interactive regression model (IRM) allowing for heterogeneous treatment effects."
      ],
      "metadata": {
        "id": "eNGM6H1Sgn1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Data Backend: `DoubleMLData`\n",
        "\n",
        "To start our analysis, we initialize the data backend, i.e., a new instance of a [DoubleMLData](https://docs.doubleml.org/dev/api/generated/doubleml.DoubleMLData.html#doubleml.DoubleMLData) object. We implement the regression model by using scikit-learn's `PolynomialFeatures` class.\n",
        "\n",
        "To implement both models (basic and flexible), we generate two data backends: `data_dml_base` and `data_dml_flex`."
      ],
      "metadata": {
        "id": "6ALoaXLBs9O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up basic model: Specify variables for data-backend\n",
        "features_base = ['age', 'inc', 'educ', 'fsize', 'marr',\n",
        "                 'twoearn', 'db', 'pira', 'hown']\n",
        "\n",
        "# Initialize DoubleMLData (data-backend of DoubleML)\n",
        "data_dml_base = doubleml.DoubleMLData(data,\n",
        "                                 y_col='net_tfa',\n",
        "                                 d_cols='e401',\n",
        "                                 x_cols=features_base)\n",
        "\n",
        "print(data_dml_base)"
      ],
      "metadata": {
        "id": "I72gY9ZQgdS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a model according to regression formula with polynomials\n",
        "features = data.copy()[['marr', 'twoearn', 'db', 'pira', 'hown']]\n",
        "\n",
        "poly_dict = {'age': 2,\n",
        "             'inc': 2,\n",
        "             'educ': 2,\n",
        "             'fsize': 2}\n",
        "for key, degree in poly_dict.items():\n",
        "    poly = PolynomialFeatures(degree, include_bias=False)\n",
        "    data_transf = poly.fit_transform(data[[key]])\n",
        "    x_cols = poly.get_feature_names_out([key])\n",
        "    data_transf = pd.DataFrame(data_transf, columns=x_cols)\n",
        "\n",
        "    features = pd.concat((features, data_transf),\n",
        "                          axis=1, sort=False)\n",
        "model_data = pd.concat((data.copy()[['net_tfa', 'e401']], features.copy()),\n",
        "                        axis=1, sort=False)\n",
        "\n",
        "# Initialize DoubleMLData (data-backend of DoubleML)\n",
        "data_dml_flex = doubleml.DoubleMLData(model_data, y_col='net_tfa', d_cols='e401')\n",
        "\n",
        "print(data_dml_flex)"
      ],
      "metadata": {
        "id": "nvuFX0VUtey7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Partially Linear Regression Model (PLR)\n",
        "\n",
        "We start using lasso to estimate the function $g_0$ and $m_0$ in the following PLR model:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& Y = D\\theta_0 + g_0(X) + \\zeta, &\\quad E[\\zeta \\mid D,X]= 0,\\\\\n",
        "& D = m_0(X) +  V, &\\quad E[V \\mid X] = 0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To estimate the causal parameter $\\theta_0$ here, we use double machine learning with 3-fold cross-fitting.\n",
        "\n",
        "Estimation of the nuisance components $g_0$ and $m_0$, is based on the lasso with cross-validated choice of the penalty term , $\\lambda$, as provided by [scikit-learn](https://scikit-learn.org). We load the learner by initializing instances from the classes [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html). Hyperparameters and options can be set during instantiation of the learner. Here we specify that the lasso should use that value of $\\lambda$ that minimizes the cross-validated mean squared error which is based on 5-fold cross validation.\n",
        "\n",
        "We start by estimation the ATE in the basic model and then repeat the estimation in the flexible model."
      ],
      "metadata": {
        "id": "I_WEvMcgttA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize learners\n",
        "Cs = 0.0001*np.logspace(0, 4, 10)\n",
        "lasso = make_pipeline(StandardScaler(), LassoCV(cv=5, max_iter=10000))\n",
        "lasso_class = make_pipeline(StandardScaler(),\n",
        "                            LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear',\n",
        "                                                 Cs = Cs, max_iter=1000))\n",
        "\n",
        "np.random.seed(60615)\n",
        "# Initialize DoubleMLPLR model\n",
        "dml_plr_lasso = doubleml.DoubleMLPLR(data_dml_base,\n",
        "                                ml_l = lasso,\n",
        "                                ml_m = lasso_class,\n",
        "                                n_folds = 3)\n",
        "\n",
        "dml_plr_lasso.fit(store_predictions=True)\n",
        "print(dml_plr_lasso.summary)"
      ],
      "metadata": {
        "id": "bSPj2Y1ntpb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate the ATE in the flexible model with lasso\n",
        "np.random.seed(60615)\n",
        "dml_plr_lasso = doubleml.DoubleMLPLR(data_dml_flex,\n",
        "                                ml_l = lasso,\n",
        "                                ml_m = lasso_class,\n",
        "                                n_folds = 3)\n",
        "\n",
        "dml_plr_lasso.fit(store_predictions=True)\n",
        "lasso_summary = dml_plr_lasso.summary\n",
        "\n",
        "print(lasso_summary)"
      ],
      "metadata": {
        "id": "y5X0MmJ8us2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can repeat this procedure with other machine learning methods, for example a random forest learner as provided by the [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class in [scikit-learn](https://scikit-learn.org)."
      ],
      "metadata": {
        "id": "NIN7YSSOu2gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "randomForest = RandomForestRegressor(\n",
        "    n_estimators=500, max_depth=7, max_features=3, min_samples_leaf=3)\n",
        "randomForest_class = RandomForestClassifier(\n",
        "    n_estimators=500, max_depth=5, max_features=4, min_samples_leaf=7)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_plr_forest = doubleml.DoubleMLPLR(data_dml_base,\n",
        "                                 ml_l = randomForest,\n",
        "                                 ml_m = randomForest_class,\n",
        "                                 n_folds = 3)\n",
        "dml_plr_forest.fit(store_predictions=True)\n",
        "forest_summary = dml_plr_forest.summary\n",
        "\n",
        "print(forest_summary)"
      ],
      "metadata": {
        "id": "lxOxNl9pu4Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees\n",
        "trees = DecisionTreeRegressor(\n",
        "    max_depth=30, ccp_alpha=0.0047, min_samples_split=203, min_samples_leaf=67)\n",
        "trees_class = DecisionTreeClassifier(\n",
        "    max_depth=30, ccp_alpha=0.0042, min_samples_split=104, min_samples_leaf=34)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_plr_tree = doubleml.DoubleMLPLR(data_dml_base,\n",
        "                               ml_l = trees,\n",
        "                               ml_m = trees_class,\n",
        "                               n_folds = 3)\n",
        "dml_plr_tree.fit(store_predictions=True)\n",
        "tree_summary = dml_plr_tree.summary\n",
        "\n",
        "print(tree_summary)"
      ],
      "metadata": {
        "id": "bdQMSV8pu-Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boosted Trees\n",
        "boost = XGBRegressor(n_jobs=1, objective = \"reg:squarederror\",\n",
        "                     eta=0.1, n_estimators=35)\n",
        "boost_class = XGBClassifier(use_label_encoder=False, n_jobs=1,\n",
        "                            objective = \"binary:logistic\", eval_metric = \"logloss\",\n",
        "                            eta=0.1, n_estimators=34)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_plr_boost = doubleml.DoubleMLPLR(data_dml_base,\n",
        "                                ml_l = boost,\n",
        "                                ml_m = boost_class,\n",
        "                                n_folds = 3)\n",
        "dml_plr_boost.fit(store_predictions=True)\n",
        "boost_summary = dml_plr_boost.summary\n",
        "\n",
        "print(boost_summary)"
      ],
      "metadata": {
        "id": "qeew4tyDvDT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP\n",
        "n_features = data_dml_base.x.shape[1]\n",
        "\n",
        "# 2. Define MLP for regression (outcome)\n",
        "#    Omit explicit input_shape and let SciKeras handle shape inference\n",
        "def create_mlp_model1_regression(n_features=10, n_hidden=32, learning_rate=0.01, **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_hidden, activation='relu', input_shape=(n_features,)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        metrics=[]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 3. Define MLP for classification (treatment)\n",
        "#    If your treatment is truly binary (0/1).\n",
        "def create_mlp_model1_classification(n_features=10, n_hidden=32, learning_rate=0.01, **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_hidden, activation='relu', input_shape=(n_features,)))\n",
        "    # Binary classification => 1 output node, sigmoid activation\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 4. Wrap each with SciKeras\n",
        "ml_l_mlp = KerasRegressor(\n",
        "    model=create_mlp_model1_regression,\n",
        "    model__n_features=n_features,     # match data_dml_base.x.shape[1]\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_m_mlp = KerasClassifier(\n",
        "    model=create_mlp_model1_classification,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# 5. Create the DoubleML PLR object\n",
        "dml_plr_mlp = DoubleMLPLR(\n",
        "    data_dml_base,\n",
        "    ml_l=ml_l_mlp,  # MLP regressor for the outcome\n",
        "    ml_m=ml_m_mlp,  # MLP classifier for the binary treatment\n",
        "    n_folds=3\n",
        "    # Optionally: score='IV-type' or 'partialling out', etc.\n",
        ")\n",
        "\n",
        "# 6. Fit\n",
        "dml_plr_mlp.fit(store_predictions=True)\n",
        "\n",
        "# 7. Show summary\n",
        "mlp_summary = dml_plr_mlp.summary\n",
        "print(mlp_summary)"
      ],
      "metadata": {
        "id": "V_3ZDEZMvIWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp_model2_regression(\n",
        "    n_features=10,\n",
        "    n_hidden1=64,\n",
        "    n_hidden2=32,\n",
        "    learning_rate=0.01,\n",
        "    dropout_rate=0.2,\n",
        "    **kwargs\n",
        "):\n",
        "    model = Sequential()\n",
        "    # First hidden layer\n",
        "    model.add(Dense(n_hidden1, activation='relu', input_shape=(n_features,)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    # Second hidden layer\n",
        "    model.add(Dense(n_hidden2, activation='relu'))\n",
        "    # Output layer for regression\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        metrics=[]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_mlp_model2_classification(\n",
        "    n_features=10,\n",
        "    n_hidden1=64,\n",
        "    n_hidden2=32,\n",
        "    learning_rate=0.01,\n",
        "    dropout_rate=0.2,\n",
        "    **kwargs\n",
        "):\n",
        "    model = Sequential()\n",
        "    # First hidden layer\n",
        "    model.add(Dense(n_hidden1, activation='relu', input_shape=(n_features,)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    # Second hidden layer\n",
        "    model.add(Dense(n_hidden2, activation='relu'))\n",
        "    # Output layer (sigmoid for binary classification)\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "ml_l_mlp = KerasRegressor(\n",
        "    model=create_mlp_model2_regression,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden1=64,\n",
        "    model__n_hidden2=32,\n",
        "    model__dropout_rate=0.2,     # Increase or decrease as needed\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_m_mlp = KerasClassifier(\n",
        "    model=create_mlp_model2_classification,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden1=64,\n",
        "    model__n_hidden2=32,\n",
        "    model__dropout_rate=0.2,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "dml_plr_mlp2 = DoubleMLPLR(\n",
        "    data_dml_base,\n",
        "    ml_l=ml_l_mlp,   # Deeper MLP regressor for outcome\n",
        "    ml_m=ml_m_mlp,   # Deeper MLP classifier for binary treatment\n",
        "    n_folds=3        # Cross-fitting folds\n",
        "    # score=...       # e.g., 'partialling out', 'IV-type', etc.\n",
        ")\n",
        "\n",
        "dml_plr_mlp2.fit(store_predictions=True)\n",
        "\n",
        "mlp2_summary = dml_plr_mlp2.summary\n",
        "print(mlp2_summary)"
      ],
      "metadata": {
        "id": "f-rr6G4Vzjdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's sum up the results:"
      ],
      "metadata": {
        "id": "QoX5KrXt0Y5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plr_summary = pd.concat((lasso_summary, forest_summary, tree_summary, boost_summary, mlp_summary, mlp2_summary))\n",
        "plr_summary.index = ['lasso', 'forest', 'tree', 'xgboost', '1-layer mlp', '2-layer mlp']\n",
        "print(plr_summary[['coef', '2.5 %', '97.5 %']])"
      ],
      "metadata": {
        "id": "BofzlXPm0X1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.full((2, plr_summary.shape[0]), np.nan)\n",
        "errors[0, :] = plr_summary['coef'] - plr_summary['2.5 %']\n",
        "errors[1, :] = plr_summary['97.5 %'] - plr_summary['coef']\n",
        "plt.errorbar(plr_summary.index, plr_summary.coef, fmt='o', yerr=errors)\n",
        "plt.ylim([0, 15000])\n",
        "\n",
        "plt.title('Partially Linear Regression Model (PLR)')\n",
        "plt.xlabel('ML method')\n",
        "_ =  plt.ylabel('Coefficients and 95%-CI')"
      ],
      "metadata": {
        "id": "ljo5Wsen1b-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These estimates that flexibly account for confounding are\n",
        "substantially attenuated relative to the baseline estimate (*19559*) that does not account for confounding. They suggest much smaller causal effects of 401(k) eligiblity on financial asset holdings."
      ],
      "metadata": {
        "id": "2VPDATxm05QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interactive Regression Model (IRM)\n",
        "\n",
        "Next, we consider estimation of average treatment effects when treatment effects are fully heterogeneous:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& Y = g_0(D,X) + U, &\\quad E[U\\mid X,D] = 0,\\\\\n",
        "& D = m_0(X) + V, &\\quad E[V\\mid X] = 0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To reduce the disproportionate impact of extreme propensity score weights in the interactive model\n",
        "we trim the propensity scores which are close to the bounds."
      ],
      "metadata": {
        "id": "4t0YRo0Z6icR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso\n",
        "lasso = make_pipeline(StandardScaler(), LassoCV(cv=5, max_iter=20000))\n",
        "\n",
        "# Initialize DoubleMLIRM model\n",
        "np.random.seed(60615)\n",
        "dml_irm_lasso = doubleml.DoubleMLIRM(data_dml_flex,\n",
        "                          ml_g = lasso,\n",
        "                          ml_m = lasso_class,\n",
        "                          trimming_threshold = 0.01,\n",
        "                          n_folds = 3)\n",
        "dml_irm_lasso.fit(store_predictions=True)\n",
        "lasso_summary = dml_irm_lasso.summary\n",
        "\n",
        "# Random Forest\n",
        "randomForest = RandomForestRegressor(n_estimators=500)\n",
        "randomForest_class = RandomForestClassifier(n_estimators=500)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_irm_forest = doubleml.DoubleMLIRM(data_dml_base,\n",
        "                                 ml_g = randomForest,\n",
        "                                 ml_m = randomForest_class,\n",
        "                                 trimming_threshold = 0.01,\n",
        "                                 n_folds = 3)\n",
        "\n",
        "# Set nuisance-part specific parameters\n",
        "dml_irm_forest.set_ml_nuisance_params('ml_g0', 'e401', {\n",
        "    'max_depth': 6, 'max_features': 4, 'min_samples_leaf': 7})\n",
        "dml_irm_forest.set_ml_nuisance_params('ml_g1', 'e401', {\n",
        "    'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 5})\n",
        "dml_irm_forest.set_ml_nuisance_params('ml_m', 'e401', {\n",
        "    'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 6})\n",
        "\n",
        "dml_irm_forest.fit(store_predictions=True)\n",
        "forest_summary = dml_irm_forest.summary\n",
        "\n",
        "# Trees\n",
        "trees = DecisionTreeRegressor(max_depth=30)\n",
        "trees_class = DecisionTreeClassifier(max_depth=30)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_irm_tree = doubleml.DoubleMLIRM(data_dml_base,\n",
        "                               ml_g = trees,\n",
        "                               ml_m = trees_class,\n",
        "                               trimming_threshold = 0.01,\n",
        "                               n_folds = 3)\n",
        "\n",
        "# Set nuisance-part specific parameters\n",
        "dml_irm_tree.set_ml_nuisance_params('ml_g0', 'e401', {\n",
        "    'ccp_alpha': 0.0016, 'min_samples_split': 74, 'min_samples_leaf': 24})\n",
        "dml_irm_tree.set_ml_nuisance_params('ml_g1', 'e401', {\n",
        "    'ccp_alpha': 0.0018, 'min_samples_split': 70, 'min_samples_leaf': 23})\n",
        "dml_irm_tree.set_ml_nuisance_params('ml_m', 'e401', {\n",
        "    'ccp_alpha': 0.0028, 'min_samples_split': 167, 'min_samples_leaf': 55})\n",
        "\n",
        "dml_irm_tree.fit(store_predictions=True)\n",
        "tree_summary = dml_irm_tree.summary\n",
        "\n",
        "# Boosted Trees\n",
        "boost = XGBRegressor(n_jobs=1, objective = \"reg:squarederror\")\n",
        "boost_class = XGBClassifier(use_label_encoder=False, n_jobs=1,\n",
        "                            objective = \"binary:logistic\", eval_metric = \"logloss\")\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_irm_boost = doubleml.DoubleMLIRM(data_dml_base,\n",
        "                                ml_g = boost,\n",
        "                                ml_m = boost_class,\n",
        "                                trimming_threshold = 0.01,\n",
        "                                n_folds = 3)\n",
        "\n",
        "# Set nuisance-part specific parameters\n",
        "dml_irm_boost.set_ml_nuisance_params('ml_g0', 'e401', {\n",
        "    'eta': 0.1, 'n_estimators': 8})\n",
        "dml_irm_boost.set_ml_nuisance_params('ml_g1', 'e401', {\n",
        "    'eta': 0.1, 'n_estimators': 29})\n",
        "dml_irm_boost.set_ml_nuisance_params('ml_m', 'e401', {\n",
        "    'eta': 0.1, 'n_estimators': 23})\n",
        "\n",
        "dml_irm_boost.fit(store_predictions=True)\n",
        "boost_summary = dml_irm_boost.summary\n",
        "\n",
        "# 1 layer mlp\n",
        "ml_g_mlp = KerasRegressor(\n",
        "    model=create_mlp_model1_regression,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_m_mlp = KerasClassifier(\n",
        "    model=create_mlp_model1_classification,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_r_mlp = KerasClassifier(\n",
        "    model=create_mlp_model1_classification,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_irm_mlp1 = doubleml.DoubleMLIRM(\n",
        "    data_dml_base,\n",
        "    ml_g=ml_g_mlp,  # MLP regressor for outcome\n",
        "    ml_m=ml_m_mlp,  # MLP classifier for binary treatment\n",
        "    trimming_threshold=0.01,\n",
        "    n_folds=3\n",
        ")\n",
        "\n",
        "# For example:\n",
        "# dml_irm_mlp1.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "#     'epochs': 15,\n",
        "#     'model__learning_rate': 0.005\n",
        "# })\n",
        "# dml_irm_mlp1.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "#     'epochs': 25,\n",
        "#     'batch_size': 64,\n",
        "#     'model__n_hidden': 64\n",
        "# })\n",
        "# dml_irm_mlp1.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "#     'epochs': 30,\n",
        "#     'model__n_hidden': 16\n",
        "# })\n",
        "\n",
        "dml_irm_mlp1.fit(store_predictions=True)\n",
        "mlp1_summary = dml_irm_mlp1.summary\n",
        "\n",
        "# 2-layer MLP\n",
        "ml_g_mlp = KerasRegressor(\n",
        "    model=create_mlp_model2_regression,\n",
        "    n_features=n_features,\n",
        "    n_hidden1=64,      # or tune\n",
        "    n_hidden2=32,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_m_mlp = KerasClassifier(\n",
        "    model=create_mlp_model2_classification,\n",
        "    n_features=n_features,\n",
        "    n_hidden1=64,\n",
        "    n_hidden2=32,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_r_mlp = KerasClassifier(\n",
        "    model=create_mlp_model2_classification,\n",
        "    n_features=n_features,\n",
        "    n_hidden1=64,\n",
        "    n_hidden2=32,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_irm_mlp2 = doubleml.DoubleMLIRM(\n",
        "    data_dml_base,\n",
        "    ml_g=ml_g_mlp,\n",
        "    ml_m=ml_m_mlp,\n",
        "    trimming_threshold=0.01,\n",
        "    n_folds=3\n",
        ")\n",
        "\n",
        "\n",
        "# (Optional) set nuisance-part-specific parameters:\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "#     'epochs': 15,\n",
        "#     'model__learning_rate': 0.005\n",
        "# })\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_g1', 'p401', {\n",
        "#     'epochs': 25,\n",
        "#     'batch_size': 32\n",
        "# })\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "#     'model__dropout_rate': 0.3\n",
        "# })\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "#     'epochs': 30,\n",
        "#     'model__n_hidden1': 128\n",
        "# })\n",
        "\n",
        "dml_irm_mlp2.fit(store_predictions=True)\n",
        "mlp2_summary = dml_irm_mlp2.summary"
      ],
      "metadata": {
        "id": "DTgtg6se6tCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irm_summary = pd.concat((lasso_summary, forest_summary, tree_summary, boost_summary, mlp1_summary, mlp2_summary))\n",
        "irm_summary.index = ['lasso', 'forest', 'tree', 'xgboost', '1-layer mlp', '2-layer mlp']\n",
        "print(irm_summary[['coef', '2.5 %', '97.5 %']])"
      ],
      "metadata": {
        "id": "O-2qZn6W7_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.full((2, irm_summary.shape[0]), np.nan)\n",
        "errors[0, :] = irm_summary['coef'] - irm_summary['2.5 %']\n",
        "errors[1, :] = irm_summary['97.5 %'] - irm_summary['coef']\n",
        "plt.errorbar(irm_summary.index, irm_summary.coef, fmt='o', yerr=errors)\n",
        "plt.ylim([0, 12500])\n",
        "\n",
        "plt.title('Interactive Regression Model (IRM)')\n",
        "plt.xlabel('ML method')\n",
        "_ = plt.ylabel('Coefficients and 95%-CI')"
      ],
      "metadata": {
        "id": "ZOfRnWD68HdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local Average Treatment Effects of 401(k) Participation on Net Financial Assets"
      ],
      "metadata": {
        "id": "7t4uUBzu09Xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interactive IV Model (IIVM)\n",
        "\n",
        "In the examples above, we estimated the average treatment effect of *eligibility* on financial asset holdings. Now, we consider estimation of local average treatment effects (LATE) of *participation* using eligibility as an instrument for the participation decision. Under appropriate assumptions, the LATE identifies the treatment effect for so-called compliers, i.e., individuals who would only participate if eligible and otherwise not participate in the program.\n",
        "\n",
        "As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates. We use `e401` as a binary instrument for the treatment variable `p401`. Here the structural equation model is:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& Y = g_0(Z,X) + U, &\\quad E[U\\mid Z,X] = 0,\\\\\n",
        "& D = r_0(Z,X) + V, &\\quad E[V\\mid Z, X] = 0,\\\\\n",
        "& Z = m_0(X) + \\zeta, &\\quad E[\\zeta \\mid X] = 0.\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "VO320hvm1HQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DoubleMLData with an instrument\n",
        "\n",
        "# Basic model\n",
        "data_dml_base_iv = doubleml.DoubleMLData(data,\n",
        "                                    y_col='net_tfa',\n",
        "                                    d_cols='p401',\n",
        "                                    z_cols='e401',\n",
        "                                    x_cols=features_base)"
      ],
      "metadata": {
        "id": "GSaDVWk92_-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flexible model\n",
        "model_data = pd.concat((data.copy()[['net_tfa', 'e401', 'p401']], features.copy()),\n",
        "                        axis=1, sort=False)\n",
        "\n",
        "data_dml_iv_flex = doubleml.DoubleMLData(model_data,\n",
        "                                    y_col='net_tfa',\n",
        "                                    d_cols='p401',\n",
        "                                    z_cols='e401')"
      ],
      "metadata": {
        "id": "_iF5AVsO3F3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso\n",
        "lasso = make_pipeline(StandardScaler(), LassoCV(cv=5, max_iter=20000))\n",
        "\n",
        "# Initialize DoubleMLIRM model\n",
        "np.random.seed(60615)\n",
        "dml_iivm_lasso = doubleml.DoubleMLIIVM(data_dml_iv_flex,\n",
        "                                  ml_g = lasso,\n",
        "                                  ml_m = lasso_class,\n",
        "                                  ml_r = lasso_class,\n",
        "                                  subgroups = {'always_takers': False,\n",
        "                                             'never_takers': True},\n",
        "                                  trimming_threshold = 0.01,\n",
        "                                  n_folds = 3)\n",
        "dml_iivm_lasso.fit(store_predictions=True)\n",
        "lasso_summary = dml_iivm_lasso.summary\n",
        "\n",
        "# Random Forest\n",
        "randomForest = RandomForestRegressor(n_estimators=500)\n",
        "randomForest_class = RandomForestClassifier(n_estimators=500)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_iivm_forest = doubleml.DoubleMLIIVM(data_dml_base_iv,\n",
        "                                   ml_g = randomForest,\n",
        "                                   ml_m = randomForest_class,\n",
        "                                   ml_r = randomForest_class,\n",
        "                                   subgroups = {'always_takers': False,\n",
        "                                                'never_takers': True},\n",
        "                                   trimming_threshold = 0.01,\n",
        "                                   n_folds = 3)\n",
        "\n",
        "# Set nuisance-part specific parameters\n",
        "dml_iivm_forest.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "    'max_depth': 6, 'max_features': 4, 'min_samples_leaf': 7})\n",
        "dml_iivm_forest.set_ml_nuisance_params('ml_g1', 'p401', {\n",
        "    'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 5})\n",
        "dml_iivm_forest.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "    'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 6})\n",
        "dml_iivm_forest.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "    'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 6})\n",
        "\n",
        "dml_iivm_forest.fit(store_predictions=True)\n",
        "forest_summary = dml_iivm_forest.summary\n",
        "\n",
        "# Trees\n",
        "trees = DecisionTreeRegressor(max_depth=30)\n",
        "trees_class = DecisionTreeClassifier(max_depth=30)\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_iivm_tree = doubleml.DoubleMLIIVM(data_dml_base_iv,\n",
        "                                 ml_g = trees,\n",
        "                                 ml_m = trees_class,\n",
        "                                 ml_r = trees_class,\n",
        "                                 subgroups = {'always_takers': False,\n",
        "                                              'never_takers': True},\n",
        "                                 trimming_threshold = 0.01,\n",
        "                                 n_folds = 3)\n",
        "\n",
        "# Set nuisance-part specific parameters\n",
        "dml_iivm_tree.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "    'ccp_alpha': 0.0016, 'min_samples_split': 74, 'min_samples_leaf': 24})\n",
        "dml_iivm_tree.set_ml_nuisance_params('ml_g1', 'p401', {\n",
        "    'ccp_alpha': 0.0018, 'min_samples_split': 70, 'min_samples_leaf': 23})\n",
        "dml_iivm_tree.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "    'ccp_alpha': 0.0028, 'min_samples_split': 167, 'min_samples_leaf': 55})\n",
        "dml_iivm_tree.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "    'ccp_alpha': 0.0576, 'min_samples_split': 55, 'min_samples_leaf': 18})\n",
        "\n",
        "dml_iivm_tree.fit(store_predictions=True)\n",
        "tree_summary = dml_iivm_tree.summary\n",
        "\n",
        "# Boosted Trees\n",
        "boost = XGBRegressor(n_jobs=1, objective = \"reg:squarederror\")\n",
        "boost_class = XGBClassifier(use_label_encoder=False, n_jobs=1,\n",
        "                            objective = \"binary:logistic\", eval_metric = \"logloss\")\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_iivm_boost = doubleml.DoubleMLIIVM(data_dml_base_iv,\n",
        "                                  ml_g = boost,\n",
        "                                  ml_m = boost_class,\n",
        "                                  ml_r = boost_class,\n",
        "                                  subgroups = {'always_takers': False,\n",
        "                                               'never_takers': True},\n",
        "                                  trimming_threshold = 0.01,\n",
        "                                  n_folds = 3)\n",
        "\n",
        "# Set nuisance-part specific parameters\n",
        "dml_iivm_boost.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "    'eta': 0.1, 'n_estimators': 9})\n",
        "dml_iivm_boost.set_ml_nuisance_params('ml_g1', 'p401', {\n",
        "    'eta': 0.1, 'n_estimators': 33})\n",
        "dml_iivm_boost.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "    'eta': 0.1, 'n_estimators': 12})\n",
        "dml_iivm_boost.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "    'eta': 0.1, 'n_estimators': 25})\n",
        "\n",
        "dml_iivm_boost.fit(store_predictions=True)\n",
        "boost_summary = dml_iivm_boost.summary\n",
        "\n",
        "# 1-layer MLP\n",
        "ml_g_mlp = KerasRegressor(\n",
        "    model=create_mlp_model1_regression,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_m_mlp = KerasClassifier(\n",
        "    model=create_mlp_model1_classification,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_r_mlp = KerasClassifier(\n",
        "    model=create_mlp_model1_classification,\n",
        "    model__n_features=n_features,\n",
        "    model__n_hidden=32,\n",
        "    model__learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_iivm_mlp1 = doubleml.DoubleMLIIVM(\n",
        "    data_dml_base_iv,\n",
        "    ml_g=ml_g_mlp,  # MLP regressor for outcome\n",
        "    ml_m=ml_m_mlp,  # MLP classifier for binary treatment\n",
        "    ml_r=ml_r_mlp,  # MLP classifier for binary instrument propensity\n",
        "    subgroups={'always_takers': False,\n",
        "               'never_takers': True},\n",
        "    trimming_threshold=0.01,\n",
        "    n_folds=3\n",
        ")\n",
        "\n",
        "# For example:\n",
        "# dml_iivm_mlp1.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "#     'epochs': 15,\n",
        "#     'model__learning_rate': 0.005\n",
        "# })\n",
        "# dml_iivm_mlp1.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "#     'epochs': 25,\n",
        "#     'batch_size': 64,\n",
        "#     'model__n_hidden': 64\n",
        "# })\n",
        "# dml_iivm_mlp1.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "#     'epochs': 30,\n",
        "#     'model__n_hidden': 16\n",
        "# })\n",
        "\n",
        "dml_iivm_mlp1.fit(store_predictions=True)\n",
        "mlp1_summary = dml_iivm_mlp1.summary\n",
        "\n",
        "# 2-layer MLP\n",
        "ml_g_mlp = KerasRegressor(\n",
        "    model=create_mlp_model2_regression,\n",
        "    n_features=n_features,\n",
        "    n_hidden1=64,      # or tune\n",
        "    n_hidden2=32,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_m_mlp = KerasClassifier(\n",
        "    model=create_mlp_model2_classification,\n",
        "    n_features=n_features,\n",
        "    n_hidden1=64,\n",
        "    n_hidden2=32,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "ml_r_mlp = KerasClassifier(\n",
        "    model=create_mlp_model2_classification,\n",
        "    n_features=n_features,\n",
        "    n_hidden1=64,\n",
        "    n_hidden2=32,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "np.random.seed(60615)\n",
        "dml_iivm_mlp2 = doubleml.DoubleMLIIVM(\n",
        "    data_dml_base_iv,\n",
        "    ml_g=ml_g_mlp,\n",
        "    ml_m=ml_m_mlp,\n",
        "    ml_r=ml_r_mlp,\n",
        "    subgroups={'always_takers': False,\n",
        "               'never_takers': True},\n",
        "    trimming_threshold=0.01,\n",
        "    n_folds=3\n",
        ")\n",
        "\n",
        "\n",
        "# (Optional) set nuisance-part-specific parameters:\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_g0', 'p401', {\n",
        "#     'epochs': 15,\n",
        "#     'model__learning_rate': 0.005\n",
        "# })\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_g1', 'p401', {\n",
        "#     'epochs': 25,\n",
        "#     'batch_size': 32\n",
        "# })\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_m', 'p401', {\n",
        "#     'model__dropout_rate': 0.3\n",
        "# })\n",
        "# dml_iivm_mlp.set_ml_nuisance_params('ml_r1', 'p401', {\n",
        "#     'epochs': 30,\n",
        "#     'model__n_hidden1': 128\n",
        "# })\n",
        "\n",
        "dml_iivm_mlp2.fit(store_predictions=True)\n",
        "mlp2_summary = dml_iivm_mlp2.summary"
      ],
      "metadata": {
        "id": "ymWgykW13KVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iivm_summary = pd.concat((lasso_summary, forest_summary, tree_summary, boost_summary, mlp1_summary, mlp2_summary))\n",
        "iivm_summary.index = ['lasso', 'forest', 'tree', 'xgboost', '1-layer mlp', '2-layer mlp']\n",
        "print(iivm_summary[['coef', '2.5 %', '97.5 %']])"
      ],
      "metadata": {
        "id": "J_szw9pw5ueK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.full((2, iivm_summary.shape[0]), np.nan)\n",
        "errors[0, :] = iivm_summary['coef'] - iivm_summary['2.5 %']\n",
        "errors[1, :] = iivm_summary['97.5 %'] - iivm_summary['coef']\n",
        "plt.errorbar(iivm_summary.index, iivm_summary.coef, fmt='o', yerr=errors)\n",
        "plt.ylim([0, 17500])\n",
        "\n",
        "plt.title('Interactive IV Model (IIVM)')\n",
        "plt.xlabel('ML method')\n",
        "_ = plt.ylabel('Coefficients and 95%-CI')"
      ],
      "metadata": {
        "id": "z9EApko3519k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Results\n",
        "\n",
        "To sum up, let's merge all our results so far and illustrate them in a plot."
      ],
      "metadata": {
        "id": "zgJ8636p6N5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary = pd.concat((plr_summary, irm_summary, iivm_summary)).reset_index().rename(columns={'index': 'ML'})\n",
        "df_summary['Model'] = np.concatenate((np.repeat('PLR', 6), np.repeat('IRM', 6), np.repeat('IIVM', 6)))\n",
        "print(df_summary.set_index(['Model', 'ML']))"
      ],
      "metadata": {
        "id": "J4peh7836R4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 15))\n",
        "for ind, model in enumerate(['PLR', 'IRM', 'IIVM']):\n",
        "    plt.subplot(3, 1, ind+1)\n",
        "    this_df = df_summary.query('Model == @model')\n",
        "    errors = np.full((2, this_df.shape[0]), np.nan)\n",
        "    errors[0, :] = this_df['coef'] - this_df['2.5 %']\n",
        "    errors[1, :] = this_df['97.5 %'] - this_df['coef']\n",
        "    plt.errorbar(this_df.ML, this_df.coef, fmt='o', yerr=errors,\n",
        "                 color=face_colors[ind], ecolor=face_colors[ind])\n",
        "    plt.ylim([0, 20000])\n",
        "    plt.title(model)\n",
        "    plt.ylabel('Coefficients and 95%-CI')\n",
        "\n",
        "_ = plt.xlabel('ML method')"
      ],
      "metadata": {
        "id": "FWBw7mOw8Pwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We report results based on six ML and DL methods for estimating the nuisance functions used in\n",
        "forming the orthogonal estimating equations. Except the single hidden-layer MLP, which did not converge, we find that the estimates of the treatment effect are stable across ML methods. The estimates are highly significant, hence we would reject the hypothesis\n",
        "that 401(k) participation has no effect on financial wealth."
      ],
      "metadata": {
        "id": "upN54sf28WKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The S-Learner, T-Learner, and TARNet for Heterogenous Treatment Effects Estimation"
      ],
      "metadata": {
        "id": "FLo2FDAREB_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to the paper (https://osf.io/preprints/socarxiv/aeszf)  regarding the core concepts in the notebook"
      ],
      "metadata": {
        "id": "61bv7_Q5XKZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Social scientists aim to understand how one variable causally influences another. Consider an example from Veitch et al., 2020: you have data from Reddit and want to determine how the author's stated gender (=T) influences the number of upvotes a post receives (=Y). There's a complexity, though: gender may influence the post's content (=X) — such as tone, style, or topics — which in turn can affect its upvotes. To isolate the direct causal impact of gender on upvotes, it's crucial to control for these content-related factors. Essentially, the goal is to discern if gender (=T) still affects upvotes (=Y) when assuming posts have similar content attributes."
      ],
      "metadata": {
        "id": "kc7SbC1h_ukH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypM4RTCdy7iV"
      },
      "source": [
        "\n",
        "The following tutorials are a gentle introduction to building deep learning models for causal inference using the selection on observables identification strategy. In particular, these model are designed to estimate the  average treatment effect (ATE) and the conditional average treatment effect (CATE). The ATE is defined as:\n",
        "\n",
        "$$ATE =\\mathbb{E}[Y(1)-Y(0)]$$\n",
        "\n",
        "where $Y(1)$ and $Y(0)$ are the potential outcomes had the unit received or not received the treatment, respectively. The CATE is defined as,\n",
        "\n",
        "$$CATE =\\mathbb{E}[Y(1)-Y(0)|X=x]$$\n",
        "\n",
        "where $X$ is the set of selected, observable covariates, and $x \\in X$.\n",
        "\n",
        "Because selection on observables is a simple identification strategy, these estimators are simple neural networks. This tutorial is thus also a gentle introduction to writing models in TensorFlow, and getting started coding deep learning models.\n",
        "\n",
        "**These tutorials are for you if:**\n",
        "\n",
        "1. You want a quick and dirty introduction to DL + Selection on Observables literature with minimal math, or...\n",
        "\n",
        "2. You want a gentle introduction to writing and training custom models in Tensorflow 2 and...\n",
        "\n",
        "3. You have a basic familiarity with causal inference and...\n",
        "\n",
        "4. You have a basic familiarity with Python and object oriented programming.\n",
        "\n",
        "**DISCLAIMER**: Before we get started, I want to make clear that the point of any of these pedagogical tutorials is not to argue that one of these models is empirically (or theoretically) straight-up superior to another. This is only one tiny simulation (from a [benchmark](https://arxiv.org/abs/2107.13346) that has itself been critiqued) without hyperparameter optimization. These are toy pedagogical programming examples, not benchmarking notebooks. We use the same specification and hyperparameters for all three models for comparison, but I'm sure you could get better results with each if you tweaked them. If you want to apply these to real data, you should try different things and do careful model selection.\n",
        "\n",
        "----\n",
        "\n",
        "## What are we doing here?\n",
        "\n",
        "These model are designed to estimate the  average treatment effect (ATE) and the conditional average treatment effect(CATE) under a selection on observables identification strategy. The ATE is defined as:\n",
        "\n",
        "$$ATE =\\mathbb{E}[Y_i(1)-Y_i(0)]= \\mathbb{E}[{\\tau_i}]$$\n",
        "\n",
        "where $Y_i(1)$ and $Y_i(0)$ are the potential outcomes had unit $i$ received or not received the treatment, respectively. The CATE is defined as,\n",
        "\n",
        "$$CATE(x) =\\mathbb{E}[Y_i(1)-Y_i(0)|X=x]$$\n",
        "\n",
        "where $X$ is the set of selected, observable covariates, and $x \\in X$.\n",
        "\n",
        "Because selection on observables is a simple identification strategy, these estimators are simple neural networks. This tutorial is thus also a gentle introduction to writing models in TensorFlow, and getting started coding deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8pWGN7_jn8n"
      },
      "source": [
        "## Why use deep learning for causal inference?\n",
        "\n",
        "1. Appropriately built neural network models are among the **lowest bias** estimators in our statistical arsenal.\n",
        "\n",
        "2. For similar reasons, the complex response surfaces learned by neural networks make them well-suited for estimating **heterogeneous treatment effects**.\n",
        "\n",
        "4. Most excitingly, deep learning has the ability to allow us to control for confounding found in **complex data types like images, text, and networks**.\n",
        "\n",
        "3. Although most of these models don't make theoretical guarantees, representation learning **might be more robust to empirical violations of overlap** than simpler adjustment strategies.\n",
        "\n",
        "One more point: even if we cannot formally satisfy causal inference assumptions, these architectures are still very useful for **creating interpretable ML models** where we can isolate the contributions of specific covariates to predicting the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTkpw_cxBq3H"
      },
      "source": [
        "## Notation\n",
        "**Causal identification**\n",
        "\n",
        "- Observed covariates/features: $X$\n",
        "\n",
        "- Potential outcomes: $Y(0)$ and $Y(1)$\n",
        "\n",
        "- Treatment: $T$\n",
        "\n",
        "- Average Treatment Effect: $ATE =\\mathbb{E}[Y(1)-Y(0)]$\n",
        "\n",
        "- Conditional Average Treatment Effect: $CATE =\\mathbb{E}[Y(1)-Y(0)|X=x]$\n",
        "\n",
        "**Deep learning estimation**\n",
        "\n",
        "- Predicted outcomes: $\\hat{Y}(0)$ and $\\hat{Y}(1)$\n",
        "\n",
        "- Outcome modeling functions: $\\hat{Y}(T)=h(X,T)$\n",
        "\n",
        "- Representation functions: $\\Phi(X)$ (producing representations $\\phi$)\n",
        "\n",
        "- Loss functions: $\\mathcal{L}(true,predicted)$, with the mean squared error abbreviated $MSE$ and binary cross-entropy as $BCE$\n",
        "\n",
        "- Estimated CATE: $\\hat{CATE}=(1-2t)(\\hat{{y}}(t)-\\hat{y}(1-t))$\n",
        "\n",
        "- Estimated ATE: $\\hat{ATE}=\\frac{1}{n}\\sum_{i=1}^n\\hat{CATE_i}$\n",
        "\n",
        "\n",
        "## Standard assumptions for causal identification under selection on observables\n",
        "Standard assumptions for model-based causal inference apply here (from [Johansson et al., 2020](https://arxiv.org/pdf/2001.07426.pdf)):\n",
        "1. **Conditional Ignorability/Exchangability**.The potential outcomes $Y(0)$, $Y(1)$ and the treatment $T$ are conditionally independent given $X$,\n",
        "$$Y(0),Y(1)\\perp \\!\\!\\! \\perp T|X $$\n",
        "Conditional gnorability specifies that there are no *unmeasured confounders* that affect both treatment and outcome outside of those in the observed covariates/features $X$.\n",
        "\n",
        "\n",
        "2. **Consistency/Stable Unit Treatment Value Assumption (SUTVA)**. Consistency specifies that when a unit recieves treatment, we observe the potential outcome. Moreover, the response of any unit does not vary with the treatment assignment to other units (i.e., no network effects), and the form/level of treatment is homogeneous and consistent across units,\n",
        "$$T=t \\rightarrow Y=Y(T)$$\n",
        "\n",
        "\n",
        "3. **Overlap** In any context $x \\in X$, any treatment $t\\in \\{0,1\\}$ has a non-zero probability of being observed in the data,\n",
        "\n",
        "$$\\forall x \\in X, t\\in\\{0,1\\}:p(T=t|X=x)>0$$\n",
        "\n",
        "Note that the overlap assumption does not require that the empirical data are necessarily balanced, but that the two treatment distributions have common support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVhJelhqCMD7"
      },
      "source": [
        "## Data\n",
        "\n",
        "The IHDP dataset used in this example is a naturalistic simulation introduced in [Hill, 2011](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162?casa_token=b8-rfzagECIAAAAA:QeP7C4lKN6nZ7MkDjJHFrEberXopD9M5qPBMeBqbk84mI_8qGxj01ctgt4jdZtORpu9aZvpVRe07PA) to evaluate estimation of heterogeneous treatment effects ($CATE$). The  25 covariates/features for the 747 units (139 treated) in the dataset were taken from an experiment, but Hill simulated the outcomes to create known counterfactuals. The data are available from Fredrik Johansson's website. IHDP is the de facto benchmark in this literature.\n",
        "\n",
        "<details><summary>Additional details from Hill, 2011</summary>\n",
        "<blockquote>[Hill] used experimental data from the Infant Health and Development Program (IHDP), a randomized experiment that began in 1985, targeted low-birth-weight, premature infants, and provided the treatment group with both intensive high-quality child care and home visits from a trained provider.... [The response surface] is nonlinear and not parallel across treatment conditions, with $Y(0)∼\\mathcal{N}(exp((X+W)\\beta_B),1)$ and $Y(1)∼\\mathcal{N}(X\\beta_B−\\omega^s_B,1)$, where $W$ is an offset matrix of the same dimension as $X$ with every value equal to 0.5, $\\beta_B$ is a vector of regression coefficients (0, 0.1, 0.2, 0.3, 0.4) randomly sampled with probabilities (0.6, 0.1, 0.1, 0.1,0.1). For the sth simulation, $\\omega^s_B$ was chosen in the overlap setting, where we estimate the effect of the treatment on the treated, such that theconditional average treatment effect for the treated equals 4.</blockquote>\n",
        "</details>\n",
        "\n",
        "`y` is the simulated outcome that may represent $Y(0)$ or $Y(1)$ depending on `t`. Note that we rescale it here to improve convergence. `mu_0` and `mu_1` are \"noiseless\" potential outcomes where Hill simply used the mean of the normal distribution described in the spoiler.\n",
        "\n",
        "There are 100 stochastic simulations in this data. For this example we will just use the eighth one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psz99l_n-9po"
      },
      "source": [
        "import numpy as np\n",
        "!pip install scikit-learn==0.24.2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
        "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz\n",
        "\n",
        "def load_IHDP_data(training_data,testing_data,i=7):\n",
        "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
        "        train_data=np.load(trf); test_data=np.load(tef)\n",
        "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
        "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
        "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
        "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
        "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
        "\n",
        "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
        "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n",
        "        data['y']=data['y'].reshape(-1,1)\n",
        "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
        "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
        "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
        "\n",
        "    return data\n",
        "\n",
        "data =load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz')\n",
        "\n",
        "#concatenate t so we can use it as input\n",
        "xt = np.concatenate([data['x'], data['t']], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7BlBgyZvQAG"
      },
      "source": [
        "`data` is a dictionary (equivalent to a list in R). I'll print out a bit for you to see what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdDs3weLvLc-"
      },
      "source": [
        "for key in data:\n",
        "  if key == 'y_scaler': continue\n",
        "  print(key, data[key][:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBX_76ms7yl"
      },
      "source": [
        "## Attempt Number 1: Pure Outcome Modeling (S[ingle]-Learner)\n",
        "\n",
        "As a means to get our feet wet, we're going to start with the simplest way to estimate $\\hat{CATE}$: a single multi-layer peceptron (sometimes called feed-forward or just deep neural network).\n",
        "\n",
        "<figure><img src=https://github.com/kochbj/Deep-Learning-for-Causal-Inference/blob/main/images/Slearner.png?raw=true width=\"900\"><figcaption><b>Fig 1: S-learner.</b> The S-learner is a deep feed-forward network or multilayer-percepetron. In a feed-forward neural network, additional fully connected (parameterized) layers of neurons are added between the inputs and output neuron. Purple indicates inputs, orange indicates network layers, and white indicates outputs. In this figure, the size of the hidden layers are first shown as nodes and then generically abstracted as boxes. The dashes between the orange shapes indicate an unspecifed number of additional hidden layers. The dashed lines on the right indicate non-gradient plug-in computations that occur after training. In causal inference settings, this architecture is sometimes called a S(ingle)-learner because one feed-forward network learns to predict both potential outcomes.</figcaption></figure>\n",
        "\n",
        "To make this feasible, the network will take $X$ and $T$ as input and predict $Y$.\n",
        "\n",
        "We'll label this function $h$ so $\\hat{Y}=h(X,T)$.\n",
        "\n",
        "\n",
        "We'll use the mean squared error as the loss,\n",
        "$$\\mathcal{L}(Y,h(X,T))=MSE(Y,h(X,T))=\\frac{1}{n}\\sum_{i=1}^n [h(x_i,t_i)-y_i]^2$$\n",
        "\n",
        "Let's start by importing packages...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_o2BBef1THI"
      },
      "source": [
        "!pip install -q tensorflow==2.8.0\n",
        "import tensorflow as tf\n",
        "import numpy as np #numpy is the numerical computing package in python\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKIJJarlyej2"
      },
      "source": [
        "The next block specifies a function to build the model using Tensorflow 2's **Sequential API**. The **Sequential API** is the simplest of three API's in Tensorflow (see this [post](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021) for pros and cons). Most of the tutorial will be taught in the more powerful functional API, but I wanted to show you this first.\n",
        "\n",
        "In words, this API is just taking a list of fully connected layers and creating an MLP. For now just ignore the other arguments beyond number of `units` in the layer; I'm just including all of these other specifications to make our S-learner comparable to our T-learner and TARNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx-UK5d-s63K"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "reg_l2=.01\n",
        "s_learner = tf.keras.models.Sequential([\n",
        "  Dense(units=200, activation='elu', kernel_initializer='RandomNormal'),\n",
        "  Dense(units=200, activation='elu', kernel_initializer='RandomNormal'),\n",
        "  Dense(units=200, activation='elu', kernel_initializer='RandomNormal'),\n",
        "  Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2)),\n",
        "  Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2)),\n",
        "  Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaWlWprwXrU-"
      },
      "source": [
        "Now let's specify the loss function..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfo2Gf21Xp-b"
      },
      "source": [
        "loss_fn = tf.keras.losses.MeanSquaredError() #specify the loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cbAbiP0Xz00"
      },
      "source": [
        "#@title Run this block. (Details we'll abstract away for now) { display-mode: \"form\" }\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "val_split=0.2\n",
        "batch_size=64\n",
        "verbose=1\n",
        "i = 0\n",
        "tf.random.set_seed(i)\n",
        "np.random.seed(i)\n",
        "\n",
        "sgd_callbacks = [\n",
        "        TerminateOnNaN(),\n",
        "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.),\n",
        "        #40 is Shi's recommendation for this dataset, but you should tune for your data\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
        "                          min_delta=0., cooldown=0, min_lr=0),\n",
        "    ]\n",
        "#optimzier hyperparameters\n",
        "sgd_lr = 1e-5\n",
        "momentum = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQATJ_7YPcL"
      },
      "source": [
        "Below are the two core methods we need to train our model.\n",
        "\n",
        "`compile` creates a static computational graph of your network for training. At a minimum, you need to give it an optimizer and a loss function.\n",
        "\n",
        "`fit` is your main training loop.\n",
        "\n",
        "We'll pass through our data up to 300 times (may be less due to regularization conditions we'll discuss later), using batch sizes of 64. We reserve 20% of the data for validation. Ignore `sgd_callbacks` for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQFp3PxQsIi"
      },
      "source": [
        "s_learner.compile(optimizer=SGD(learning_rate=sgd_lr, momentum=momentum, nesterov=True),\n",
        "                    loss=loss_fn,\n",
        "                    metrics=loss_fn)\n",
        "\n",
        "s_learner.fit(x=xt,y=data['ys'],\n",
        "                validation_split=.2,\n",
        "                epochs=300,\n",
        "                batch_size=64,\n",
        "                callbacks=sgd_callbacks,\n",
        "                verbose=1)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb_FVYIny1XC"
      },
      "source": [
        "### Estimating the ATE/CATE\n",
        "\n",
        "Now we can estimate causal effects in either the whole dataset or a heldout testing sample. For simplicity, we just use the whole dataset here.  This is unorthodox in machine learning, but here we are interested in inference, not prediction.\n",
        "\n",
        "Although our ultimate goal is to estimate the $CATE$, our loss function only minimizes the factual error to estimate $\\hat{Y}$. This is a reflection of the fundamental problem of causal inference: we only observe one potential outcome for each unit. To get the quantities we want, we'll have to artificially toggle the treatment to get both $\\hat{y}(t)$ and $\\hat{y}(1-t)$ for each unit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSV6g8uJuJio"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#create fake ones and zeros to feed network\n",
        "zeros=np.expand_dims(np.zeros(data['x'].shape[0]),1)\n",
        "ones=np.expand_dims(np.ones(data['x'].shape[0]),1)\n",
        "x_untreated = np.concatenate([data['x'], zeros], 1)\n",
        "x_treated = np.concatenate([data['x'], ones], 1)\n",
        "y0_pred_slearner=s_learner.predict(x_untreated)\n",
        "y1_pred_slearner=s_learner.predict(x_treated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKpbzzZaaYo9"
      },
      "source": [
        "We can then plug in our predictions $\\hat{Y}(0)$ and $\\hat{Y}(1)$ to calculate the predicted CATE as\n",
        "\n",
        "$$\\hat{CATE_i}=(1-2t_i)(\\hat{y_i}(t)-\\hat{y_i}(1-t))$$\n",
        "and the predicted average treatment effect as,\n",
        "$$\\hat{ATE}=\\frac{1}{n}\\sum_{i=1}^n\\hat{CATE_i}$$\n",
        "\n",
        "Since we know the true $CATE$s in our simulations, let's go over some commonly used evaluation metrics in this literature...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlhZs9NIbP0M"
      },
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "Within this literature, it is common practice to evaluate model performance on simulations using the Precision Estimation of Heterogeneous  Effects ($PEHE$) from [Hill, 2011](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162?casa_token=b8-rfzagECIAAAAA:QeP7C4lKN6nZ7MkDjJHFrEberXopD9M5qPBMeBqbk84mI_8qGxj01ctgt4jdZtORpu9aZvpVRe07PA). $PEHE$ measures the error in estimates of the $CATE$:\n",
        "\n",
        "$$\\sqrt{PEHE}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(CATE_i-\\hat{CATE_i})^2}$$\n",
        "\n",
        "The PEHE is more than just a metric, it has theoretical significance in literature in the definition of generalization bounds.\n",
        "\n",
        "Since we know both potential outcomes in simulation we might also like to calculate bias in $\\hat{ATE}$ and $\\hat{CATE}$,\n",
        "- $ATE_{bias} = |ATE-\\hat{ATE}|$\n",
        "- $CATE_{bias} = \\frac{1}{N}\\sum_{i=1}^N |CATE_i-\\hat{CATE_i}|$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS5RYt3F01ba"
      },
      "source": [
        "def plot_cates(y0_pred,y1_pred,data):\n",
        "  #dont forget to rescale the outcome before estimation!\n",
        "  y0_pred = data['y_scaler'].inverse_transform(y0_pred)\n",
        "  y1_pred = data['y_scaler'].inverse_transform(y1_pred)\n",
        "  cate_pred=(y1_pred-y0_pred).squeeze()\n",
        "  cate_true=(data['mu_1']-data['mu_0']).squeeze() #Hill's noiseless true values\n",
        "  ate_pred=tf.reduce_mean(cate_pred)\n",
        "\n",
        "  print(pd.Series(cate_pred).plot.kde(color='blue'))\n",
        "  print(pd.Series(cate_true).plot.kde(color='green'))\n",
        "\n",
        "  print(pd.Series(cate_true-cate_pred).plot.kde(color='red'))\n",
        "  pehe=tf.reduce_mean( tf.square( ( cate_true - cate_pred) ) )\n",
        "  sqrt_pehe=tf.sqrt(pehe).numpy()\n",
        "  print(\"\\nSQRT PEHE:\",sqrt_pehe)\n",
        "  print(\"Estimated ATE (True is 4):\", ate_pred.numpy(),'\\n\\n')\n",
        "\n",
        "  print(\"\\nError CATE Estimates: RED\")\n",
        "  print(\"Individualized CATE Estimates: BLUE\")\n",
        "  print(\"Individualized CATE True: GREEN\")\n",
        "  return sqrt_pehe,np.abs(ate_pred.numpy()-4)\n",
        "\n",
        "plot_cates(y0_pred_slearner,y1_pred_slearner,data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_BT_blacfvs"
      },
      "source": [
        "### Analysis of S-learner results\n",
        "\n",
        "Before we move on, let's actually LOOK at our results. It's clear that this model ran for 300 epochs and didn't learn anything at all! Take this finding with a major grain of salt; if you played around with the optimizer, capacity (number of layers and neurons), and other settings you might get this model to converge. This would be a good exercise! But do look at the disclaimer above. Let's keep going and see if we can find something more interesting.\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlAkk_DKtjSC"
      },
      "source": [
        "## Attempt Number 2: T-Learner\n",
        "\n",
        "The next most sophisticated thing we could do is fit two outcomes models independently, one for $Y(0)$ and one for $Y(1)$. This network is called a T-Learner.\n",
        "<figure><img src=https://github.com/kochbj/Deep-Learning-for-Causal-Inference/blob/main/images/TLearner.png?raw=true width=\"900\"><figcaption><b>Fig 2: T-learner.</b> The T-learner consists of two independent feed-forward networks learning the two different potential outcomes. Purple indicates inputs, orange indicates network layers, and white indicates outputs. The dashes between colored shapes indicate an unspecifed number of additional hidden layers. The dashed lines on the right indicate non-gradient, plug-in computations that occur after training.</figcaption></figure>\n",
        "\n",
        "It would be pretty easy to implement the T-Learner as we did above using two Sequential API models. Instead we are going to transition to the **Functional API.** The Functional API is keras before it was absorbed into TF2. It is much easier to write complex models with the Functional API or Imperative (OOP) API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kS9vHSO2fsq"
      },
      "source": [
        "### Coding up T-Learner\n",
        "\n",
        "Okay, let's build the model! The rest of this tutorial basically modifies Claudia Shi's beautiful [implementation](https://github.com/claudiashi57/dragonnet) of TARNet from her [DragonNet paper](https://arxiv.org/pdf/1906.02120.pdf) (featured in a subsequent tutorial).\n",
        "\n",
        "It's idiomatic in the functional API to declare a layer and immediately pass it's inputs so you can follow the forward-pass through the network. The only\n",
        "layers that should be unfamiliar to you are the `Input` layer and `Concatenate` layer. Every graph is required to have an input layer to specify the dimensions of the input before compilation. `Concatenate` is just one of several utility layers in the API.\n",
        "\n",
        "The model itself is still pretty simple: we use two output layers for each head with 100 neurons each. There are again a couple ways to have multiple outputs in the Functional API, but here we concatenate the two outputs into a list of vectors. We apply regularization to the output heads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbB8VikMzdkS"
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "def make_tlearner(input_dim, reg_l2):\n",
        "    '''\n",
        "    The first argument is the column dimension of our data.\n",
        "    It needs to be specified because the functional API creates a static computational graph\n",
        "    The second argument is the strength of regularization we'll apply to the output layers\n",
        "    '''\n",
        "    x = Input(shape=(input_dim,), name='input')\n",
        "\n",
        "    #in TF2/Keras it is idiomatic to instantiate a layer and pass its inputs on the same line unless the layer will be reused\n",
        "    # HYPOTHESIS\n",
        "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(x)\n",
        "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(x)\n",
        "\n",
        "    # second layer\n",
        "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
        "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
        "\n",
        "    # third\n",
        "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
        "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
        "\n",
        "    #a convenience \"layer\" that concatenates arrays as columns in a matrix\n",
        "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions])\n",
        "    #the declarations above have specified the computational graph of our network, now we instantiate it\n",
        "    model = Model(inputs=x, outputs=concat_pred)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdagu-s95Gd1"
      },
      "source": [
        "The `summary` method can be used to confirm that the architecture is specified correctly.\n",
        "\n",
        "One of the advantages of the functional API is that you can also visualize static computational graphs (very similar to the cartoon representation above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXtL9ccg5Gd2"
      },
      "source": [
        "tlearner_model=make_tlearner(25,.01)\n",
        "\n",
        "print(tlearner_model.summary())\n",
        "tf.keras.utils.plot_model(tlearner_model, show_shapes=True, show_layer_names=True, to_file='tlearner.png')\n",
        "\n",
        "from IPython.display import Image # this just Jupyter notebook stuff\n",
        "Image(retina=True, filename='tlearner.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc96gKmHDcaY"
      },
      "source": [
        "\n",
        "<font color='red'><h2>Check Your Understanding:</h2></font>\n",
        "\n",
        "What will happen if we use the same loss function as above?\n",
        "\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "The two heads are getting the same input data so they will be calculating the same gradients and will end up learning the same thing. We have two options:\n",
        "\n",
        "A. Feed the two networks different data. This would essentially mean training the two networks independently or restructuring our data so that batch sizes of treated and control units can be split equally after input.\n",
        "\n",
        "B. Somehow ensure that each head only receives error gradients for the correct treatment group. This will require writing a custom loss function.\n",
        "\n",
        "Let's go with B.\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCQf10Qm7zaS"
      },
      "source": [
        "### Specifying the loss function\n",
        "There are again at least four different ways to specify loss functions in Tensorflow2: if you have a standard loss there are built-in options (as above), you can specify them as custom functions, custom objects, or build them into custom layers of your network. Here we've written a function.\n",
        "\n",
        " Note that we compute $\\mathcal{L}(Y(0),h(X,0))$ and $\\mathcal{L}(Y(1),h(X,1))$ separately and just add them to get the whole loss. Tensorflow will apply the gradients appropriately to the different outcome and representation layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEp-MzSX7zaS"
      },
      "source": [
        "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
        "def regression_loss(concat_true, concat_pred):\n",
        "    #computes a standard MSE loss for TARNet\n",
        "    y_true = concat_true[:, 0] #get individual vectors\n",
        "    t_true = concat_true[:, 1]\n",
        "\n",
        "    y0_pred = concat_pred[:, 0]\n",
        "    y1_pred = concat_pred[:, 1]\n",
        "\n",
        "    #Each head outputs a prediction for both potential outcomes\n",
        "    #We use t_true as a switch to only calculate the factual loss\n",
        "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
        "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
        "    #note Shi uses tf.reduce_sum for her losses instead of tf.reduce_mean.\n",
        "    #They should be equivalent but it's possible that having larger gradients accelerates convergence.\n",
        "    #You can always try changing it!\n",
        "    return loss0 + loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuZXExdr9ZFK"
      },
      "source": [
        "### Training and Fitting the Model\n",
        "\n",
        "\n",
        "<details><summary>A brief spoiler about training neural networks if you've never done so before.</summary>\n",
        "\n",
        "When you use other types of machine learning models, optimization of the model parameters is typically done for you under the hood and you simply wait for training to finish. In contrast, neural networks have so many parameters that optimization becomes an art.\n",
        "\n",
        "Rather than training on the whole training dataset at once, neural networks are trained on mini-batches of dozens to a few hundred examples. This is a compromise between applying error gradients from a single example (computationally expensive) and using the whole training dataset (expensive in terms of memory; may not work as well for losses that are not perfectly convex). The error gradient is applied to the network parameters after each mini-batch. A complete iteration through all mini-batches in the training set is called an **epoch.**\n",
        "\n",
        "After each epoch we run prediction on the entire validation set. While there are a number of regularization techniques used in DL to prevent overfitting (norms, dropout, batch normalization), the most important is **early stopping.** To prevent overfitting, we wish to stop training after several consecutive epochs where the validation loss has failed to improve. The number of epochs to wait after early stopping is often called a *patience* hyperparameter.\n",
        "\n",
        "The proportion of the gradient the optimizer backpropagates to the parameters is called the **learning rate.** A learning rate that is too small takes a long time to train. A learning rate that is too large will overshoot optima. Learning rate schedulers are used to adaptively slow the learning rate as you get closer to an optimum.\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "We will continue to use the builtin Keras `.fit` infrastructure for training the model which makes things super easy (you can of course write the training loop and calculate the gradients yourself). There are a lot of hyperparameter choices here, but I won't dwell on them because hyperparameter selection will be covered in the next tutorial.\n",
        "\n",
        "Now let's get to the details I hid from you above!\n",
        "\n",
        " In this example we use stochastic gradient descent to optimize the model with an initial learning rate of 1E-4 and momentum of .9. You can also try other optimizers (e.g., ADAM). **While you should experiment with different learning rates, I recommend having a conservative (smaller) learning rate because we really want our estimator to be unbiased.**\n",
        "\n",
        " To avoid overfitting, we stop training deep learning models when the validation loss stops improving. In Tensorflow the `EarlyStopping` callback automatically stops training after a number of epochs with no improvement on the validation loss (`patience` parameter). The `ReduceLROnPlateau` adaptively lowers the learning rate of the optimizer as we approach validation loss plateaus so that the optimizer does not overshoot the current optimum.\n",
        "\n",
        "We use a mini-batch size of 64. Other papers have recommmended batch sizes up to 200 with this dataset. **The batch size is an important consideration for these causal inference architectures because you really want to make sure each mini-batch has both treatment and control examples for the representation layers.** This is obviously less of a problem for datasets with high proportions of treated units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-PVlqY89ZFL"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "val_split=0.2\n",
        "batch_size=64\n",
        "verbose=1\n",
        "i = 0\n",
        "tf.random.set_seed(i)\n",
        "np.random.seed(i)\n",
        "yt = np.concatenate([data['ys'], data['t']], 1) #we'll use both y and t to compute the loss\n",
        "\n",
        "\n",
        "sgd_callbacks = [\n",
        "        TerminateOnNaN(),\n",
        "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.),\n",
        "        #40 is Shi's recommendation for this dataset, but you should tune for your data\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
        "                          min_delta=0., cooldown=0, min_lr=0),\n",
        "    ]\n",
        "#optimzier hyperparameters\n",
        "sgd_lr = 1e-5\n",
        "momentum = 0.9\n",
        "tlearner_model.compile(optimizer=SGD(learning_rate=sgd_lr, momentum=momentum, nesterov=True),\n",
        "                    loss=regression_loss,\n",
        "                    metrics=regression_loss)\n",
        "\n",
        "tlearner_model.fit(x=data['x'],y=yt,\n",
        "                callbacks=sgd_callbacks,\n",
        "                validation_split=val_split,\n",
        "                epochs=300,\n",
        "                batch_size=batch_size,\n",
        "                verbose=verbose)\n",
        "print(\"DONE!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDWDNVd-eWYk"
      },
      "source": [
        "Great! Let's calculate our causal effects and see what we get..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hinfsprNKdt0"
      },
      "source": [
        "concat_pred=tlearner_model.predict(data['x'])\n",
        "#dont forget to rescale the outcome before estimation!\n",
        "y0_pred_tlearner,y1_pred_tlearner = concat_pred[:, 0],concat_pred[:, 1]\n",
        "plot_cates(y0_pred_tlearner,y1_pred_tlearner,data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAFxc6YiIPGu"
      },
      "source": [
        "### Analyzing the T-learner results\n",
        "\n",
        "This model actually converged! The $ATE_{pred}$ is around 3.54 and $\\sqrt{PEHE}$ should be around .95 We can see that the estimated $CATE$ distribution has some bias towards lower treatment effects.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2vI_6nYfEX9"
      },
      "source": [
        "## Attempt 3: Representation learning as a balancing strategy (TARNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKyjje9UDs2j"
      },
      "source": [
        "### Representation learning\n",
        "\n",
        "A core concept in deep learning is the idea that artificial neural networks have the capacity to project a set of complex features $X$ into a useful vector space. When data are transformed into this space, we call the resulting tensor a **representation** ([Goodfellow, et al. 2016](https://www.deeplearningbook.org/contents/representation.html)) (you might also see the term \"embedding\"). For social scientists most comfortable with linear models, we can think about the parameters in each feed-forward layer of a deep neural network as capturing every possible interaction between the values produced by the previous layer. Tasking the network to minimize error on a relevant downstream task encourages it to adjust these interaction parameters to learn useful representations. We can also think about these representation layers as automatically extracting useful  latent covariates/features.\n",
        "\n",
        "The key intuition in this literature is that we want to train neural networks to learn a representation function $\\Phi(X)$ where the data are deconfounded/balanced in the representation space. In other words, the distributions of the representations $\\Phi(X|T=0)$ and $\\Phi(X|T=1)$ are similar.\n",
        "\n",
        "<figure><img src=https://github.com/kochbj/Deep-Learning-for-Causal-Inference/blob/main/images/balancing.png?raw=true width=\"900\"><figcaption><a href=https://github.com/maxwshen/iap-cidl>From Shen and Johansson talk 2018</a></figcaption></figure>\n",
        "\n",
        "Note that $\\Phi$ must, in theory, be an invertible function for the  ignorability and overlap assumptions to hold. By invertible we mean that there is an inverse function such that $\\Phi^{-1}(\\Phi(X))=X$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfgwjo_WUESS"
      },
      "source": [
        "### TARNet\n",
        "To encourage balanced representations, [Shalit et al., 2017](http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf) propose a simple two-headed neural network called Treatment Agnostic Regression Network (TARNet). Each head models a separate outcome. One head learns the function $\\hat{Y}(1)=h(\\Phi(X),1)$, and the other head learns the function $\\hat{Y}(0)=h(\\Phi(X),0)$. Both heads backpropagate their gradients to shared representation layers that learn $\\Phi(X)$. Again, the hope is that these representation layers will learn to balance the data because they are used to predict both outcomes.\n",
        "\n",
        "<figure><img src=https://github.com/kochbj/Deep-Learning-for-Causal-Inference/blob/main/images/TARNet.png?raw=true width=\"900\"><figcaption><b>Fig 3: TARNet.</b> This architecture, originally introduced in <a href=http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf>Shalit et al., 2017</a>, is a T-learner with shared representation layers. Purple indicates inputs, orange indicates network layers, other colors indicate output layers, and white indicates outputs. The dashes between colored shapes indicate an unspecifed number of additional hidden layers. The dashed lines on the right indicate non-gradient, plug-in computations that occur after training.</figcaption></figure>\n",
        "\n",
        "\n",
        "Other than this architectural change, this has the same loss as the T-Learner:\n",
        "\n",
        "$$\\mathcal{L}(Y,h(\\Phi(X),T))=MSE(Y,h(\\Phi(X),T))=\\frac{1}{n}\\sum_{i=1}^n [h(\\Phi(x_i),t_i)-y_i(t_i)]^2$$\n",
        "\n",
        " The complete objective for the network is to minimize the parameters of $h$ and $\\Phi$ for all $n$ units in the training sample such that,\n",
        "\n",
        "\\begin{equation}\n",
        "\\min_{h,\\Phi}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(y_i(t_i),h(\\Phi(x_i),t_i)) + \\lambda \\mathcal{R}(h)\\end{equation}\n",
        "\n",
        "where $\\mathcal{R}(h)$ is a model complexity term (e.g., for $L_2$ regularization) and $\\lambda$ is a hyperparameter chosen by the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUrXNmiAG7zk"
      },
      "source": [
        "### Coding up TARNet\n",
        "\n",
        "Okay, let's build the model!\n",
        "\n",
        "\n",
        "<font color='red'><h2>Check Your Understanding:</h2></font>\n",
        "\n",
        "How can you modify the T-Learner to make it TARNet?\n",
        "\n",
        "If you want to see the answer you can double click on the hidden TARNet block below. **Note that even if you don't want to look at the code, you need to run this block to proceed!**\n",
        "\n",
        "You can put your attempt in the empty code block below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBUUXoChhzik"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKp8LczJYDAF"
      },
      "source": [
        "#@title <font color='red'>Answer:</font> Full TARNet model (Definitely read this in detail and run it!) { display-mode: \"form\" }\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "def make_tarnet(input_dim, reg_l2):\n",
        "    '''\n",
        "    The first argument is the column dimension of our data.\n",
        "    It needs to be specified because the functional API creates a static computational graph\n",
        "    The second argument is the strength of regularization we'll apply to the output layers\n",
        "    '''\n",
        "    x = Input(shape=(input_dim,), name='input')\n",
        "\n",
        "    # REPRESENTATION\n",
        "    #in TF2/Keras it is idiomatic to instantiate a layer and pass its inputs on the same line unless the layer will be reused\n",
        "    #Note that we apply no regularization to the representation layers\n",
        "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
        "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
        "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
        "\n",
        "    # HYPOTHESIS\n",
        "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
        "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
        "\n",
        "    # second layer\n",
        "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
        "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
        "\n",
        "    # third\n",
        "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
        "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
        "\n",
        "    #a convenience \"layer\" that concatenates arrays as columns in a matrix\n",
        "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions])\n",
        "    #the declarations above have specified the computational graph of our network, now we instantiate it\n",
        "    model = Model(inputs=x, outputs=concat_pred)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52IF4BLWidMG"
      },
      "source": [
        "Cool! Let's check out what the model looks like..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxvD3WAwZD47"
      },
      "source": [
        "tarnet_model=make_tarnet(25,.01)\n",
        "\n",
        "print(tarnet_model.summary())\n",
        "tf.keras.utils.plot_model(tarnet_model, show_shapes=True, show_layer_names=True, to_file='tarnet.png')\n",
        "\n",
        "from IPython.display import Image # this just Jupyter notebook stuff\n",
        "Image(retina=True, filename='tarnet.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTZ6WxJPiGGz"
      },
      "source": [
        "### Aside: Imperative/Object Oriented Implementation\n",
        "\n",
        "If you prefer OOP or would like to see what this model might look like in Pytorch you can check out the spoiler below...\n",
        "\n",
        "<details><summary>Imperative API Implementation</summary>\n",
        "\n",
        " The same model above might look something like this in the imperative API:\n",
        "```python\n",
        "class TarNet(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 name='tarnet',\n",
        "                 regularization=.01,\n",
        "                 **kwargs):\n",
        "        super(TarNet, self).__init__(name=name, **kwargs)\n",
        "        self.encoder1=Dense(units=200, activation='elu', kernel_initializer='RandomNormal')\n",
        "        self.encoder2=Dense(units=200, activation='elu', kernel_initializer='RandomNormal')\n",
        "        self.encoder3=Dense(units=200, activation='elu', kernel_initializer='RandomNormal')\n",
        "\n",
        "        self.regressor1_y0 = Dense(units=100, activation='elu', kernel_regularizer=tf.keras.regularizers.l2(regularization))\n",
        "        self.regressor2_y0 = Dense(units=100, activation='elu', kernel_regularizer=tf.keras.regularizers.l2(regularization))\n",
        "        self.regressorO_y0 = Dense(units=1, activation=None, kernel_regularizer=tf.keras.regularizers.l2(regularization))\n",
        "\n",
        "        self.regressor1_y1 = Dense(units=100, activation='elu', kernel_regularizer=tf.keras.regularizers.l2(regularization))\n",
        "        self.regressor2_y1 = Dense(units=100, activation='elu', kernel_regularizer=tf.keras.regularizers.l2(regularization))\n",
        "        self.regressorO_y1 = Dense(units=1, activation=None, kernel_regularizer=tf.keras.regularizers.l2(regularization))\n",
        "\n",
        "\n",
        "    def call(self,inputs):\n",
        "        x=self.encoder1(inputs)\n",
        "        x=self.encoder2(x)\n",
        "        phi=self.encoder3(x)\n",
        "\n",
        "        out_y0=self.regressor1_y0(phi)\n",
        "        out_y0=self.regressor2_y0(out_y0)\n",
        "        y0=self.regressorO_y0(out_y0)\n",
        "\n",
        "        out_y1=self.regressor1_y1(phi)\n",
        "        out_y1=self.regressor2_y1(out_y1)\n",
        "        y1=self.regressorO_y1(out_y1)\n",
        "\n",
        "        concat=tf.concat([y0,y1,propensity],axis=1)\n",
        "        return concat\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixInwwcKmMfO"
      },
      "source": [
        "### Training and Fitting the Model\n",
        "\n",
        "Last time, we used a built-in MSE loss function, but this time we'll write it from scratch as a function. This is good practice for future tutorials."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
        "def regression_loss(concat_true, concat_pred):\n",
        "    #computes a standard MSE loss for TARNet\n",
        "    y_true = concat_true[:, 0] #get individual vectors\n",
        "    t_true = concat_true[:, 1]\n",
        "\n",
        "    y0_pred = concat_pred[:, 0]\n",
        "    y1_pred = concat_pred[:, 1]\n",
        "\n",
        "    #Each head outputs a prediction for both potential outcomes\n",
        "    #We use t_true as a switch to only calculate the factual loss\n",
        "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
        "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
        "    #note Shi uses tf.reduce_sum for her losses instead of tf.reduce_mean.\n",
        "    #They should be equivalent but it's possible that having larger gradients accelerates convergence.\n",
        "    #You can always try changing it!\n",
        "    return loss0 + loss1"
      ],
      "metadata": {
        "id": "_te9Mt5o3bH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll compile and train the model as we did for the T-learner."
      ],
      "metadata": {
        "id": "5SDTEHcg3okU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZQB19mL4sON"
      },
      "source": [
        "tarnet_model.compile(optimizer=SGD(learning_rate=sgd_lr, momentum=momentum, nesterov=True),\n",
        "                    loss=regression_loss,\n",
        "                    metrics=regression_loss)\n",
        "\n",
        "tarnet_model.fit(x=data['x'],y=yt,\n",
        "                callbacks=sgd_callbacks,\n",
        "                validation_split=val_split,\n",
        "                epochs=300,\n",
        "                batch_size=batch_size,\n",
        "                verbose=verbose)\n",
        "print(\"DONE!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cheUT_xjAMa"
      },
      "source": [
        "### Estimating the ATE/CATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQatSe6YSTfj"
      },
      "source": [
        "concat_pred=tarnet_model.predict(data['x'])\n",
        "#dont forget to rescale the outcome before estimation!\n",
        "y0_pred_tarnet,y1_pred_tarnet = concat_pred[:, 0],concat_pred[:, 1]\n",
        "plot_cates(y0_pred_tarnet,y1_pred_tarnet,data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWKcDkN1GPW"
      },
      "source": [
        "### Analyzing TARNet Results\n",
        "Compared to the T-learner, the distribution of predicted $CATE$s visually appears to be less biased. The $ATE$ and $\\sqrt{PEHE}$ estimates are slightly more accurate as well. In the next tutorial, we'll focus on hyperparameter optimization to further zero in our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIiEGZEO2i1o"
      },
      "source": [
        "### Exploring Heterogeneity\n",
        "\n",
        "Of course we can also break down these heterogeneous treatment effects to see if we can find any interesting patterns using, for example, Google's [Facet Dive](https://pair-code.github.io/facets/). This is just demonstrative since our covariates are meaningless in the simulation, but it's still cool. The Facet Dive is now built into TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDKuTeP-2i1t"
      },
      "source": [
        "#@title Explore Heterogeneity Using the Facet Dive\n",
        "\n",
        "data['cate_pred']=y1_pred_tarnet-y0_pred_tarnet\n",
        "facet_df=pd.DataFrame(data['x'])\n",
        "facet_df['t']=data['t']\n",
        "facet_df['y']=data['y']\n",
        "facet_df['cate_pred']=data['cate_pred']\n",
        "\n",
        "\n",
        "# Display the Dive visualization for the training data.\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "jsonstr = facet_df.to_json(orient='records')\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
        "        <script>\n",
        "          var data = {jsonstr};\n",
        "          document.querySelector(\"#elem\").data = data;\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(jsonstr=jsonstr)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG3IM93059Qx"
      },
      "source": [
        "# Thats it!\n",
        "\n",
        "- We first learned the motivations of double/debiased learning method for causal estimation based on a DGP example.\n",
        "\n",
        "- Based on the 401(k) dataset, we applied `DoubleML` for ATE and LATE estimation with six different types of ML/DL models.\n",
        "\n",
        "- In this tutorial we wrote some simple causal estimators starting with the S-learner, moving up to the T-learner, and ending with TARNet.\n",
        "\n",
        "- We learned how to write custom models using the sequential and functional APIs in TF2, as well as custom losses.\n",
        "\n",
        "- We built a TARNet model and tested it on the IHDP data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBo2i85qL-7Y"
      },
      "source": [
        "# Homework\n",
        "\n",
        "In this notebook, we saw multiple ways to leverage neural networks for causal inference. In this homework you will explore the application of neural networks with your own data and questions regarding causal relationships.\n",
        "\n",
        "**1)** Define key variables - T (treatment), Y (outcome), and X (covariates) - in your question about a causal relationship. For instance, if you want to determine how the author's stated gender influences the number of upvotes a post receives, T would be gender, the number of upvotes would be Y, and post's tone, style, or topics might be X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6UOXGOiMWwa"
      },
      "outputs": [],
      "source": [
        "T = 'value' #@param {type:\"string\"}\n",
        "Y = 'value' #@param {type:\"string\"}\n",
        "X = 'value' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvVzdCRMezi"
      },
      "source": [
        "**2)** How would you use deep learning for the causal inference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRs7LF7VMkN1"
      },
      "outputs": [],
      "source": [
        "causal_inference = 'value' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxK5Gki7Mktg"
      },
      "source": [
        "**3)** Using your own dataset, implement deep learning-based causal inference in any form in a way that is related to your project or research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmtVREscMwKX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4)** Interpret how the results from 3 support or reject your hypothesis about the causal relationship."
      ],
      "metadata": {
        "id": "QRpuSeG8X5H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpretation = 'value' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "0JGah4KIYDBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}