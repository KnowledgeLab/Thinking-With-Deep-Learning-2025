{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Thinking with Deep Learning: Week 2 Homework Modules**\n",
        "\n",
        "\n",
        "- __Instructor:__ James Evans\n",
        "\n",
        "- __Notebook Author & TAs:__ Bhargav Srinivasa Desikan, Shiyang, Avi\n",
        "\n",
        "\n",
        "__Perform 3 out of this week's following 4 modules:__\n",
        "\n",
        "##1. [**Basic Feedforward Neural Network Variations**](https://colab.research.google.com/github/KnowledgeLab/Thinking-With-Deep-Learning-2025/blob/main/Tutorials-Homework_Notebooks/Week%202/Week_2_2025.ipynb#scrollTo=_yrDxW32Fyqg)\n",
        "\n",
        "### **Summary:** Feed Forward networks or Multi-layer perceptrons (MLPs) are simple neural networks where each neuron in a layer connects to every neuron in the subsequent layer. For classification tasks, the output layer's size corresponds to the number of classes and often uses a softmax function to generate probability distributions. For regression tasks, the network typically uses linear or ReLU activation functions to output a single real number prediction. We explore this basic architecture and variations with data.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Build a neural network model implemented in this notebook for a dataset of your choice. You can use the model to perform a predictive task, such as classification or numerical prediction.\n",
        "\n",
        "**2)** Use this model to extract features from your data-points. Once you extract these features, convert these to a low dimensional space and visualise them (e.g PCA, t-SNE, UMAP).\n",
        "\n",
        "##2. [**Optimizing Neural Networks**](https://colab.research.google.com/github/KnowledgeLab/Thinking-With-Deep-Learning-2025/blob/main/Tutorials-Homework_Notebooks/Week%202/Week_2_2025.ipynb#scrollTo=9smLXu7gjd2y)\n",
        "\n",
        "### **Summary:** Neural networks are optimized through an iterative process of adjusting their weights to improve the accuracy of predictions and classifications. The primary goal during optimization is to minimize the error between the network's predictions and the actual target values. This process relies on various techniques and algorithms to systematically update the weights based on how well the network is performing.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Build or reuse an older model you have built, and use 3 different optimization techniques or methods on the model.\n",
        "\n",
        "**2)** Which optimizer performed the best? How did each method effect your model performance on in-sample training data and out-of-sample test data?\n",
        "\n",
        "##3. [**Regularizing Neural Networks**](https://colab.research.google.com/github/KnowledgeLab/Thinking-With-Deep-Learning-2025/blob/main/Tutorials-Homework_Notebooks/Week%202/Week_2_2025.ipynb#scrollTo=M4CEq6DLjd3B&line=1&uniqifier=1)\n",
        "\n",
        "### **Summary:** Regularizing neural networks involves implementing techniques to prevent overfitting and improve the model's ability to generalize to new data. These techniques help ensure the network learns meaningful patterns rather than memorizing the training data, ultimately leading to better performance on unseen examples.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Build or reuse an older model you have built, and use 3 different regularization approaches on the model.\n",
        "\n",
        "**2)** Which regularization approach performed the best? How did each effect your model performance on out-of-sample test data?\n",
        "\n",
        "##4. [**Fine-tuning Neural Networks**](https://colab.research.google.com/github/KnowledgeLab/Thinking-With-Deep-Learning-2025/blob/main/Tutorials-Homework_Notebooks/Week%202/Week_2_2025.ipynb#scrollTo=8DVd5yiyCMsN)\n",
        "\n",
        "### **Summary:**Fine-tuning neural networks involves taking a pre-trained model and adjusting its parameters allowing it to perform well on a specific task or dataset. This process sometimes involves (1) retraining all model weights, (2) unfreezing selected layers of the network and training them with a low learning rate on the new data, while keeping other layers frozen to preserve useful features learned during initial training, or (3) building adapators that translate the model weights through low-rank approaches, the purpose of this section. Fine-tuning is particularly valuable when working with limited data or computational resources, as it leverages knowledge transferred from the pre-trained model while allowing adaptation to the target task.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "**1)**Adapt an LLM (e.g., Galactical used above) with LoRA to your text and evaluate its perplexity before and after adaptation.\n",
        "\n",
        "**2)**QLoRA: Load a 4-bit quantized model (via bitsandbytes), apply LoRA, and fine-tune. Compare GPU memory usage, speed, and resulting perplexity with the standard LoRA approach above.\n",
        "\n",
        "**3)**Hyperparameter Tuning: Adjust r, lora_alpha, lora_dropout, or the learning rate.\n",
        "\n",
        "**4)**Regularization: Introduce weight decay, dropout in LoRA layers, or regularization of adapter weights.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9warZnyKmQsm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YC8qSj6l6ll",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown Mark the Modules you completed\n",
        "Basic Feedforward Neural Network Variations = False  # @param {type:\"boolean\"}\n",
        "Optimizing Neural Networks = False  # @param {type:\"boolean\"}\n",
        "Regularizing Neural Networks = False  # @param {type:\"boolean\"}\n",
        "Fine-tuning Neural Networks = False  # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrDxW32Fyqg"
      },
      "source": [
        "## Module 1: Basic Feedforward Neural Network Variations\n",
        "\n",
        "**Summary: Feed Forward networks or Multi-layer perceptrons (MLPs) are simple neural networks where each neuron in a layer connects to every neuron in the subsequent layer. For classification tasks, the output layer's size corresponds to the number of classes and often uses a softmax function to generate probability distributions. For regression tasks, the network typically uses linear or ReLU activation functions to output a single real number prediction. We explore this basic architecture and variations with data.**\n",
        "\n",
        "We saw these back in Notebook 1. Adding multiple layers, we have fully-connected,  feed-forward NNs, where the output of each neuron in the layer above is directed to every neuron in the layer below. The number of neurons in the input layer is the same as the size of the data. The size of the output layer is set to the number of classes and often provides a probability distribution over the classes by passing the neurons through a softmax function. In a regression setting, the activation function is often a simple linear function or ReLU, and the prediction is a real number instead of a probability distribution and class.\n",
        "\n",
        "In the code below, we create a single layer network in PyTorch, as well as scikit-learn. We do not train these models (refer to [Tut 1.2](https://colab.research.google.com/drive/18NT8eyzhlDN9h-lb6svRsbxJ6r3tPNRU?usp=sharing) for basic training).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2XVU-m5Fyql"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1DRBMLcFyql"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(200, 200)\n",
        "        # Output layer, 1 output for the classification\n",
        "        self.output = nn.Linear(200, 1)\n",
        "        # Define sigmoid activation and softmax output\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFIqa2rMFyql"
      },
      "outputs": [],
      "source": [
        "net = Network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V34oLbthFyql"
      },
      "outputs": [],
      "source": [
        "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uniCVhZeFyql"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VrsRDwuFyql"
      },
      "source": [
        "#### scikit-learn\n",
        "\n",
        "This is just to show a comparison, as the simplest neural network model. Scikit-learn focuses on machine learning and not deep learning, but the multi-layer perceptron is included as a basic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXQsTHHPFyql"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(200, ), activation='relu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exkUL4A0Fyqm"
      },
      "source": [
        "## Deep Neural Models\n",
        "\n",
        "Another network you have already seen in the last chapter, a deep feed forward network is one where we have (usually) more than 2 layers of nodes which are fully connected. The crucial distinction here is that while a shallow network could get away with not needing to backpropagate values through the network, a deep network must crucially be able to do this. It is with the Stochastic Gradient Descent powered backpropagation of values that we are able to change our weights and make DNNs useful, and since then, different kinds of DNNs have been the state of the art.\n",
        "\n",
        "The last time we saw deep neural models, we added a single layer to our shallow network (refer to [tut 1.2](https://colab.research.google.com/github/KnowledgeLab/thinking_with_deep_learning2025/blob/main/Tutorials-Homework_Notebooks/Week%201/Tutorial_2_Intro_NNs.ipynb).)\n",
        "\n",
        "**IMPORTANT NOTE**: this sort of model includes many new topics, so you may not understand every aspect of this model right away. What is important to get from this example, however, is the kind of data we are feeding into the model, the parameters of the model, and the task. In this case, the setting is diverse dataset of forest cover type, using both numerical features and categorical features, and predicting a label over multiple classes. You will often find such scenarios in social sciences datasets, such as census data. Notice the way the data is set up, and the parts of the neural network - the loss function, activation functions, optimisers, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvAPQK53Fyqm"
      },
      "source": [
        "This example demonstrates how to do structured data classification using the\n",
        "two modeling\n",
        "techniques. It is highly recommended to skim these two articles before diving in! The description below the model is the abstract.\n",
        "\n",
        "1. [Wide & Deep](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) models\n",
        "\n",
        "- The human brain is a sophisticated learning machine, forming rules by memorizing everyday events (“sparrows can fly” and “pigeons can fly”) and generalizing those learnings to apply to things we haven't seen before (“animals with wings can fly”). Perhaps more powerfully, memorization also allows us to further refine our generalized rules with exceptions (“penguins can't fly”). As we were exploring how to advance machine intelligence, we asked ourselves the question—can we teach computers to learn like humans do, by combining the power of memorization and generalization? It's not an easy question to answer, but by jointly training a wide linear model (for memorization) alongside a deep neural network (for generalization), one can combine the strengths of both to bring us one step closer. At Google, we call it Wide & Deep Learning. It's useful for generic large-scale regression and classification problems with sparse inputs (categorical features with a large number of possible feature values), such as recommender systems, search, and ranking problems.\n",
        "\n",
        "2. [Deep & Cross](https://arxiv.org/abs/1708.05123) models -\n",
        "\n",
        "- Feature engineering has been the key to the success of many prediction models. However, the process is nontrivial and often requires manual feature engineering or exhaustive searching. DNNs\n",
        "are able to automatically learn feature interactions; however, they\n",
        "generate all interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of\n",
        "a DNN model, and beyond that, it introduces a novel cross network\n",
        "more efficient in learning certain bounded-degree feature\n",
        "interactions. In particular, DCN explicitly applies feature crossing\n",
        "at each layer, requires no manual feature engineering, and adds\n",
        "negligible extra complexity to the DNN model. Our experimental\n",
        "results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification\n",
        "dataset, in terms of both model accuracy and memory usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulNZigS1Fyqm"
      },
      "source": [
        "### The dataset\n",
        "\n",
        "This example uses the [Covertype](https://archive.ics.uci.edu/ml/datasets/covertype) dataset from the UCI\n",
        "Machine Learning Repository. The task is to predict forest cover type from cartographic variables.\n",
        "The dataset includes 506,011 instances with 12 input features: 10 numerical features and 2\n",
        "categorical features. Each instance is categorized into 1 of 7 classes, and is **multilabel classification task**. Even though the dataset isn't social scientific, it combines both numerical values and categorical features, which is something you might be dealing with often."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Ptq8ElFyqm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUN2ZPGUFyqm"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n",
        "DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaFyaDrGFyqm"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the dataset\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
        "raw_data = pd.read_csv(data_url, header=None)\n",
        "print(f\"Dataset shape: {raw_data.shape}\")\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoJ5mSWUFyqn"
      },
      "source": [
        "The two categorical features in the dataset are binary-encoded.\n",
        "We will convert this dataset representation to the typical representation, where each\n",
        "categorical feature is represented as a single integer value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyfe6zItFyqn"
      },
      "outputs": [],
      "source": [
        "soil_type_values = [f\"soil_type_{idx+1}\" for idx in range(40)]\n",
        "wilderness_area_values = [f\"area_type_{idx+1}\" for idx in range(4)]\n",
        "\n",
        "soil_type = raw_data.loc[:, 14:53].apply(\n",
        "    lambda x: soil_type_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
        ")\n",
        "wilderness_area = raw_data.loc[:, 10:13].apply(\n",
        "    lambda x: wilderness_area_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
        ")\n",
        "\n",
        "CSV_HEADER = [\n",
        "    \"Elevation\",\n",
        "    \"Aspect\",\n",
        "    \"Slope\",\n",
        "    \"Horizontal_Distance_To_Hydrology\",\n",
        "    \"Vertical_Distance_To_Hydrology\",\n",
        "    \"Horizontal_Distance_To_Roadways\",\n",
        "    \"Hillshade_9am\",\n",
        "    \"Hillshade_Noon\",\n",
        "    \"Hillshade_3pm\",\n",
        "    \"Horizontal_Distance_To_Fire_Points\",\n",
        "    \"Wilderness_Area\",\n",
        "    \"Soil_Type\",\n",
        "    \"Cover_Type\",\n",
        "]\n",
        "\n",
        "data = pd.concat(\n",
        "    [raw_data.loc[:, 0:9], wilderness_area, soil_type, raw_data.loc[:, 54]],\n",
        "    axis=1,\n",
        "    ignore_index=True,\n",
        ")\n",
        "data.columns = CSV_HEADER\n",
        "\n",
        "# Convert the target label indices into a range from 0 to 6 (there are 7 labels in total).\n",
        "data[\"Cover_Type\"] = data[\"Cover_Type\"] - 1\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "data.head().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbunU0LqFyqn"
      },
      "source": [
        "The shape of the DataFrame shows there are 13 columns per sample\n",
        "(12 for the features and 1 for the target label).\n",
        "\n",
        "Let's split the data into training (85%) and test (15%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpF3uAIoFyqn"
      },
      "outputs": [],
      "source": [
        "train_splits = []\n",
        "test_splits = []\n",
        "\n",
        "for _, group_data in data.groupby(\"Cover_Type\"):\n",
        "    random_selection = np.random.rand(len(group_data.index)) <= 0.85\n",
        "    train_splits.append(group_data[random_selection])\n",
        "    test_splits.append(group_data[~random_selection])\n",
        "\n",
        "train_data = pd.concat(train_splits).sample(frac=1).reset_index(drop=True)\n",
        "test_data = pd.concat(test_splits).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"Train split size: {len(train_data.index)}\")\n",
        "print(f\"Test split size: {len(test_data.index)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2eNJ8hXFyqn"
      },
      "source": [
        "Next, store the training and test data in separate CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sHx3mnYFyqn"
      },
      "outputs": [],
      "source": [
        "train_data_file = \"train_data.csv\"\n",
        "test_data_file = \"test_data.csv\"\n",
        "\n",
        "train_data.to_csv(train_data_file, index=False)\n",
        "test_data.to_csv(test_data_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH5D0DDCFyqo"
      },
      "source": [
        "### Define dataset metadata\n",
        "\n",
        "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
        "the data into input features, and encoding the input features with respect to their types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzLLtectFyqo"
      },
      "outputs": [],
      "source": [
        "TARGET_FEATURE_NAME = \"Cover_Type\"\n",
        "\n",
        "TARGET_FEATURE_LABELS = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
        "\n",
        "NUMERIC_FEATURE_NAMES = [\n",
        "    \"Aspect\",\n",
        "    \"Elevation\",\n",
        "    \"Hillshade_3pm\",\n",
        "    \"Hillshade_9am\",\n",
        "    \"Hillshade_Noon\",\n",
        "    \"Horizontal_Distance_To_Fire_Points\",\n",
        "    \"Horizontal_Distance_To_Hydrology\",\n",
        "    \"Horizontal_Distance_To_Roadways\",\n",
        "    \"Slope\",\n",
        "    \"Vertical_Distance_To_Hydrology\",\n",
        "]\n",
        "\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"Soil_Type\": list(data[\"Soil_Type\"].unique()),\n",
        "    \"Wilderness_Area\": list(data[\"Wilderness_Area\"].unique()),\n",
        "}\n",
        "\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(TARGET_FEATURE_LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXd2SMdYFyqo"
      },
      "source": [
        "### Experiment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we organize and store the processed data in a way that PyTorch can easily use and further preprocesses the data by converting categorical features and labels into numerical representations using LabelEncoder."
      ],
      "metadata": {
        "id": "3bJ4iny66LLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYN5h_lLFyqo"
      },
      "outputs": [],
      "source": [
        "class ForestDataset(Dataset):\n",
        "    def __init__(self, dataframe, numeric_features, categorical_features):\n",
        "        self.numeric_data = dataframe[numeric_features].values.astype(np.float32)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.labels = self.label_encoder.fit_transform(dataframe[\"Cover_Type\"].values)\n",
        "        self.cat_encoders = {\n",
        "            feature: LabelEncoder().fit(dataframe[feature])\n",
        "            for feature in categorical_features\n",
        "        }\n",
        "        self.categorical_data = [\n",
        "            self.cat_encoders[feature].transform(dataframe[feature])\n",
        "            for feature in categorical_features\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        numeric = torch.tensor(self.numeric_data[idx], dtype=torch.float32)\n",
        "        categorical = torch.tensor([self.categorical_data[i][idx] for i in range(len(self.categorical_data))], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return numeric, categorical, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "train_dataset = ForestDataset(train_data, NUMERIC_FEATURE_NAMES, CATEGORICAL_FEATURE_NAMES)\n",
        "test_dataset = ForestDataset(test_data, NUMERIC_FEATURE_NAMES, CATEGORICAL_FEATURE_NAMES)\n",
        "\n",
        "batch_size = 265\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "nkcjKKO4scVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-meOl2RFyqt"
      },
      "source": [
        "### Experiment 1: a baseline model\n",
        "\n",
        "In the first experiment, let's create a multi-layer feed-forward network,\n",
        "where the categorical features are one-hot encoded.\n",
        "\n",
        "**Note**: Some new terms here! We see a Batch Normalisation layer. We will encounter these in our second notebook on regularisation, but for a brief explanation:\n",
        "\n",
        "Batch normalization tries to reduce the “internal covariate shift” between training and testing data. Internal covariate shift is the change in the distribution of network activations due to the change in paramaters during training. In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on. When the parameters of a layer change, so does the distribution of inputs to subsequent layers. These shifts in input distributions can be problematic for neural networks, especially deep neural networks that could have a large number of layers. Batch normalization tries to mitigate this. You can check out [this](https://arxiv.org/abs/1502.03167) paper where the idea of mitigating internal covariance shift with batch normalization was first introduced.\n",
        "\n",
        "We also see Dropout, another term from the regularisation universe. Dropout involves injecting noise while computing each internal layer during forward propagation. The method is called dropout because we literally drop out some neurons during training. Throughout training, on each iteration, standard dropout consists of zeroing out some fraction of the nodes in each layer before calculating the subsequent layer.\n",
        "\n",
        "![Dropout](https://d2l.ai/_images/dropout2.svg)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, num_numeric_features, categorical_cardinalities, hidden_units, num_classes, dropout_rate):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.cat_embeddings = nn.ModuleList(\n",
        "            [nn.Embedding(cardinality, int(math.sqrt(cardinality))) for cardinality in categorical_cardinalities]\n",
        "        )\n",
        "        input_dim = num_numeric_features + sum(emb.embedding_dim for emb in self.cat_embeddings)\n",
        "        self.hidden_layers = nn.ModuleList(\n",
        "            [nn.Linear(input_dim if i == 0 else hidden_units[i-1], hidden_units[i]) for i in range(len(hidden_units))]\n",
        "        )\n",
        "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(units) for units in hidden_units])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.output_layer = nn.Linear(hidden_units[-1], num_classes)\n",
        "\n",
        "    def forward(self, numeric_inputs, categorical_inputs):\n",
        "        embedded_cats = [emb(cat) for emb, cat in zip(self.cat_embeddings, categorical_inputs.T)]\n",
        "        features = torch.cat([numeric_inputs] + embedded_cats, dim=1)\n",
        "        for layer, bn in zip(self.hidden_layers, self.batch_norms):\n",
        "            features = self.dropout(torch.relu(bn(layer(features))))\n",
        "        return torch.softmax(self.output_layer(features), dim=1)\n",
        "\n",
        "num_classes = len(data[\"Cover_Type\"].unique())\n",
        "categorical_cardinalities = [len(train_dataset.cat_encoders[feature].classes_) for feature in CATEGORICAL_FEATURE_NAMES]\n",
        "\n",
        "model = BaselineModel(\n",
        "    num_numeric_features=len(NUMERIC_FEATURE_NAMES),\n",
        "    categorical_cardinalities=categorical_cardinalities,\n",
        "    hidden_units=[32, 32],\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "nBeBiSMgsjNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up for Training\n"
      ],
      "metadata": {
        "id": "smpe_bml60Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for numeric, categorical, labels in dataloader:\n",
        "            numeric, categorical, labels = numeric.to(device), categorical.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(numeric, categorical)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "Q-Tv9vl5stXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Evaluation Function"
      ],
      "metadata": {
        "id": "bWtf_ISr65UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for numeric, categorical, labels in dataloader:\n",
        "            numeric, categorical, labels = numeric.to(device), categorical.to(device), labels.to(device)\n",
        "            outputs = model(numeric, categorical)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion, num_epochs=50)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EBbRDlP68tb",
        "outputId": "789ea3e8-c37b-499f-8793-26277a7b50f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 1.4863\n",
            "Epoch 2/50, Loss: 1.4445\n",
            "Epoch 3/50, Loss: 1.4371\n",
            "Epoch 4/50, Loss: 1.4326\n",
            "Epoch 5/50, Loss: 1.4293\n",
            "Epoch 6/50, Loss: 1.4269\n",
            "Epoch 7/50, Loss: 1.4253\n",
            "Epoch 8/50, Loss: 1.4236\n",
            "Epoch 9/50, Loss: 1.4223\n",
            "Epoch 10/50, Loss: 1.4210\n",
            "Epoch 11/50, Loss: 1.4203\n",
            "Epoch 12/50, Loss: 1.4191\n",
            "Epoch 13/50, Loss: 1.4187\n",
            "Epoch 14/50, Loss: 1.4176\n",
            "Epoch 15/50, Loss: 1.4166\n",
            "Epoch 16/50, Loss: 1.4156\n",
            "Epoch 17/50, Loss: 1.4152\n",
            "Epoch 18/50, Loss: 1.4139\n",
            "Epoch 19/50, Loss: 1.4136\n",
            "Epoch 20/50, Loss: 1.4137\n",
            "Epoch 21/50, Loss: 1.4127\n",
            "Epoch 22/50, Loss: 1.4122\n",
            "Epoch 23/50, Loss: 1.4121\n",
            "Epoch 24/50, Loss: 1.4117\n",
            "Epoch 25/50, Loss: 1.4115\n",
            "Epoch 26/50, Loss: 1.4108\n",
            "Epoch 27/50, Loss: 1.4107\n",
            "Epoch 28/50, Loss: 1.4100\n",
            "Epoch 29/50, Loss: 1.4095\n",
            "Epoch 30/50, Loss: 1.4096\n",
            "Epoch 31/50, Loss: 1.4091\n",
            "Epoch 32/50, Loss: 1.4091\n",
            "Epoch 33/50, Loss: 1.4088\n",
            "Epoch 34/50, Loss: 1.4083\n",
            "Epoch 35/50, Loss: 1.4087\n",
            "Epoch 36/50, Loss: 1.4082\n",
            "Epoch 37/50, Loss: 1.4080\n",
            "Epoch 38/50, Loss: 1.4074\n",
            "Epoch 39/50, Loss: 1.4074\n",
            "Epoch 40/50, Loss: 1.4066\n",
            "Epoch 41/50, Loss: 1.4064\n",
            "Epoch 42/50, Loss: 1.4062\n",
            "Epoch 43/50, Loss: 1.4062\n",
            "Epoch 44/50, Loss: 1.4062\n",
            "Epoch 45/50, Loss: 1.4055\n",
            "Epoch 46/50, Loss: 1.4052\n",
            "Epoch 47/50, Loss: 1.4055\n",
            "Epoch 48/50, Loss: 1.4051\n",
            "Epoch 49/50, Loss: 1.4047\n",
            "Epoch 50/50, Loss: 1.4049\n",
            "Test Accuracy: 73.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4aoEyqPFyqu"
      },
      "source": [
        "The baseline linear model achieves ~75% test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFJ4mVLzFyqu"
      },
      "source": [
        "### Experiment 2: Wide & Deep model\n",
        "\n",
        "In the second experiment, we create a Wide & Deep model. The wide part of the model\n",
        "a linear model, while the deep part of the model is a multi-layer feed-forward network.\n",
        "\n",
        "Use the sparse representation of the input features in the wide part of the model and the\n",
        "dense representation of the input features for the deep part of the model.\n",
        "\n",
        "Note that every input features contributes to both parts of the model with different\n",
        "representations. You can see that we merge the layers near the end before reaching the output of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dlQSFYAbFyqu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Helper function for embedding layers\n",
        "def create_embeddings(categorical_cardinalities):\n",
        "    return nn.ModuleList(\n",
        "        [nn.Embedding(cardinality, int(math.sqrt(cardinality))) for cardinality in categorical_cardinalities]\n",
        "    )\n",
        "\n",
        "# Wide and Deep Model\n",
        "class WideAndDeepModel(nn.Module):\n",
        "    def __init__(self, num_numeric_features, categorical_cardinalities, hidden_units, num_classes, dropout_rate):\n",
        "        super(WideAndDeepModel, self).__init__()\n",
        "\n",
        "        # Wide part: Sparse representation\n",
        "        self.wide_embeddings = create_embeddings(categorical_cardinalities)\n",
        "        wide_input_dim = num_numeric_features + sum(emb.embedding_dim for emb in self.wide_embeddings)\n",
        "\n",
        "        # Deep part: Dense representation\n",
        "        self.deep_embeddings = create_embeddings(categorical_cardinalities)\n",
        "        deep_input_dim = num_numeric_features + sum(emb.embedding_dim for emb in self.deep_embeddings)\n",
        "\n",
        "        self.deep_hidden_layers = nn.ModuleList(\n",
        "            [nn.Linear(deep_input_dim if i == 0 else hidden_units[i - 1], hidden_units[i]) for i in range(len(hidden_units))]\n",
        "        )\n",
        "        self.deep_batch_norms = nn.ModuleList([nn.BatchNorm1d(units) for units in hidden_units])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Merged output\n",
        "        self.output_layer = nn.Linear(wide_input_dim + hidden_units[-1], num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, numeric_inputs, categorical_inputs):\n",
        "        # Wide part\n",
        "        wide_embedded = [emb(cat) for emb, cat in zip(self.wide_embeddings, categorical_inputs.T)]\n",
        "        wide_features = torch.cat([numeric_inputs] + wide_embedded, dim=1)\n",
        "        wide_features = wide_features.float()\n",
        "\n",
        "        # Deep part\n",
        "        deep_embedded = [emb(cat) for emb, cat in zip(self.deep_embeddings, categorical_inputs.T)]\n",
        "        deep_features = torch.cat([numeric_inputs] + deep_embedded, dim=1)\n",
        "        for layer, bn in zip(self.deep_hidden_layers, self.deep_batch_norms):\n",
        "            deep_features = self.dropout(torch.relu(bn(layer(deep_features))))\n",
        "\n",
        "        # Merge and output\n",
        "        merged = torch.cat([wide_features, deep_features], dim=1)\n",
        "        outputs = self.softmax(self.output_layer(merged))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model creation\n",
        "num_classes = len(data[\"Cover_Type\"].unique())\n",
        "categorical_cardinalities = [len(train_dataset.cat_encoders[feature].classes_) for feature in CATEGORICAL_FEATURE_NAMES]\n",
        "\n",
        "wide_and_deep_model = WideAndDeepModel(\n",
        "    num_numeric_features=len(NUMERIC_FEATURE_NAMES),\n",
        "    categorical_cardinalities=categorical_cardinalities,\n",
        "    hidden_units=[32, 32],\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "NQQi2bWHzKhs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gyccSEWFyqu"
      },
      "source": [
        "Let's run it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUj7fHDDFyqu"
      },
      "outputs": [],
      "source": [
        "# Training and evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wide_and_deep_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(wide_and_deep_model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for numeric, categorical, labels in dataloader:\n",
        "            numeric, categorical, labels = numeric.to(device), categorical.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(numeric, categorical)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for numeric, categorical, labels in dataloader:\n",
        "            numeric, categorical, labels = numeric.to(device), categorical.to(device), labels.to(device)\n",
        "            outputs = model(numeric, categorical)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(wide_and_deep_model, train_loader, optimizer, criterion, num_epochs=50)\n",
        "evaluate_model(wide_and_deep_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi47j_J3Fyqu"
      },
      "source": [
        "The wide and deep model achieves ~79% test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UExGJJXaFyqu"
      },
      "source": [
        "### Experiment 3: Deep & Cross model\n",
        "\n",
        "In the third experiment, we create a Deep & Cross model. The deep part of this model\n",
        "is the same as the deep part created in the previous experiment. The key idea of\n",
        "the cross part is to apply explicit feature crossing in an efficient way,\n",
        "where the degree of cross features grows with layer depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8GpvzLtpFyqv"
      },
      "outputs": [],
      "source": [
        "# Define the deep and cross model\n",
        "class DeepAndCrossModel(nn.Module):\n",
        "    def __init__(self, num_numeric_features, categorical_cardinalities, hidden_units, num_classes, dropout_rate):\n",
        "        super(DeepAndCrossModel, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.cat_embeddings = nn.ModuleList(\n",
        "            [nn.Embedding(cardinality, int(math.sqrt(cardinality))) for cardinality in categorical_cardinalities]\n",
        "        )\n",
        "        input_dim = num_numeric_features + sum(emb.embedding_dim for emb in self.cat_embeddings)\n",
        "\n",
        "        # Cross part\n",
        "        self.cross_layers = nn.ModuleList(\n",
        "            [nn.Linear(input_dim, input_dim) for _ in range(len(hidden_units))]\n",
        "        )\n",
        "        self.cross_bn = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "        # Deep part\n",
        "        self.hidden_layers = nn.ModuleList(\n",
        "            [nn.Linear(input_dim if i == 0 else hidden_units[i-1], hidden_units[i]) for i in range(len(hidden_units))]\n",
        "        )\n",
        "        self.deep_bn = nn.ModuleList([nn.BatchNorm1d(units) for units in hidden_units])\n",
        "\n",
        "        # Merged output layer\n",
        "        self.output_layer = nn.Linear(input_dim + hidden_units[-1], num_classes)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, numeric_inputs, categorical_inputs):\n",
        "        # Embedding\n",
        "        embedded_cats = [emb(cat) for emb, cat in zip(self.cat_embeddings, categorical_inputs.T)]\n",
        "        x0 = torch.cat([numeric_inputs] + embedded_cats, dim=1)\n",
        "\n",
        "        # Cross part\n",
        "        cross = x0\n",
        "        for layer in self.cross_layers:\n",
        "            x = layer(cross)\n",
        "            cross = x0 * x + cross\n",
        "        cross = self.cross_bn(cross)\n",
        "\n",
        "        # Deep part\n",
        "        deep = x0\n",
        "        for layer, bn in zip(self.hidden_layers, self.deep_bn):\n",
        "            deep = self.dropout(torch.relu(bn(layer(deep))))\n",
        "\n",
        "        # Merged output\n",
        "        merged = torch.cat([cross, deep], dim=1)\n",
        "        outputs = torch.softmax(self.output_layer(merged), dim=1)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deep_and_cross_model = DeepAndCrossModel(\n",
        "    num_numeric_features=len(NUMERIC_FEATURE_NAMES),\n",
        "    categorical_cardinalities=categorical_cardinalities,\n",
        "    hidden_units=[32, 32],\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "PlHECqJOzNFb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1QJ6euFyqv"
      },
      "source": [
        "Let's run it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model creation\n",
        "deep_and_cross_model = DeepAndCrossModel(\n",
        "    num_numeric_features=len(NUMERIC_FEATURE_NAMES),\n",
        "    categorical_cardinalities=categorical_cardinalities,\n",
        "    hidden_units=[32, 32],\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "# Training and evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "deep_and_cross_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(deep_and_cross_model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for numeric, categorical, labels in dataloader:\n",
        "            numeric, categorical, labels = numeric.to(device), categorical.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(numeric, categorical)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for numeric, categorical, labels in dataloader:\n",
        "            numeric, categorical, labels = numeric.to(device), categorical.to(device), labels.to(device)\n",
        "            outputs = model(numeric, categorical)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(deep_and_cross_model, train_loader, optimizer, criterion, num_epochs=50)\n",
        "evaluate_model(deep_and_cross_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVfidvXO8eer",
        "outputId": "fa923711-326c-49a2-e987-91ef4ea2483e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 1.4716\n",
            "Epoch 2/50, Loss: 1.4317\n",
            "Epoch 3/50, Loss: 1.4252\n",
            "Epoch 4/50, Loss: 1.4210\n",
            "Epoch 5/50, Loss: 1.4171\n",
            "Epoch 6/50, Loss: 1.4145\n",
            "Epoch 7/50, Loss: 1.4130\n",
            "Epoch 8/50, Loss: 1.4111\n",
            "Epoch 9/50, Loss: 1.4094\n",
            "Epoch 10/50, Loss: 1.4076\n",
            "Epoch 11/50, Loss: 1.4062\n",
            "Epoch 12/50, Loss: 1.4055\n",
            "Epoch 13/50, Loss: 1.4044\n",
            "Epoch 14/50, Loss: 1.4037\n",
            "Epoch 15/50, Loss: 1.4031\n",
            "Epoch 16/50, Loss: 1.4025\n",
            "Epoch 17/50, Loss: 1.4009\n",
            "Epoch 18/50, Loss: 1.4005\n",
            "Epoch 19/50, Loss: 1.3997\n",
            "Epoch 20/50, Loss: 1.3990\n",
            "Epoch 21/50, Loss: 1.3983\n",
            "Epoch 22/50, Loss: 1.3982\n",
            "Epoch 23/50, Loss: 1.3974\n",
            "Epoch 24/50, Loss: 1.3965\n",
            "Epoch 25/50, Loss: 1.3966\n",
            "Epoch 26/50, Loss: 1.3961\n",
            "Epoch 27/50, Loss: 1.3962\n",
            "Epoch 28/50, Loss: 1.3955\n",
            "Epoch 29/50, Loss: 1.3950\n",
            "Epoch 30/50, Loss: 1.3948\n",
            "Epoch 31/50, Loss: 1.3942\n",
            "Epoch 32/50, Loss: 1.3937\n",
            "Epoch 33/50, Loss: 1.3936\n",
            "Epoch 34/50, Loss: 1.3931\n",
            "Epoch 35/50, Loss: 1.3928\n",
            "Epoch 36/50, Loss: 1.3925\n",
            "Epoch 37/50, Loss: 1.3920\n",
            "Epoch 38/50, Loss: 1.3922\n",
            "Epoch 39/50, Loss: 1.3920\n",
            "Epoch 40/50, Loss: 1.3913\n",
            "Epoch 41/50, Loss: 1.3912\n",
            "Epoch 42/50, Loss: 1.3911\n",
            "Epoch 43/50, Loss: 1.3907\n",
            "Epoch 44/50, Loss: 1.3906\n",
            "Epoch 45/50, Loss: 1.3901\n",
            "Epoch 46/50, Loss: 1.3905\n",
            "Epoch 47/50, Loss: 1.3898\n",
            "Epoch 48/50, Loss: 1.3898\n",
            "Epoch 49/50, Loss: 1.3893\n",
            "Epoch 50/50, Loss: 1.3892\n",
            "Test Accuracy: 72.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BEX66ukFyqv"
      },
      "source": [
        "The deep and cross model also achieves ~75% test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciQbUdc1Fyqx"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "You can use Preprocessing Layers to easily handle categorical features\n",
        "with different encoding mechanisms, including one-hot encoding and feature embedding.\n",
        "In addition, different model architectures — like wide, deep, and cross networks\n",
        "— have different advantages, with respect to different dataset properties.\n",
        "You can explore using them independently or combining them to achieve the best result\n",
        "for your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJlLO7O4FyrG"
      },
      "source": [
        "# Homework Exercise\n",
        "\n",
        "**1)** Build a neural network model implemented in this notebook for a dataset of your choice. You can use the model to perform a predictive task, such as classification or numerical prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZGbvB1Q6FyrG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oe-JJbjFyrG"
      },
      "source": [
        "**2)** Use this model to extract features from your data-points. Once you extract these features, convert these to a low dimensional space and visualise them (e.g PCA, t-SNE, UMAP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Dd6BuoY_FyrG"
      },
      "outputs": [],
      "source": [
        "explanation = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9smLXu7gjd2y"
      },
      "source": [
        "# Module 2: Optimization\n",
        "\n",
        "Neural networks are optimized through an iterative process of adjusting their weights to improve the accuracy of predictions and classifications. The primary goal during optimization is to minimize the error between the network's predictions and the actual target values. This process relies on various techniques and algorithms to systematically update the weights based on how well the network is performing.\n",
        "\n",
        "This notebooks features some code and concepts from the UPenn CS-522 course, specifically their [week 4 on Optimisation](https://github.com/CIS-522/course-content/tree/main/tutorials/W04_Optimization), and [week 5 on Regularisation](https://github.com/CIS-522/course-content/tree/main/tutorials/W05_Regularization), and we are grateful for their content and support! We would also highly recommend them as resources for more code and content on optimisation, regularisation, and other content on deep learning. (We are co-teaching a summer Deep Learning Academy with them this summer).\n",
        "\n",
        "**Notes about the tutorial**: Much like the last tutorial, there is a lot more code here. Once again, a lot of that code is just to setup data and some of the neural nets, and we have tried to comment the code as extensively as possible. Like last time, we highly recommend looking at the homework at the bottom of the notebook so that you can manage your time on the notebook better. Getting to know about the possible range of optimization options and regularisation options is a necessary exercise to get the most out of your deep neural models. Throughout this notebook, you will also encounter some exploratory questions. You need not answer them in the notebook but you are encouraged to search for the answers (or ask us if you can't find them!)\n",
        "\n",
        "Your purpose in this notebook is to be able to adapt your PyTorch models with the various techniques we discuss below. All the primary code in the notebook is in PyTorch with links to resources or documentation for similar options in Keras.\n",
        "\n",
        "Optimization in a neural network refers to the process of adjusting the weights of your neural network on the way to make a better prediction. Here, we are attempting to find a local minima which minimizes the error of the predictions made. Here are some basic resources to read up on the topic:\n",
        "\n",
        "[Neural Network Optimization](https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0).\n",
        "\n",
        "[Parameter and weights optimization for Neural Networks](https://www.deeplearning.ai/ai-notes/optimization/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Qb3jspnujd2y"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import io\n",
        "from urllib.request import urlopen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_fet4L0jd2z"
      },
      "source": [
        "---\n",
        "## Minibatch stochastic gradient descent (SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGwKWILcjd20"
      },
      "source": [
        "In stochastic gradient descent, we replace the actual gradient vector with a stochastic estimation of the gradient vector. Specifically for a neural network, the stochastic estimation uses the gradient of the loss for a single data point (single instance).\n",
        "\n",
        "Given $f_i=l(x_i, y_i, w)$, the expected value of the $t$-th step of SGD is the same as the $t$-th step of full gradient descent.\n",
        "\n",
        "$$\\mathbb{E}[w_{t+1}]=w_t-\\eta \\mathbb{E}[\\nabla f_i(w_t)]=w_t-\\eta\\nabla f(w_t)$$\n",
        "\n",
        "where $i$ is chosen uniformly at random, thereby $f_i$ is a noisy but unbiased estimator of $f$.\n",
        "\n",
        "$$w_{t+1}=w_t-\\eta\\nabla f_i(w_t)$$\n",
        "\n",
        "We update the weights according to the gradient over $f_i$ (as opposed to the gradient over the total loss $f$).\n",
        "\n",
        "SGD advantages:\n",
        "*   The noise in the SGD update can prevent convergence to a bad (shallow) local minima.\n",
        "*   It is drastically cheaper to compute (as you don’t go over all data points).\n",
        "\n",
        "\n",
        "### Minibatching\n",
        "\n",
        "Often we are able to make better use of our hardware by using mini batches instead of single instances. We compute the loss over a mini-batch -- a set of randomly selected instances instead of calculating it over just one instance. This reduces the noise in the step update.\n",
        "\n",
        "Given the $t$th minibatch $B_t$ consisting of $k$ observations:\n",
        "\n",
        "$$w_{t+1}=w_t-\\eta \\frac{1}{|B_t|}\\sum_{i\\in B}\\nabla f_i(w_t)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfA0AHuXjd20"
      },
      "source": [
        "One of the main constraints of training deep neural networks is the relatively limited size of GPU memory. Being able to quickly estimate if your minibatch size can be held in that memory will save you time and out-of-memory errors.\n",
        "\n",
        "What do we need to store at training time?\n",
        "- outputs of intermediate layers (forward pass):\n",
        "- model parameters\n",
        "- error signal at each neuron\n",
        "- the gradient of parameters\n",
        "plus any extra memory needed by optimizer (e.g. for momentum)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CWOQKazIjd20"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cRDPnmDnjd21"
      },
      "outputs": [],
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # set optimiser to zero\n",
        "        optimizer.zero_grad()\n",
        "        # pass the data through the model\n",
        "        output = model(data)\n",
        "        # calculate loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # backprop\n",
        "        loss.backward()\n",
        "        # move optimiser\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Rt-QT-VGjd21"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Wlhv38j6jd22"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    acc_list, time_list = [], []\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        time_list.append(time.time()-start_time)\n",
        "        acc = test(model, device, test_loader)\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    return acc_list, time_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zSXD0EoXjd22"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "args = {'batch_size': 32,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 3,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }\n",
        "\n",
        "batch_size = [8, 16, 32, 64, 256, 512, 1024]\n",
        "acc_dict = {}\n",
        "test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAG2QOw6jd22",
        "outputId": "6f121fb6-5170-44d1-a9ee-8ea26a1e1a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:10<00:00, 902kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 239kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.93MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.212950\n",
            "Train Epoch: 1 [800/60000 (1%)]\tLoss: 0.633358\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.387849\n",
            "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.146681\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.374804\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.176324\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 1.137902\n",
            "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.165800\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.556258\n",
            "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.226273\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.011656\n",
            "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 1.092710\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.028158\n",
            "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 0.230705\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.546691\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.198070\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.174880\n",
            "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 0.493409\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.001052\n",
            "Train Epoch: 1 [15200/60000 (25%)]\tLoss: 0.002356\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.109389\n",
            "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 0.012016\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.147232\n",
            "Train Epoch: 1 [18400/60000 (31%)]\tLoss: 0.105661\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.591042\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.027270\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.249576\n",
            "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 1.247134\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.010450\n",
            "Train Epoch: 1 [23200/60000 (39%)]\tLoss: 0.000241\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.054903\n",
            "Train Epoch: 1 [24800/60000 (41%)]\tLoss: 0.077533\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.010759\n",
            "Train Epoch: 1 [26400/60000 (44%)]\tLoss: 0.009023\n",
            "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.018299\n",
            "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.458873\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.072566\n",
            "Train Epoch: 1 [29600/60000 (49%)]\tLoss: 0.284788\n",
            "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.670338\n",
            "Train Epoch: 1 [31200/60000 (52%)]\tLoss: 0.152470\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.060412\n",
            "Train Epoch: 1 [32800/60000 (55%)]\tLoss: 0.003099\n",
            "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.023698\n",
            "Train Epoch: 1 [34400/60000 (57%)]\tLoss: 0.214571\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.042157\n",
            "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.354852\n",
            "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.059182\n",
            "Train Epoch: 1 [37600/60000 (63%)]\tLoss: 0.125965\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.697826\n",
            "Train Epoch: 1 [39200/60000 (65%)]\tLoss: 0.170226\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.200247\n",
            "Train Epoch: 1 [40800/60000 (68%)]\tLoss: 0.376890\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.611953\n",
            "Train Epoch: 1 [42400/60000 (71%)]\tLoss: 1.109457\n",
            "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.132520\n",
            "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.000193\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.000274\n",
            "Train Epoch: 1 [45600/60000 (76%)]\tLoss: 0.112109\n",
            "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.010824\n",
            "Train Epoch: 1 [47200/60000 (79%)]\tLoss: 0.152095\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.034561\n",
            "Train Epoch: 1 [48800/60000 (81%)]\tLoss: 0.013242\n",
            "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.063088\n",
            "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 0.003120\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.001784\n",
            "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.314737\n",
            "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.552611\n",
            "Train Epoch: 1 [53600/60000 (89%)]\tLoss: 0.008825\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.032627\n",
            "Train Epoch: 1 [55200/60000 (92%)]\tLoss: 0.421233\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.388513\n",
            "Train Epoch: 1 [56800/60000 (95%)]\tLoss: 0.017026\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091176\n",
            "Train Epoch: 1 [58400/60000 (97%)]\tLoss: 0.611032\n",
            "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.061405\n",
            "\n",
            "Test set: Average loss: 0.1973, Accuracy: 9460/10000 (94.6000%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.169194\n",
            "Train Epoch: 2 [800/60000 (1%)]\tLoss: 0.607277\n",
            "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.054789\n",
            "Train Epoch: 2 [2400/60000 (4%)]\tLoss: 0.003840\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.011929\n",
            "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.004830\n",
            "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.010578\n",
            "Train Epoch: 2 [5600/60000 (9%)]\tLoss: 0.410451\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.003965\n",
            "Train Epoch: 2 [7200/60000 (12%)]\tLoss: 0.561194\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.009506\n",
            "Train Epoch: 2 [8800/60000 (15%)]\tLoss: 0.002469\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.516480\n",
            "Train Epoch: 2 [10400/60000 (17%)]\tLoss: 0.000203\n",
            "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.022628\n",
            "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.002384\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.011637\n",
            "Train Epoch: 2 [13600/60000 (23%)]\tLoss: 0.227119\n",
            "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.017990\n",
            "Train Epoch: 2 [15200/60000 (25%)]\tLoss: 0.364683\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.180610\n",
            "Train Epoch: 2 [16800/60000 (28%)]\tLoss: 0.129088\n",
            "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.000160\n",
            "Train Epoch: 2 [18400/60000 (31%)]\tLoss: 0.000097\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.117566\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.845158\n",
            "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.000023\n",
            "Train Epoch: 2 [21600/60000 (36%)]\tLoss: 0.130738\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.006625\n",
            "Train Epoch: 2 [23200/60000 (39%)]\tLoss: 0.001635\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.044546\n",
            "Train Epoch: 2 [24800/60000 (41%)]\tLoss: 0.004936\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007956\n",
            "Train Epoch: 2 [26400/60000 (44%)]\tLoss: 0.000168\n",
            "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.002207\n",
            "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.003716\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.271708\n",
            "Train Epoch: 2 [29600/60000 (49%)]\tLoss: 0.008470\n",
            "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.002401\n",
            "Train Epoch: 2 [31200/60000 (52%)]\tLoss: 0.000277\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.006362\n",
            "Train Epoch: 2 [32800/60000 (55%)]\tLoss: 0.011134\n",
            "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.952399\n",
            "Train Epoch: 2 [34400/60000 (57%)]\tLoss: 0.001906\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.013132\n",
            "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.012836\n",
            "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.000027\n",
            "Train Epoch: 2 [37600/60000 (63%)]\tLoss: 0.042769\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.000161\n",
            "Train Epoch: 2 [39200/60000 (65%)]\tLoss: 0.002139\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.000128\n",
            "Train Epoch: 2 [40800/60000 (68%)]\tLoss: 0.046545\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.043467\n",
            "Train Epoch: 2 [42400/60000 (71%)]\tLoss: 0.016170\n",
            "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.051281\n",
            "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.015288\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.002729\n",
            "Train Epoch: 2 [45600/60000 (76%)]\tLoss: 0.011999\n",
            "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.344978\n",
            "Train Epoch: 2 [47200/60000 (79%)]\tLoss: 0.000647\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.043931\n",
            "Train Epoch: 2 [48800/60000 (81%)]\tLoss: 0.011649\n",
            "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.017798\n",
            "Train Epoch: 2 [50400/60000 (84%)]\tLoss: 1.613126\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.023424\n",
            "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.006401\n",
            "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.147749\n",
            "Train Epoch: 2 [53600/60000 (89%)]\tLoss: 3.831974\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.063447\n",
            "Train Epoch: 2 [55200/60000 (92%)]\tLoss: 0.366370\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.000109\n",
            "Train Epoch: 2 [56800/60000 (95%)]\tLoss: 0.000514\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.000027\n",
            "Train Epoch: 2 [58400/60000 (97%)]\tLoss: 1.464050\n",
            "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.085122\n",
            "\n",
            "Test set: Average loss: 0.1559, Accuracy: 9603/10000 (96.0300%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.004380\n",
            "Train Epoch: 3 [800/60000 (1%)]\tLoss: 0.057970\n",
            "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.002121\n",
            "Train Epoch: 3 [2400/60000 (4%)]\tLoss: 0.006910\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.004256\n",
            "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.036165\n",
            "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.126385\n",
            "Train Epoch: 3 [5600/60000 (9%)]\tLoss: 0.041885\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.114151\n",
            "Train Epoch: 3 [7200/60000 (12%)]\tLoss: 0.450340\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.000006\n",
            "Train Epoch: 3 [8800/60000 (15%)]\tLoss: 0.000523\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.005588\n",
            "Train Epoch: 3 [10400/60000 (17%)]\tLoss: 0.000160\n",
            "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.214558\n",
            "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.000566\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.000651\n",
            "Train Epoch: 3 [13600/60000 (23%)]\tLoss: 0.167767\n",
            "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.000063\n",
            "Train Epoch: 3 [15200/60000 (25%)]\tLoss: 0.067753\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.000063\n",
            "Train Epoch: 3 [16800/60000 (28%)]\tLoss: 0.003029\n",
            "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.351907\n",
            "Train Epoch: 3 [18400/60000 (31%)]\tLoss: 0.010068\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.000005\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.013492\n",
            "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.025082\n",
            "Train Epoch: 3 [21600/60000 (36%)]\tLoss: 0.246744\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000025\n",
            "Train Epoch: 3 [23200/60000 (39%)]\tLoss: 0.000124\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.060447\n",
            "Train Epoch: 3 [24800/60000 (41%)]\tLoss: 0.053457\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.207073\n",
            "Train Epoch: 3 [26400/60000 (44%)]\tLoss: 0.007613\n",
            "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.489355\n",
            "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.020574\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.032459\n",
            "Train Epoch: 3 [29600/60000 (49%)]\tLoss: 0.000121\n",
            "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.000492\n",
            "Train Epoch: 3 [31200/60000 (52%)]\tLoss: 0.415662\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.113739\n",
            "Train Epoch: 3 [32800/60000 (55%)]\tLoss: 0.000085\n",
            "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.015418\n",
            "Train Epoch: 3 [34400/60000 (57%)]\tLoss: 0.003390\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.000847\n",
            "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.405370\n",
            "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.000081\n",
            "Train Epoch: 3 [37600/60000 (63%)]\tLoss: 0.000233\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.074263\n",
            "Train Epoch: 3 [39200/60000 (65%)]\tLoss: 0.935007\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.000021\n",
            "Train Epoch: 3 [40800/60000 (68%)]\tLoss: 0.000071\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.000819\n",
            "Train Epoch: 3 [42400/60000 (71%)]\tLoss: 0.000179\n",
            "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.058968\n",
            "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.659177\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.000005\n",
            "Train Epoch: 3 [45600/60000 (76%)]\tLoss: 0.111688\n",
            "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.140550\n",
            "Train Epoch: 3 [47200/60000 (79%)]\tLoss: 0.000269\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.039626\n",
            "Train Epoch: 3 [48800/60000 (81%)]\tLoss: 0.000039\n",
            "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.460090\n",
            "Train Epoch: 3 [50400/60000 (84%)]\tLoss: 0.000039\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.480121\n",
            "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.040164\n",
            "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.053578\n",
            "Train Epoch: 3 [53600/60000 (89%)]\tLoss: 0.088256\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.154290\n",
            "Train Epoch: 3 [55200/60000 (92%)]\tLoss: 0.000974\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.001341\n",
            "Train Epoch: 3 [56800/60000 (95%)]\tLoss: 0.004699\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.851190\n",
            "Train Epoch: 3 [58400/60000 (97%)]\tLoss: 0.172946\n",
            "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.000095\n",
            "\n",
            "Test set: Average loss: 0.2008, Accuracy: 9546/10000 (95.4600%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305983\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.285838\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.542683\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for i in range(len(batch_size)):\n",
        "    args['batch_size'] = batch_size[i]\n",
        "    acc, timer = main(args)\n",
        "    acc_dict['acc'+str(batch_size[i])] = acc\n",
        "    acc_dict['time'+str(batch_size[i])] = timer\n",
        "    test_acc.append(acc[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9cqCR8ojd23"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.plot(batch_size, test_acc, linewidth=2)\n",
        "plt.title('Optimal Minibach Size')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.savefig('minibatch.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHE54gVtjd23"
      },
      "source": [
        "---\n",
        "## Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7A51_Qbjd23"
      },
      "source": [
        "Rather than improving the optimization algorithms, batch normalization improves the network structure itself by adding additional layers in between existing layers. The goal is to improve the optimization and generalization performance.\n",
        "\n",
        "In neural networks, we typically alternate linear (weighted summation) operations with non-linear operations, the activation functions, such as ReLU. The most common practice is to put the normalization between the linear layers and activation functions.\n",
        "\n",
        "More formally, normalization is as follows:\n",
        "$$\\tilde x_j = a\\frac{x_j-\\mu_j}{\\sigma_j}+b$$\n",
        "where\n",
        "*   $x_j$ is the output of a neuron or, equivalently, the input to the next layer,\n",
        "*   $\\tilde x_j$ is that same feature after being normalized ,\n",
        "*   $\\mu_j$ is the mean of the feature $x_j$ over the minibatch,\n",
        "*   $\\sigma_j$ is the estimate of the standard deviation of $x_j$ over the minibatch (with $\\epsilon$ added, so we don't divide by zero),\n",
        "*   $a$ is the learnable scaling factor,\n",
        "*   $b$ is the learnable bias term.\n",
        "\n",
        "Batch normalization tries to reduce the “internal covariate shift” between training and testing data. Internal covariate shift is the change in the distribution of network activations due to the change in paramaters during training. In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on. When the parameters of a layer change, so does the distribution of inputs to subsequent layers. These shifts in input distributions can be problematic for neural networks, especially deep neural networks that could have a large number of layers. Batch normalization tries to mitigate this. You can check out [this](https://arxiv.org/abs/1502.03167) paper where the idea of mitigating internal covariance shift with batch normalization was first introduced.\n",
        "\n",
        "\n",
        "The advantages of BN are as follows:\n",
        "\n",
        "*   Networks with normalization layers are easier to optimize, allowing for the use of larger learning rates, speeding up the training of neural networks.\n",
        "*   The mean/std deviation estimates are noisy due to the randomness of the samples in batch. This extra “noise” sometimes results in better generalization. Normalization has a regularization effect.\n",
        "*   Normalization reduces sensitivity to weight initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZoDCxZMjd23"
      },
      "outputs": [],
      "source": [
        "#help functions\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    avg_loss = 0.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        avg_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    avg_loss /= len(train_loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return test_loss\n",
        "\n",
        "def bn_eval(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    if args['net_type'] == 'Shallow':\n",
        "        model = Net().to(device)\n",
        "    elif args['net_type'] == 'BNShallow':\n",
        "        model = BNShallowNet().to(device)\n",
        "    elif args['net_type'] == 'Deep':\n",
        "        model = DeepNet().to(device)\n",
        "    elif args['net_type'] == 'BNDeep':\n",
        "        model = BNDeepNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    train_list, test_list = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        train_list.append(train_loss)\n",
        "        test_list.append(test_loss)\n",
        "\n",
        "    return train_list, test_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LbJ8Ta5jd24"
      },
      "outputs": [],
      "source": [
        "#models\n",
        "class BNShallowNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNShallowNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.bn = nn.BatchNorm1d(128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUXueESwjd24"
      },
      "outputs": [],
      "source": [
        "class BNDeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNDeepNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 32)\n",
        "        self.fc5 = nn.Linear(32, 10)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.bn4 = nn.BatchNorm1d(32)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc5(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPgV0vLyjd24"
      },
      "outputs": [],
      "source": [
        "class DeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 32)\n",
        "        self.fc5 = nn.Linear(32, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc5(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlOlGETijd24"
      },
      "outputs": [],
      "source": [
        "#train\n",
        "args = {'batch_size': 64,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 10,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'net_type': 'Net',\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }\n",
        "\n",
        "net = ['Shallow', 'BNShallow', 'Deep', 'BNDeep']\n",
        "loss_dict = {}\n",
        "\n",
        "for i in range(len(net)):\n",
        "    args['net_type'] = net[i]\n",
        "    train_loss, test_loss = bn_eval(args)\n",
        "    loss_dict['train' + str(net[i])] = train_loss\n",
        "    loss_dict['test' + str(net[i])] = test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koS2NOEIjd24"
      },
      "source": [
        "## Momentum\n",
        "\n",
        "Momentum in gradient descent is similar to the concept of momentum in physics. The optimization process resembles a ball rolling down the hill. Momentum keeps the ball moving in the same direction that it is already moving in. The gradient can be thought of as a force pushing the ball in some other direction.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/640/1*i1Qc2E0TVlPHEKG7LepXgA.gif\">\n",
        "</p>\n",
        "\n",
        "Mathematically it can be expressed as follows-\n",
        "$$w_{t+1}=w_t-\\eta (\\nabla f(w_t) +\\beta m_{t}) $$\n",
        "$$m_{t+1}= \\nabla f(w_t) +\\beta m_{t}$$\n",
        "or, equivalently\n",
        "$$w_{t+1}= w_t -\\eta\\nabla f(w_t) +\\beta (w_{t} -w_{t-1})$$\n",
        "\n",
        "where\n",
        "*   $m$ is the momentum (the running average of the past gradients, initialized at zero),\n",
        "*   $\\beta\\in [0,1)$ is the damping factor, usually $0.9$ or $0.99$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF4kgfEfjd24"
      },
      "source": [
        "\n",
        "\n",
        "Let’s consider two extreme cases to understand this decay rate parameter better. If the decay rate is 0, then it is exactly the same as (vanilla) gradient descent (blue ball). If the decay rate is 1 (and provided that the learning rate is reasonably small), then it rocks back and forth endlessly like the frictionless ball we saw previously; you do not want that. Typically the decay rate is chosen around 0.8–0.9 — it’s like a surface with a little bit of friction so it eventually slows down and stops (purple ball).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/800/1*zVi4ayX9u0MQQwa90CnxVg.gif\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRdrd3hMjd24"
      },
      "outputs": [],
      "source": [
        "#useful link:\n",
        "#https://distill.pub/2017/momentum/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMlzXoz2jd25"
      },
      "source": [
        "In the standard SGD formulation, every weight in network is updated with the same learning rate (global $\\eta$). Here, we adapt a learning rate for each weight individually, using information we get from their gradients.\n",
        "\n",
        "## Adagrad\n",
        "\n",
        "Adagrad adapts the learning rate of each parameter, downweighting the learning rates for parameters that have changed a lot and upweighting the learning rates of parameters that have changed very little.\n",
        "\n",
        "It uses a different learning rate for every parameter $w_j$ at every time step, $t$. (The time step here in practice is a minibatch, with everything averaged over that minibatch.) The update for every parameter $w_j$ at each time step (or epoch) $t$ then becomes\n",
        "\n",
        "$$w_{t+1}=w_t- \\frac{\\eta}{\\sqrt{v_{t+1}+\\epsilon}} \\nabla f(w_t)$$\n",
        "\n",
        "where the equation holds for every feature $w_j$ separately. Thus, $\\nabla f(w_{t})$ is the partial derivative of the objective function w.r.t. to the parameter $w_j$ at time step $t$ and the learning rate for each feature is scaled using the sum of the gradients for that feature:\n",
        "\n",
        "$$v_{t+1} = \\sum^t_{\\tau=1} \\nabla f(w_{\\tau})^2$$\n",
        "\n",
        "Adagrad effectively selects low learning rates for parameters associated with frequently occurring features, and high learning rates for parameters associated with infrequent features. It is thus well-suited for dealing with sparse data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn5Lxo0ujd25"
      },
      "source": [
        "## RMSprop\n",
        "\n",
        "RMSprop seeks to reduce Adagrad's aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, RMSprop restricts the window of accumulated past gradients to some fixed size. The sum of gradients is recursively defined as a decaying average of all past squared gradients.\n",
        "\n",
        "$$w_{t+1}=w_t- \\frac{\\eta}{\\sqrt{v_{t+1}+\\epsilon}} \\nabla f(w_t)$$\n",
        "$$v_{t+1}=\\alpha v_t+(1-\\alpha)(\\nabla f(w_t))^2$$\n",
        "\n",
        "where\n",
        "*   $v$ is the 2nd moment estimate which depends (as a fraction $\\alpha$ similarly to the Momentum term) on the previous average and the current gradient.\n",
        "*   $\\alpha$ is usually set to $0.9$, while a good default value for the learning rate $\\eta$ is $0.001$.\n",
        "\n",
        "We update $v$ to estimate this noisy quantity via an exponential moving average (which is a standard way of maintaining an average of a quantity that may change over time). We need to put larger weights on the newer values as they provide more information. One way to do that is down-weight old values exponentially. The values in the $v$ calculation that are very old are down-weighted at each step by an $\\alpha$ constant, which varies between 0 and 1. This dampens the old values until they are no longer an important part of the exponential moving average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4UvEO84jd25"
      },
      "source": [
        "## Adam\n",
        "\n",
        "Adam (from \"Adaptive moments\") is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.\n",
        "\n",
        "**How does Adam work?**\n",
        "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction).\n",
        "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction).\n",
        "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
        "\n",
        "The update rule is, for $l = 1, ..., L$:\n",
        "\n",
        "$$\\begin{cases}\n",
        "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
        "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
        "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
        "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
        "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
        "\\end{cases}$$\n",
        "where:\n",
        "- t counts the number of steps taken of Adam\n",
        "- L is the number of layers\n",
        "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages.\n",
        "- $\\alpha$ is the learning rate\n",
        "- $\\varepsilon$ is a very small number to avoid dividing by zero\n",
        "\n",
        "As usual, we will store all parameters in the `parameters` dictionary.\n",
        "\n",
        "The"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbBl1afHjd25"
      },
      "source": [
        "## Learn and compare different adaptive learning rate optimizers\n",
        "\n",
        "For SGD with fixed schedule, Adagrad, RMSprop, Adam, how do they differ on train and test error? Which one works the best?\n",
        "\n",
        "We compare these optimizers by performing digit classification task in MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "245YhQQjjd25"
      },
      "outputs": [],
      "source": [
        "def optimizer_eval(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    if args['optimizer'] == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args['lr'])\n",
        "    elif args['optimizer'] == 'adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=args['lr'])\n",
        "    elif args['optimizer'] == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
        "    elif args['optimizer'] == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    train_list, test_list = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train_acc = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_list.append(100.-train_acc)\n",
        "        test_acc = test(model, device, test_loader)\n",
        "        test_list.append(100.-test_acc)\n",
        "\n",
        "    return train_list, test_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvozRsR7jd25"
      },
      "source": [
        "The training takes over 20 mins. Please skip running below cells for now and come back when time allows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE01RDBdjd25"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "args = {'batch_size': 64,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 10,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'net_type': 'Net',\n",
        "        'anneal_type': 'linear',\n",
        "        'optimizer': 'sgd',\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otq3M6tCjd26"
      },
      "outputs": [],
      "source": [
        "optimizer = ['sgd', 'adagrad', 'rmsprop', 'adam']\n",
        "error_dict = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJoa3qWsjd26"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(len(optimizer)):\n",
        "    args['optimizer'] = optimizer[i]\n",
        "    train_error, test_error = optimizer_eval(args)\n",
        "    error_dict['train' + str(optimizer[i])] = train_error\n",
        "    error_dict['test' + str(optimizer[i])] = test_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDOoOxB-jd26"
      },
      "outputs": [],
      "source": [
        "  fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
        "  axs[0].plot(error_dict['trainsgd'], label='SGD', color='b')\n",
        "  axs[1].plot(error_dict['testsgd'], label='SGD', color='b', linestyle='dashed')\n",
        "  axs[0].plot(error_dict['trainadagrad'], label='Adagrad', color='r')\n",
        "  axs[1].plot(error_dict['testadagrad'], label='Adagrad', color='r', linestyle='dashed')\n",
        "  axs[0].plot(error_dict['trainrmsprop'], label='RMSprop', color='g')\n",
        "  axs[1].plot(error_dict['testrmsprop'], label='RMSprop', color='g', linestyle='dashed')\n",
        "  axs[0].plot(error_dict['trainadam'], label='Adam', color='orange')\n",
        "  axs[1].plot(error_dict['testadam'], label='Adam', color='orange', linestyle='dashed')\n",
        "  axs[0].set_title('Train')\n",
        "  axs[1].set_title('Test')\n",
        "  axs[0].set_ylabel('Error (%)')\n",
        "  #plt.yscale('log')\n",
        "  axs[0].set_xlabel('Epoch')\n",
        "  axs[1].set_xlabel('Epoch')\n",
        "  axs[0].legend()\n",
        "  axs[1].legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBXfaiNsjd26"
      },
      "source": [
        "Plot the train and test classification error curves of different optimizers by running below cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T9IdimZjd26"
      },
      "source": [
        "## Learning rate scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLrpycG0jd26"
      },
      "source": [
        "If the learning rate is too large, optimization diverges; if it is too small, it takes too long to train or we end up with a suboptimal result. People often start large learning rate and then 'decay' or 'anneal'(decrease) it.  This can help both optimization and generalization.\n",
        "\n",
        "Common beliefs in how annealing works come from the optimization analysis of stochastic gradient descent:\n",
        "\n",
        "1.   An initial large learning rate accelerates training or helps the network escape spurious local minima\n",
        "2.   Decaying the learning rate helps the network converge to a local minimum and avoid oscillation.\n",
        "\n",
        "The simplest learning rate schedule is to decrease the learning rate linearly from a large initial value to a small value. This allows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process. There are other schedules such as square root and exponential decay.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2eH8g3ijd26"
      },
      "source": [
        "### Compare different annealing schedules: constant, linear, sqrt(t) and exp(-t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yfpR42ejd26"
      },
      "source": [
        "Firstly, let's plot the simulation of different annealing scheduels: constant, linear, sqrt(t) and exp(-t) in below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBPIN31Hjd27"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Linear(2, 1)\n",
        "lr_anneal = ['constant', 'linear', 'sqrt', 'exp']\n",
        "lr_dict = defaultdict(list)\n",
        "\n",
        "for idx in range(len(lr_anneal)):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "    if lr_anneal[idx] == 'constant':\n",
        "        lambda1 = lambda epoch: 1\n",
        "    elif lr_anneal[idx] == 'linear':\n",
        "        lambda1 = lambda epoch: max(1e-7, 1 - 0.1*epoch)\n",
        "    elif lr_anneal[idx] == 'sqrt':\n",
        "        lambda1 = lambda epoch: (epoch + 1.0) ** -0.5\n",
        "    elif lr_anneal[idx] == 'exp':\n",
        "        lambda1 = lambda epoch: 0.1 ** epoch\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "    for i in range(10):\n",
        "        optimizer.step()\n",
        "        lr_dict[lr_anneal[idx]].append(optimizer.param_groups[0][\"lr\"])\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "plt.plot(range(10), lr_dict['constant'], label='Constant')\n",
        "plt.plot(range(10), lr_dict['linear'], label='Linear')\n",
        "plt.plot(range(10), lr_dict['sqrt'], label='Sqrt')\n",
        "plt.plot(range(10), lr_dict['exp'], label='Exp')\n",
        "plt.title('Annealing Schedules')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rICnd68Tjd2_"
      },
      "source": [
        "Now, check your assumption by running below digit classification example with different learning rate scheduelers: linear, sqrt(t) and exp(-t)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clr-P_2Hjd2_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.fc3 = nn.Linear(784, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAcdDOmPjd3A"
      },
      "outputs": [],
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    avg_loss, correct = (0., 0.)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        avg_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    avg_loss /= len(train_loader.dataset)\n",
        "    return 100. * correct / len(train_loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VVkJLnajd3A"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp5Tw2Rnjd3A"
      },
      "outputs": [],
      "source": [
        "def schedular_eval(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    if args['anneal_type'] == 'constant':\n",
        "        lambda1 = lambda epoch: 1\n",
        "    elif args['anneal_type'] == 'linear':\n",
        "        lambda1 = lambda epoch: max(1e-7, 1 -0.1 * epoch)\n",
        "    elif args['anneal_type'] == 'sqrt':\n",
        "        lambda1 = lambda epoch: (epoch + 1.0) ** -0.5\n",
        "    elif args['anneal_type'] == 'exp':\n",
        "        lambda1 = lambda epoch: 0.1 ** epoch\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        "    train_list, test_list = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        '''\n",
        "        if epoch > 1:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= 0.1\n",
        "        '''\n",
        "        train_acc = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_list.append(100.-train_acc)\n",
        "        test_acc = test(model, device, test_loader)\n",
        "        test_list.append(100.-test_acc)\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_list, test_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pefrux_jd3A"
      },
      "source": [
        "The training takes over 20 mins. Please skip running below cells for now and come back when (or if) time allows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDD0urJUjd3A"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "args = {'batch_size': 64,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 10,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'net_type': 'Net',\n",
        "        'anneal_type': 'linear',\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wrkwwbDjd3A"
      },
      "outputs": [],
      "source": [
        "lr_anneal = ['constant', 'linear', 'sqrt', 'exp']\n",
        "error_dict = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGjMVvNNjd3A"
      },
      "outputs": [],
      "source": [
        "for i in range(len(lr_anneal)):\n",
        "    args['anneal_type'] = lr_anneal[i]\n",
        "    train_error, test_error = schedular_eval(args)\n",
        "    error_dict['train' + str(lr_anneal[i])] = train_error\n",
        "    error_dict['test' + str(lr_anneal[i])] = test_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPbJUd8jd3B"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
        "axs[0].plot(error_dict['trainconstant'], label='Constant', color='b')\n",
        "axs[1].plot(error_dict['testconstant'], label='Constant', color='b', linestyle='dashed')\n",
        "axs[0].plot(error_dict['trainlinear'], label='Linear', color='r')\n",
        "axs[1].plot(error_dict['testlinear'], label='Linear', color='r', linestyle='dashed')\n",
        "axs[0].plot(error_dict['trainsqrt'], label='Sqrt', color='g')\n",
        "axs[1].plot(error_dict['testsqrt'], label='Sqrt', color='g', linestyle='dashed')\n",
        "axs[0].plot(error_dict['trainexp'], label='Exp', color='orange')\n",
        "axs[1].plot(error_dict['testexp'], label='Exp', color='orange', linestyle='dashed')\n",
        "axs[0].set_title('Train')\n",
        "axs[1].set_title('Test')\n",
        "axs[0].set_ylabel('Error (%)')\n",
        "#plt.yscale('log')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) ResNet\n",
        "Residual Networks can be used to improve the deep models in your experiments, particularly in tasks where vanishing gradients or overfitting are issues. Specifically ResNet architectures are inherently regularized due to residual connections. Incorporating them can complement techniques like dropout or batch normalization for better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "tjv-7_QC_Xc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "MelmkdiU_W02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a Residual Block, the core building block of ResNet. It includes:\n",
        "\n",
        "- Two fully connected layers with batch normalization.\n",
        "- A shortcut connection that adds the input to the output of the block, enabling residual learning"
      ],
      "metadata": {
        "id": "7Xeiv77P_60H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.linear1 = nn.Linear(in_features, out_features)\n",
        "        self.bn1 = nn.BatchNorm1d(out_features)\n",
        "        self.linear2 = nn.Linear(out_features, out_features)\n",
        "        self.bn2 = nn.BatchNorm1d(out_features)\n",
        "        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.linear1(x)))\n",
        "        out = self.bn2(self.linear2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n"
      ],
      "metadata": {
        "id": "iNewSflS_4Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the ResNet Optimization Model, which stacks the residual blocks for feature extraction. The model architecture:\n",
        "\n",
        "- Uses two residual blocks.\n",
        "- Outputs probabilities using a softmax layer."
      ],
      "metadata": {
        "id": "4f2DpoGyAJ0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetOptimizationModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ResNetOptimizationModel, self).__init__()\n",
        "        # Residual Blocks\n",
        "        self.res_block1 = ResidualBlock(input_dim, 128)\n",
        "        self.res_block2 = ResidualBlock(128, 64)\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "        return F.log_softmax(self.output_layer(x), dim=1)\n"
      ],
      "metadata": {
        "id": "MHzu5SH9AJNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Evaluation"
      ],
      "metadata": {
        "id": "A9gzD0zgAsgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_resnet(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    avg_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.view(data.size(0), -1).to(device), target.to(device)  # Flatten data\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        avg_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
        "                  f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "    avg_loss /= len(train_loader.dataset)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "-weLi2m2AVnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_resnet(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.view(data.size(0), -1).to(device), target.to(device)  # Flatten data\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
        "          f\"({accuracy:.2f}%)\\n\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "HZN_xYBaAXj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimization_resnet(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Load MNIST data\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\"../data\", train=True, download=True, transform=transform),\n",
        "        batch_size=args['batch_size'], shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(\"../data\", train=False, transform=transform),\n",
        "        batch_size=args['test_batch_size'], shuffle=False\n",
        "    )\n",
        "\n",
        "    # Initialize ResNet model\n",
        "    input_dim = 28 * 28  # MNIST images are 28x28\n",
        "    model = ResNetOptimizationModel(input_dim, num_classes=10).to(device)\n",
        "\n",
        "    # Set optimizer\n",
        "    if args['optimizer'] == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "    elif args['optimizer'] == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    elif args['optimizer'] == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args['lr'])\n",
        "    elif args['optimizer'] == 'adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    train_losses, test_accuracies = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train_loss = train_resnet(args, model, device, train_loader, optimizer, epoch)\n",
        "        test_accuracy = test_resnet(model, device, test_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "    return train_losses, test_accuracies\n"
      ],
      "metadata": {
        "id": "TiZUc4Y7AcL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run and compare the results"
      ],
      "metadata": {
        "id": "F-XqwpT3AjuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment parameters\n",
        "args = {\n",
        "    'batch_size': 64,\n",
        "    'test_batch_size': 1000,\n",
        "    'epochs': 10,\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "    'optimizer': 'adam',  # Change to 'sgd', 'adagrad', or 'rmsprop' for comparison\n",
        "    'no_cuda': False,\n",
        "    'seed': 1,\n",
        "    'log_interval': 100\n",
        "}\n",
        "\n",
        "# Run experiments with different optimizers\n",
        "optimizers = ['sgd', 'adam', 'rmsprop', 'adagrad']\n",
        "results = {}\n",
        "\n",
        "for opt in optimizers:\n",
        "    print(f\"\\nRunning experiment with optimizer: {opt}\")\n",
        "    args['optimizer'] = opt\n",
        "    train_losses, test_accuracies = optimization_resnet(args)\n",
        "    results[opt] = {\n",
        "        'train_losses': train_losses,\n",
        "        'test_accuracies': test_accuracies\n",
        "    }\n",
        "\n",
        "# Visualize results\n",
        "for opt in optimizers:\n",
        "    plt.plot(results[opt]['test_accuracies'], label=opt)\n",
        "\n",
        "plt.title(\"ResNet Optimization Comparison\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Test Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "enmxGebkAeIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCoDPowjd3B"
      },
      "source": [
        "## Concluding Optimisation\n",
        "\n",
        "So we've seen a bunch of different optimizing methods, Batch Normalisation layers, and annealing methods which control the way the optimisation tapers off. Usually, we focus on optimisation technique and fine tuning after we've decided on the model we want to use on the data, though it is worth keeping an eye on the general optimisation techniques when we are training (such as the kind of optimiser we will be choosing). Optimisation works in tandem with the second set of topics we will be exploring, Regularisation, in part of being the key ways we truly master training a model. Model architecture and deciding the right kind of model for your data is crucial, but optimisation and regularisation is what it make the model ready for production (if that is your goal!).\n",
        "\n",
        "So while optimisation dealt with the way our model back propagates information about changing the model weights based on the predictions, *regularisation* deals with ways we can allow our model to generalise better, and avoid over-fitting our model, and allowing it to be flexible in the way it deals with new data. In the following section, we will deal with multiple ways to regularise our data. We will continue dealing with image based models, but will move to a different dataset which is more complex, of animal faces.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkF3el8Zjd3B"
      },
      "source": [
        "## Keras Options\n",
        "\n",
        "Here are set of links and resources for optimisation with Keras.\n",
        "\n",
        "[Keras Optimizer Documentation](https://keras.io/api/optimizers/)\n",
        "\n",
        "[Blog post on Keras optimizer options](https://machinelearningknowledge.ai/keras-optimizers-explained-with-examples-for-beginners/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RZx0jBb-ohUS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp5MEdS_ohlF"
      },
      "source": [
        "# Homework Assignments\n",
        "\n",
        "**1)** Build or reuse an older model you have built, and use 3 different optimization techniques or methods on the model. For one of these models, add a batch normalization layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn90XecvouNY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBqzUaSUohlF"
      },
      "source": [
        "**2)** Which optimizer performed the best? How did Batch Normalization method effect your model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORYo2jV_ouNY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4CEq6DLjd3B"
      },
      "source": [
        "# Module 3: Regularisation\n",
        "\n",
        "Regularizing neural networks involves implementing techniques to prevent overfitting and improve the model's ability to generalize to new data. These techniques help ensure the network learns meaningful patterns rather than memorizing the training data, ultimately leading to better performance on unseen examples.\n",
        "\n",
        "Useful links:\n",
        "\n",
        "- [Chapter 7, deep learning book](https://www.deeplearningbook.org/contents/regularization.html)\n",
        "- [Slides and Code for PyTorch regularisation from Sebastian Raschka](https://github.com/rasbt/stat479-deep-learning-ss19/tree/master/L10_regularization)\n",
        "- [towards data science blog post](https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)\n",
        "- [medium blog post](https://medium.com/@dhartidhami/regularization-in-deep-learning-2065b7c889e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zkZeFDXjd3B"
      },
      "source": [
        "## Setup\n",
        "Note that some of the code for today can take up to an hour to run. We have therefore \"hidden\" that code and shown the resulting outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc5ew-9ujd3B"
      },
      "outputs": [],
      "source": [
        "#@title Import functions\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import pathlib\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiWvZDBRjd3C"
      },
      "outputs": [],
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline\n",
        "fig_w, fig_h = (8, 6)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "SMALL_SIZE = 12\n",
        "\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "plt.rc('animation', html='jshtml')\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=SMALL_SIZE)  # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFajuDoSjd3C"
      },
      "source": [
        "## Loading Animal Faces data\n",
        "\n",
        "The following cells setup our Animal Faces data source. If you run into trouble with any of these commands,\n",
        "you can download the zip files directly from the github repos and unzip them manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXMZfBUKjd3C"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!rm -r AnimalFaces32x32/\n",
        "!git clone https://github.com/arashash/AnimalFaces32x32\n",
        "!rm -r afhq/\n",
        "!unzip ./AnimalFaces32x32/afhq_32x32.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xTGXgJbjd3C"
      },
      "outputs": [],
      "source": [
        "# @title Loading Animal Faces Randomized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxf3hNNejd3C"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!rm -r Animal_faces_random/\n",
        "!git clone https://github.com/Ravi3191/Animal_faces_random.git\n",
        "!rm -r afhq_random_32x32/\n",
        "!unzip ./Animal_faces_random/afhq_random_32x32.zip\n",
        "!rm -r afhq_10_32x32/\n",
        "!unzip ./Animal_faces_random/afhq_10_32x32.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WERtNEcjd3C"
      },
      "outputs": [],
      "source": [
        "#@title Seeding for Reproducibility\n",
        "seed = 90108\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.use_deterministic_algorithms(True)\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = seed % (worker_id+1)\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Y8Aq1ojd3C"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_djrf_xjd3C"
      },
      "source": [
        "Now, lets define a Animal Net model, train, test and main functions which we will use quite frequently this week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwJqxdHbjd3D"
      },
      "outputs": [],
      "source": [
        "##Network Class - Animal Faces\n",
        "class Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(104)\n",
        "        super(Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RTaxyCHjd3D"
      },
      "source": [
        "The train function takes in the current model along with the train_loader and loss function and updates the parameters for a single pass of the entire dataset. The test function takes in the current model after every epoch and calculates the accuracy on the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FUe4RPcjd3D"
      },
      "outputs": [],
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch,reg_function1=None,reg_function2=None,criterion=F.nll_loss):\n",
        "    \"\"\"\n",
        "    Trains the current inpur model using the data\n",
        "    from Train_loader and Updates parameters for a single pass\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        if reg_function1 is None:\n",
        "            loss = criterion(output, target)\n",
        "        elif reg_function2 is None:\n",
        "            loss = criterion(output, target)+args['lambda']*reg_function1(model)\n",
        "        else:\n",
        "            loss = criterion(output, target)+args['lambda1']*reg_function1(model)+args['lambda2']*reg_function2(model)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK5Fjr0Njd3D"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader, loader = 'Test',criterion=F.nll_loss):\n",
        "    \"\"\"\n",
        "    Tests the current Model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yikH6TL2jd3D"
      },
      "outputs": [],
      "source": [
        "def main(args, model,train_loader,val_loader,test_data,reg_function1=None,reg_function2=None,criterion=F.nll_loss):\n",
        "    \"\"\"\n",
        "    Trains the model with train_loader and tests the learned model using val_loader\n",
        "    \"\"\"\n",
        "\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    val_acc_list, train_acc_list,param_norm_list = [], [], []\n",
        "    for epoch in tqdm(range(args['epochs'])):\n",
        "        train(args, model, device, train_loader, optimizer, epoch,reg_function1=reg_function1,reg_function2=reg_function2)\n",
        "        train_acc = test(model,device,train_loader, 'Train')\n",
        "        val_acc = test(model,device,val_loader, 'Val')\n",
        "        param_norm = calculate_frobenius_norm(model)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        param_norm_list.append(param_norm)\n",
        "\n",
        "    return val_acc_list, train_acc_list, param_norm_list, model, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7IhPvb9jd3D"
      },
      "source": [
        "One way to think about Regularization is to think in terms of the magnitude of the overall weights of the model. A model with big weights can fit more data perfectly. Wheras a model with smaller weights tend to underperform on the train set but can suprisingly do very well on the test set. Too small weights can also be as issue and it can the underfit the model.\n",
        "\n",
        "This week we use the sum of Frobenius Norm of all the tensors in the model as a metric to measure the \"size of the model\".\n",
        "\n",
        "### Frobenius Norm\n",
        "Before we start let us do a quick recollection of Frobenius Norm. The Frobenius norm, sometimes also called the Euclidean norm (a term also used for the vector $L^2$ norm--its a high dimensional generatilization!), is matrix norm of an m×n matrix A defined as the square root of the sum of the absolute squares of its elements.\n",
        "\\begin{equation}\n",
        "||A||_F= \\sqrt(\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|^2)\n",
        "\\end{equation}\n",
        "\n",
        "Let's implement this so we have it handy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHGP8dQjd3D"
      },
      "outputs": [],
      "source": [
        "def calculate_frobenius_norm(model):\n",
        "    norm = 0.0\n",
        "\n",
        "    # Sum all the parameters\n",
        "    for param in model.parameters():\n",
        "        norm += torch.sum(param**2)\n",
        "\n",
        "    # Take a square root of the sum of squares of all the parameters\n",
        "    norm = norm**0.5\n",
        "    return norm\n",
        "\n",
        "net = nn.Linear(10,1)\n",
        "print(f'Frobenius Norm of Single Linear Layer: {calculate_frobenius_norm(net)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkRfCpB6jd3E"
      },
      "source": [
        "We use the sum of Frobenius Norm of all the tensors in the model as a metric to measure the \"size of the model\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fgDHKCjjd3E"
      },
      "source": [
        "## Overfitting and Memorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ARpwqRvjd3E"
      },
      "source": [
        "Neural networks with a high number of parameters are prone to overfitting on the training data. Overfitting is when we have low bias and high variance - the model is able to model the training data well but generalises poorly.\n",
        "\n",
        "In principle, we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?\n",
        "\n",
        "Note that there is another kind of overfitting: you do \"honest\" fitting on one set of images or posts, or medical records, but it may not generalize to other sets of images, posts or medical records.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmHmu0m0jd3E"
      },
      "source": [
        "### Validation Dataset\n",
        "\n",
        "A common practice to address this problem is to split our data three ways, using a validation dataset (or validation set) to tune the hyperparameters.\n",
        "\n",
        "Ideally we would only touch the test data once, to assess the very best model or to compare a small number of models to each other. In the real-world, test data is seldom discarded after just one use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM6Lo2kpjd3E"
      },
      "source": [
        "### Does a neural network memorize?\n",
        "\n",
        "Given sufficiently large networks and enough training, Neural Networks can acheive almost 100% train accuracy.\n",
        "\n",
        "In this section we train three MLP's one each on:\n",
        "\n",
        "\n",
        "1.   Animal Faces Dataset\n",
        "2.   Completely Noisy Dataset (Random Shuffling of all labels)\n",
        "3.   Partially Noisy Dataset (Random Shuffling of 15% labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1HPnvzXjd3E"
      },
      "source": [
        "Now, let's create the required dataloaders for all the three datasets. Take a quick look at how we split the data. We train on a fraction of the dataset as it will be easier to train and also visualize overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tspMIzw2jd3E"
      },
      "outputs": [],
      "source": [
        "##Dataloaders for the Dataset\n",
        "batch_size = 128\n",
        "classes = ('cat', 'dog', 'wild')\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "\n",
        "\n",
        "####################################################\n",
        "\n",
        "##Dataloaders for the  Original Dataset\n",
        "\n",
        "\n",
        "img_train_data, img_val_data,_ = torch.utils.data.random_split(img_dataset, [100,100,14430])\n",
        "\n",
        "#Creating train_loader and Val_loader\n",
        "train_loader = torch.utils.data.DataLoader(img_train_data,batch_size=batch_size,worker_init_fn=seed_worker)\n",
        "val_loader = torch.utils.data.DataLoader(img_val_data,batch_size=1000,worker_init_fn=seed_worker)\n",
        "\n",
        "#creating test dataset\n",
        "test_transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "img_test_dataset = ImageFolder(data_path/'val', transform=test_transform)\n",
        "\n",
        "\n",
        "####################################################\n",
        "\n",
        "##Dataloaders for the  Random Dataset\n",
        "\n",
        "#splitting randomized data into training and validation data\n",
        "data_path = pathlib.Path('.')/'afhq_random_32x32/afhq_random' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "random_img_train_data, random_img_val_data,_ = torch.utils.data.random_split(img_dataset, [100,100,14430])\n",
        "\n",
        "#Randomized train and validation dataloader\n",
        "rand_train_loader = torch.utils.data.DataLoader(random_img_train_data,batch_size=batch_size,num_workers = 0, worker_init_fn=seed_worker)\n",
        "rand_val_loader = torch.utils.data.DataLoader(random_img_val_data,batch_size=1000,num_workers = 0,worker_init_fn=seed_worker)\n",
        "\n",
        "####################################################\n",
        "\n",
        "##Dataloaders for the Partially Random Dataset\n",
        "\n",
        "#Splitting data between training and validation dataset for partially randomized data\n",
        "data_path = pathlib.Path('.')/'afhq_10_32x32/afhq_10' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "partially_random_train_data, partially_random_val_data,_ = torch.utils.data.random_split(img_dataset, [100,100,14430])\n",
        "\n",
        "#Training and Validation loader for partially randomized data\n",
        "partial_rand_train_loader = torch.utils.data.DataLoader(partially_random_train_data,batch_size=batch_size,num_workers = 0,worker_init_fn=seed_worker)\n",
        "partial_rand_val_loader = torch.utils.data.DataLoader(partially_random_val_data,batch_size=1000,num_workers = 0,worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfoqKkFrjd3F"
      },
      "source": [
        "Now let's define a model which has a very high number of parameters when compared with the training data points and train it on all these datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCc033q3jd3F"
      },
      "outputs": [],
      "source": [
        "##Network Class - Animal Faces\n",
        "class Big_Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(104)\n",
        "        super(Big_Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 124)\n",
        "        self.fc2 = nn.Linear(124, 64)\n",
        "        self.fc3 = nn.Linear(64, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrgOSOz8jd3F"
      },
      "outputs": [],
      "source": [
        "##Here we have 100 true train data.\n",
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy76JrCPjd3F"
      },
      "outputs": [],
      "source": [
        "model = Big_Animal_Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNho7Tkljd3F"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "val_acc_pure, train_acc_pure, _, model ,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Time to memorize the dataset:\",end_time - start_time)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_pure,label='Val Accuracy Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train Accuracy Pure',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_pure),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvxG-Agyjd3F"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_acc_pure,label='Val Accuracy Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train Accuracy Pure',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_pure),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AJH0aS2jd3F"
      },
      "source": [
        "### Data Visualizer\n",
        "Before we proceed to train the model on a data with random labels, let us visualize and verify for ourselves if the data is random or not. Here, we have classes = (\"cat\",\"dog\",\"wild\").\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK4zGXNejd3G"
      },
      "outputs": [],
      "source": [
        "def Visualize_data(dataloader):\n",
        "  \"\"\"\n",
        "    Inputs: Pytorch Dataloader\n",
        "    It visualizes the images in the dataset and the classes they belong to.\n",
        "  \"\"\"\n",
        "\n",
        "  for idx,(data,label) in enumerate(dataloader):\n",
        "\n",
        "    plt.figure(idx)\n",
        "    #Choose the datapoint you would like to visualize\n",
        "    index = 22\n",
        "\n",
        "    #choose that datapoint using index and permute the dimensions and bring the pixel values between [0,1]\n",
        "    data = data[index].permute(1,2,0)* torch.tensor([0.5,0.5,0.5]) + torch.tensor([0.5,0.5,0.5])\n",
        "\n",
        "    #Convert the torch tensor into numpy\n",
        "    data = data.numpy()\n",
        "\n",
        "    plt.imshow(data)\n",
        "    image_class = classes[label[index].item()]\n",
        "    print(f'The image belongs to : {image_class}')\n",
        "\n",
        "  plt.show()\n",
        "Visualize_data(rand_train_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgb7ylyOjd3G"
      },
      "source": [
        "We can see that the model is mixed up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uht3gRSAjd3G"
      },
      "outputs": [],
      "source": [
        "##Here we have 100 completely shuffled train data.\n",
        "args = {'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Big_Animal_Net()\n",
        "\n",
        "\n",
        "val_acc_random, train_acc_random, _,model,_ = main(args,model,rand_train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_random,label='Val Accuracy random',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_random,label='Train Accuracy random',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_random),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEsGaXCjd3G"
      },
      "source": [
        "Finally lets train on a partially shuffled dataset where 15% of the labels are noisy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDSm7cz7jd3G"
      },
      "outputs": [],
      "source": [
        "##Here we have 100 partially shuffled train data.\n",
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Big_Animal_Net()\n",
        "\n",
        "\n",
        "val_acc_shuffle, train_acc_shuffle, _,_,_ = main(args,model,partial_rand_train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "#train and test acc plot\n",
        "plt.plot(val_acc_shuffle,label='Val Accuracy shuffle',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_shuffle,label='Train Accuracy shuffle',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_shuffle),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQHRtHSxjd3G"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_acc_pure,label='Val - Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train - Pure',c='red',ls = 'solid')\n",
        "plt.plot(val_acc_random,label='Val - Random',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_random,label='Train - Random',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_shuffle,label='Val 15% shuffle',c='green',ls = 'dashed')\n",
        "plt.plot(train_acc_shuffle,label='Train 15% shuffle',c='green',ls = 'solid')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrNoU3tIjd3G"
      },
      "source": [
        "Given that the NN fit/memorize the training data perfectly, Do you think it generalizes well? What makes you think it does or doesn't?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYZepXlrjd3H"
      },
      "source": [
        "Isn't it supprising to see that the NN was able to acheive 100% train accuracy on randomly shuffled labels. This is one of the reasons why training accuracy is not a good indicator of model performance.\n",
        "\n",
        "Also it is interesting to note that sometimes the model trained on slightly shuffled data does slightly better than the one trained on pure data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUe-4Co5jd3H"
      },
      "source": [
        "##Early Stopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJsejU0Tjd3H"
      },
      "source": [
        "\n",
        "Now that we have established that the validation accuracy reaches the peak well before the model overfits we want to somehow stop the training early. You should have also observed from the above plots that the train/test loss on real data is not very smooth and hence you might guess that the choice of epoch can play a very large role on the val/test accuracy of your model.\n",
        "\n",
        "Early stopping is a way to end training when the validation accuracies do not increase for over a certain number of epochs. Though this makes sure that we do not overfit on the train data we still haven't solved the problem of local variance. To overcome this we also save the best model based on the val loss/accuracy for use on test dataset.\n",
        "\n",
        "![Overfitting](https://images.deepai.org/glossary-terms/early-stopping-machine-learning-5422207.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRuZdbqqjd3H"
      },
      "source": [
        "The following function figures out the epoch best suited for stopping early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O7y6Hncjd3H"
      },
      "outputs": [],
      "source": [
        "def early_stopping_main(args,model,train_loader,val_loader,test_data):\n",
        "\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            Model: Pytorch model\n",
        "            Loaders: Pytorch Train and Validation loaders\n",
        "        The function trains the model and terminates the training based on the early stopping criterion.\n",
        "    \"\"\"\n",
        "\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    patience = 20\n",
        "    wait = 0\n",
        "\n",
        "    best_acc  = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    val_acc_list, train_acc_list = [], []\n",
        "    for epoch in tqdm(range(args['epochs'])):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_acc = test(model,device,train_loader, 'Train')\n",
        "        val_acc = test(model,device,val_loader, 'Val')\n",
        "        if (val_acc > best_acc):\n",
        "          best_acc = val_acc\n",
        "          best_epoch = epoch\n",
        "          best_model = copy.deepcopy(model)\n",
        "          wait = 0\n",
        "        else:\n",
        "          wait += 1\n",
        "        if (wait > patience):\n",
        "          print('early stopped on epoch:',epoch)\n",
        "          break\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "    return val_acc_list, train_acc_list, best_model, best_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qu0PbVcjd3H"
      },
      "outputs": [],
      "source": [
        "args = {'epochs': 200,\n",
        "        'lr': 5e-4,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_pure, train_acc_pure,_,_ ,best_epoch = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "model = Animal_Net()\n",
        "val_acc_earlystop, train_acc_earlystop,_,best_epoch = early_stopping_main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "print(\"Maximum Validation Accuracy is reached at epoch:%2d\"%(best_epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZXViy7wjd3H"
      },
      "source": [
        "Do you think Early stopping can be harmful for the training of your network?Discuss among your pod why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sniz4S7Wjd3H"
      },
      "source": [
        "##L1/LASSO Regularization\n",
        "\n",
        "Some of you might have already come across L1 and L2 regularization before in other courses. L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\n",
        "\n",
        "***Cost function = Loss (say, binary cross entropy) + Regularization term***\n",
        "\n",
        "Due to the addition of this regularization term, the values of parameters decrease because it assumes that a neural network with a lower parameter values leads to simpler models. Therefore, it will also reduce overfitting to quite an extent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y1L0FsDjd3J"
      },
      "source": [
        "L1 Regularization uses a Regularization Function which is the sum of the absolute value of all the weights in DLN, resulting in the following loss function ( L  is the usual Cross Entropy loss):\n",
        "\n",
        "\\begin{equation}\n",
        "L_R=L+λ∑|w^{(r)}_{ij}|\n",
        "\\end{equation}\n",
        "\n",
        "At a high level L1 Regularization is similar to L2 Regularization since it leads to smaller weights (you will see the analogy in the next subsection). It results in the following weight update equation when using Stochastic Gradient Descent (where  sgn  is the sign function, such that  sgn(w)=+1  if  w>0 ,  sgn(w)=−1  if  $w<0$ , and sgn(0)=0 ):\n",
        "\n",
        "\\begin{equation}\n",
        "w^{(r)}_{ij}←w^{(r)}_{ij}−ηλsgn(w^{(r)}_{ij})−η\\frac{\\partial L}{\\partial w_{ij}^{r}}\n",
        "\\end{equation}\n",
        "\n",
        "In the code which follows we will create an unregularised model, a L1 model, L2 model, and an elastic model and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_oFYSSjd3J"
      },
      "source": [
        "##Unregularized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrBwoPXhjd3J"
      },
      "outputs": [],
      "source": [
        "# Dataloaders for Regularization\n",
        "\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "\n",
        "#Splitting dataset\n",
        "reg_train_data, reg_val_data,_ = torch.utils.data.random_split(img_dataset, [30,100,14500])\n",
        "\n",
        "#Creating train_loader and Val_loader\n",
        "reg_train_loader = torch.utils.data.DataLoader(reg_train_data,batch_size=batch_size,worker_init_fn=seed_worker)\n",
        "reg_val_loader = torch.utils.data.DataLoader(reg_val_data,batch_size=1000,worker_init_fn=seed_worker)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjH87ZSijd3J"
      },
      "source": [
        "Now let's train a model without any regularization and keep it aside as our bencmark for this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnuRL_1zjd3J"
      },
      "outputs": [],
      "source": [
        "args = {'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_unreg, train_acc_unreg,param_norm_unreg,_ ,_ = main(args, model, reg_train_loader, reg_val_loader, img_test_dataset)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_unreg,label='Val Accuracy',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_unreg,label='Train Accuracy',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_unreg),c = 'green',ls = 'dashed')\n",
        "plt.title('Unregularized Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_unreg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0GkG8PSjd3K"
      },
      "outputs": [],
      "source": [
        "def l1_reg(model):\n",
        "  \"\"\"\n",
        "    Inputs: Pytorch model\n",
        "    This function calculates the l1 norm of the all the tensors in the model\n",
        "  \"\"\"\n",
        "  l1 = 0\n",
        "\n",
        "  for param in model.parameters():\n",
        "    l1 += torch.sum(torch.abs(param))\n",
        "\n",
        "  return l1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlulJT8Ljd3K"
      },
      "outputs": [],
      "source": [
        "\n",
        "net = nn.Linear(20,20)\n",
        "print(f'L1 norm of the model: {l1_reg(net)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6QeHgUzjd3K"
      },
      "outputs": [],
      "source": [
        "# here is an example of model setting. But if you don't want to use their dataset, you may want to rewrite this part.\n",
        "class Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(104)\n",
        "        super(Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLVUowmgjd3K"
      },
      "source": [
        "## Lambda Parameter for Regularisation\n",
        "\n",
        "You can see below we have a new argument in the argument dict - the lambda value. This is the same lambda we saw in the L1 expression above, and we will see it again with the other regularisation methods. Lamda values often range from 0-5. We start with a value of 0.1, and the ideal way to identify the lambda value is to do a grid optimisation (i.e run it for various values and see what works best). 1 is considered a large lambda value, and often we search between [0.1, 0.2 ... 0.9, 1, 2, 5].\n",
        "\n",
        "Here is a useful Stackoverflow link with information related to the topic - [calculating lambda value](https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxeHk561jd3K"
      },
      "outputs": [],
      "source": [
        "args = {'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        'lambda': 0.1\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_l1reg, train_acc_l1reg,param_norm_l1reg,_,_ = main(args, model, reg_train_loader, reg_val_loader, img_test_dataset, reg_function1=l1_reg)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_l1reg,label='Val Accuracy L1 Regularized',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l1reg,label='Train Accuracy L1 regularized',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1reg),c = 'green',ls = 'dashed')\n",
        "plt.title('L1 regularized model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_l1reg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVYUaCs2jd3K"
      },
      "source": [
        "##L2 / Ridge Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQuFLS5jd3L"
      },
      "source": [
        "L2 Regularization is a commonly used technique in ML systems is also sometimes referred to as “Weight Decay”. It works by adding a quadratic term to the Cross Entropy Loss Function  L , called the Regularization Term, which results in a new Loss Function  LR  given by:\n",
        "\n",
        "\\begin{equation}\n",
        "LR=L+λ∑(w^{(r)}_{ij})^2\n",
        "\\end{equation}\n",
        "\n",
        "In order to get further insight into L2 Regularization, we investigate its effect on the Gradient Descent based update equations for the weight and bias parameters. Taking the derivative on both sides of the above equation, we obtain\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial L_r}{\\partial w^{(r)}_{ij}}=\\frac{\\partial L}{\\partial w^{(r)}_{ij}}+λw^{(r)}_{ij}\n",
        "\\end{equation}\n",
        "Thus the weight update rule becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "w^{(r)}_{ij}←w^{(r)}_{ij}−η\\frac{\\partial L}{\\partial W^{(r)}_{ij}}−ηλw^{(r)}_{ij}=(1−ηλ)w^{(r)}_{ij}−η\\frac{\\partial L}{\\partial w^{(r)}_{ij}}\n",
        "\\end{equation}\n",
        "\n",
        "where, $\\eta$ is learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIM8T2-Kjd3L"
      },
      "outputs": [],
      "source": [
        "def l2_reg(model):\n",
        "\n",
        "  \"\"\"\n",
        "    Inputs: Pytorch model\n",
        "    This function calculates the l2 norm of the all the tensors in the model\n",
        "  \"\"\"\n",
        "\n",
        "  l2 = 0.0\n",
        "\n",
        "  for param in model.parameters():\n",
        "    l2 += torch.sum(torch.abs(param)**2)\n",
        "\n",
        "  return l2\n",
        "\n",
        "net = nn.Linear(20,20)\n",
        "print(f'L2 norm of the model: {l2_reg(net)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDONCZbjd3L"
      },
      "source": [
        "here they just run a L1 norm model and L2 norm model and see whether the accuracy increases (and visualize them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edq_jbPcjd3L"
      },
      "outputs": [],
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        'lambda': 0.1\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_l2reg, train_acc_l2reg,param_norm_l2reg,model ,_ = main(args,model,train_loader,val_loader,img_test_dataset,reg_function1=l2_reg)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_l2reg,label='Val Accuracy L2 regularized',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l2reg,label='Train Accuracy L2 regularized',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l2reg),c = 'green',ls = 'dashed')\n",
        "plt.title('L2 Regularized Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_l2reg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxt1uAqujd3L"
      },
      "source": [
        "##L1+L2 / Elastic net regularization\n",
        "\n",
        "Elastic Net regularization uses both L1 and L2 weights for regression. The loss function becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "LR=L+ λ_{1}∑|w^{(r)}_{ij}| + λ_{2}∑(w^{(r)}_{ij})^2\n",
        "\\end{equation}\n",
        "\n",
        "The weights updated equation then becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "w^{(r)}_{ij}←(1−ηλ_{2})w^{(r)}_{ij}−ηλ_{1}sgn(w^{(r)}_{ij})−η\\frac{\\partial L}{\\partial w_{ij}^{r}}\n",
        "\\end{equation}\n",
        "\n",
        "where, $\\eta$ is learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtQFFMZ6jd3L"
      },
      "outputs": [],
      "source": [
        "args = {'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        'lambda1':0.1,\n",
        "        'lambda2':0.1\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_l1l2reg, train_acc_l1l2reg,param_norm_l1l2reg,model ,_ = main(args,model,train_loader,val_loader,img_test_dataset,reg_function1=l1_reg,reg_function2=l2_reg)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_l1l2reg,label='Val L1+L2',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l1l2reg,label='Train L1+L2',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1l2reg),c = 'green',ls = 'dashed')\n",
        "plt.title('L1+L2 Regularized Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_l1l2reg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs-KyGrtjd3L"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_acc_l2reg,c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l2reg,label='L2 regularized',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l2reg),c = 'red',ls = 'dashed')\n",
        "plt.plot(val_acc_l1reg,c='green',ls = 'dashed')\n",
        "plt.plot(train_acc_l1reg,label='L1 regularized',c='green',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1reg),c = 'green',ls = 'dashed')\n",
        "plt.plot(val_acc_unreg,c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_unreg,label='Unregularized',c='blue',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_unreg),c = 'blue',ls = 'dashed')\n",
        "plt.plot(val_acc_l1l2reg,c='orange',ls = 'dashed')\n",
        "plt.plot(train_acc_l1l2reg,label='L1+L2 regularized',c='orange',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1l2reg),c = 'orange',ls = 'dashed')\n",
        "\n",
        "plt.title('Unregularized Vs L1-Regularized vs L2-regularized Vs L1+L2 regularized')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy(%)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPg6VaKsjd3M"
      },
      "source": [
        "### Alternative ways to implement Regularisation in PyTorch\n",
        "\n",
        "It is also possible to set up L2 regularisation (weight decay) by just using the ```weight_decay``` parameter in your optimisation method. Link to [optimisers documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), and [discussion on L1 and L2 implementations on stackoverflow](https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch).\n",
        "\n",
        "### Regularisation with Keras\n",
        "\n",
        "[Keras documentation for layer regularizers](https://keras.io/api/layers/regularizers/)\n",
        "\n",
        "[How to use weight decay (L2 reg) with Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySRWMIUjd3M"
      },
      "source": [
        "## Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciz62XENjd3M"
      },
      "source": [
        "### Dropout Implementation Caveats:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9U16DzVjd3M"
      },
      "source": [
        "\n",
        "*  Dropout is used only during training, during testing the complete model weights are used and hence it is important to use model.eval() before testing the model.\n",
        "\n",
        "* Dropout reduces the capacity of the model during training and hence as a general practice wider networks are used when using dropout. If you are using a dropout with a random probability of 0.5 then you might want to double the number of hidden neurons in that layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQpnq15Qjd3M"
      },
      "source": [
        "Now, lets see how Dropout fares on the Animal Faces Dataset. We first modify the existing model to include dropouts and then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEGdmJ62jd3M"
      },
      "outputs": [],
      "source": [
        "##Network Class - Animal Faces\n",
        "class Animal_Net_Dropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(32)\n",
        "        super(Animal_Net_Dropout, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 248)\n",
        "        self.fc2 = nn.Linear(248, 210)\n",
        "        self.fc3 = nn.Linear(210, 3)\n",
        "        self.dropout1 = nn.Dropout(p = 0.5)\n",
        "        self.dropout2 = nn.Dropout(p = 0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.leaky_relu(self.dropout1(self.fc1(x)))\n",
        "        x =F.leaky_relu(self.dropout2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtSfMlkMjd3M"
      },
      "outputs": [],
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'batch_size': 32,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net_Dropout()\n",
        "\n",
        "val_acc_dropout, train_acc_dropout, _, model ,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_pure,label='Val',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_dropout,label='Val - DP',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_dropout,label='Train - DP',c='red',ls = 'solid')\n",
        "plt.title('Dropout')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnrvwFUCjd3M"
      },
      "source": [
        "When do you think dropouts can perform bad and do you think their placement within a model matters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QvqxvtSjd3N"
      },
      "source": [
        "### Dropout for Keras\n",
        "\n",
        "We saw in the last tutorial how we can use dropout for Keras - similar to PyTorch, it is just adding a layer in your model.\n",
        "\n",
        "[Keras Documentation](https://keras.io/api/layers/regularization_layers/dropout)\n",
        "\n",
        "[Blog post on Dropout with Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMSfNjWmjd3N"
      },
      "source": [
        "## Data Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw1rHT9ijd3N"
      },
      "source": [
        "We will explore the effects of Data Augmentation on regularization. Here regularization is acheived by adding noise into training data after every epoch.\n",
        "\n",
        "Pytorch's torchvision module provides a few inbuilt data augmentation techniques which we can use on image datasets. Some of the techniques we most frequently use are:\n",
        "\n",
        "\n",
        "*   Random Crop\n",
        "*   Random Rotate\n",
        "*   Vertical Flip\n",
        "*   Horizontal Flip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi2bzfARjd3N"
      },
      "outputs": [],
      "source": [
        "##Data Augmentation using transforms\n",
        "new_transforms = transforms.Compose([\n",
        "                                     transforms.RandomHorizontalFlip(p=0.1),\n",
        "                                     transforms.RandomVerticalFlip(p=0.1),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=new_transforms)\n",
        "#Splitting dataset\n",
        "new_train_data, _,_ = torch.utils.data.random_split(img_dataset, [250,100,14280])\n",
        "\n",
        "#Creating train_loader and Val_loader\n",
        "new_train_loader = torch.utils.data.DataLoader(new_train_data,batch_size=batch_size,worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzVfsyXBjd3N"
      },
      "outputs": [],
      "source": [
        "args = {'epochs': 250,\n",
        "        'lr': 1e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQEDYqNYjd3N"
      },
      "outputs": [],
      "source": [
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_dataaug, train_acc_dataaug, param_norm_datadug, _ ,_ = main(args,model,new_train_loader,val_loader,img_test_dataset)\n",
        "model = Animal_Net()\n",
        "val_acc_pure, train_acc_pure, param_norm_pure,_,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_pure,label='Val Accuracy Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train Accuracy Pure',c='red',ls = 'solid')\n",
        "\n",
        "plt.plot(val_acc_dataaug,label='Val Accuracy data augment',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_dataaug,label='Train Accuracy data augment',c='blue',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_pure),c = 'red',ls = 'dashed')\n",
        "plt.axhline(y=max(val_acc_dataaug),c = 'blue',ls = 'dashed')\n",
        "plt.title('Data Augmentation')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmKHvHmJjd3O"
      },
      "outputs": [],
      "source": [
        "param_norm_pure = [tensor.detach().numpy() for tensor in param_norm_pure]\n",
        "param_norm_datadug = [tensor.detach().numpy() for tensor in param_norm_datadug]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-hseHoAjd3O"
      },
      "outputs": [],
      "source": [
        "plt.plot(param_norm_pure,c='red',label = 'Without Augmentation')\n",
        "plt.plot(param_norm_datadug,c='blue',label='With Augmentation')\n",
        "plt.title('Norm of parameters as a function of training epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Norm of model parameters')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoMRBBVKjd3O"
      },
      "source": [
        "### Data Augmentation for Keras\n",
        "\n",
        "MixUp augmentation for image classification - https://keras.io/examples/vision/mixup/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFAiz-u0jd3O"
      },
      "source": [
        "## Batch Size\n",
        "Batch size, in some cases, can also help in regularizing the models. Lower batch size leads to a noisy convergence and hence helps in converging to a broader local minima. Whereas, higher batch size lead to a smoother convergence thereby converging easily to a  deeper local minima.  This can be good or bad.\n",
        "\n",
        "In the below blcok we will train the Animal Net model with different batch sizes and see how that is going to affect the regularization performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtS8HMgWjd3O"
      },
      "outputs": [],
      "source": [
        "#@title Dataset for Batch_size\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "\n",
        "#Splitting dataset\n",
        "reg_train_data, reg_val_data,_ = torch.utils.data.random_split(img_dataset, [250,100,14280])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHrr2cdKjd3O"
      },
      "outputs": [],
      "source": [
        "args = {'lr': 5e-3,\n",
        "        'epochs': 60,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False\n",
        "        }\n",
        "\n",
        "batch_sizes = [32,64,128]\n",
        "acc_dict = {}\n",
        "\n",
        "for i in range(len(batch_sizes)):\n",
        "    model = Animal_Net()\n",
        "    #Creating train_loader and Val_loader\n",
        "    reg_train_loader = torch.utils.data.DataLoader(reg_train_data,batch_size=batch_sizes[i],worker_init_fn=seed_worker)\n",
        "    reg_val_loader = torch.utils.data.DataLoader(reg_val_data,batch_size=1000,worker_init_fn=seed_worker)\n",
        "    val_acc, train_acc,param_norm,_,_ = main(args,model,reg_train_loader,reg_val_loader,img_test_dataset)\n",
        "    acc_dict['train_'+str(i)] = train_acc\n",
        "    acc_dict['val_'+str(i)] = val_acc\n",
        "    acc_dict['param_norm'+str(i)] = param_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhzC5d5Zjd3O"
      },
      "outputs": [],
      "source": [
        "#Plot Train and Val curves\n",
        "plt.plot(acc_dict['train_0'], label='mb_size =' + str(batch_sizes[0]), c = 'blue')\n",
        "plt.plot(acc_dict['val_0'], linestyle='dashed', c = 'blue')\n",
        "\n",
        "plt.plot(acc_dict['train_1'], label='mb_size =' + str(batch_sizes[1]), c = 'orange')\n",
        "plt.plot(acc_dict['val_1'], linestyle='dashed', c = 'orange')\n",
        "plt.plot(acc_dict['train_2'], label='mb_size =' + str(batch_sizes[2]), c = 'green')\n",
        "plt.plot(acc_dict['val_2'], linestyle='dashed', c = 'green')\n",
        "print('maximum accuracy for mini batchsize = 32: '+str(max(acc_dict['val_0'])))\n",
        "print('maximum accuracy for mini batchsize = 64: '+str(max(acc_dict['val_1'])))\n",
        "print('maximum accuracy for mini batchsize = 128: '+str(max(acc_dict['val_2'])))\n",
        "\n",
        "plt.title('Optimal Batch Size')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch (Sec)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIkSW04Vjd3O"
      },
      "outputs": [],
      "source": [
        "acc_dict['param_norm0'] = [tensor.detach().numpy() for tensor in acc_dict['param_norm0']]\n",
        "acc_dict['param_norm1'] = [tensor.detach().numpy() for tensor in acc_dict['param_norm1']]\n",
        "acc_dict['param_norm2'] = [tensor.detach().numpy() for tensor in acc_dict['param_norm2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhufmh7Ijd3P"
      },
      "outputs": [],
      "source": [
        "# Plot Parametric Norms\n",
        "plt.plot(acc_dict['param_norm0'],c='blue',label='mb_size =' + str(batch_sizes[0]))\n",
        "plt.plot(acc_dict['param_norm1'],c='orange',label='mb_size =' + str(batch_sizes[1]))\n",
        "plt.plot(acc_dict['param_norm2'],c='green',label='mb_size =' + str(batch_sizes[2]))\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Parameter Norm')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E74aJtWojd3P"
      },
      "source": [
        "Here what observation can you make for different batch size. Why do you think this is happening?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUHJurxijd3P"
      },
      "source": [
        "## Pruning\n",
        "\n",
        "Google is known for training very big language models and recently it trained a [trillion paramter](https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/) model. This is almost 1.2e6 times bigger than the models we have been training. So it is sufficient to say that these big models need intense compute power to train while also becomeing harder to deploy and get real time inference on smaller micro - proccesors.\n",
        "\n",
        "This is where regularization and pruning come in very handy. Until now you should have noticed that the Frobenious norm of the regularized models that we trained tend to be smaller than those of unregualrized models. This indicates that the regualarization is shrinking the weights (making the model sparser) while improving the test performance.\n",
        "\n",
        "While methods like L1 regularization promote implicit sparsity, in pruning we explicitly set a few weights of the trained model to zero and then retrain the model to adjust the other weights. This reduces the memory consumption, improves inference and helps the planet :)\n",
        "\n",
        "One of the most common methods of pruning a NN is to zero out a certain percentage of parameters based on their L1 norm. We don't actually remove the parameters because that makes forward computation difficult.\n",
        "\n",
        "Luckily we have Pytorch's torch.nn.utils.prune methods to play around and test pruning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcqfZaKXjd3P"
      },
      "outputs": [],
      "source": [
        "def prune_l1_unstructured(model,prune_percent_weight,prune_percent_bias = 0):\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=prune_percent_weight)\n",
        "            prune.l1_unstructured(module, name='bias', amount=prune_percent_bias)\n",
        "\n",
        "            print(\n",
        "                \"Sparsity in {}: {:.2f}%\".format(name,\n",
        "                    100. * float(torch.sum(module.weight == 0))\n",
        "                    / float(module.weight.nelement())\n",
        "                )\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqLJytt_jd3P"
      },
      "outputs": [],
      "source": [
        "##uncomment to run the test\n",
        "test_model = Animal_Net()\n",
        "prune_percent = 0.15\n",
        "prune_l1_unstructured(test_model,0.15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9shrK21Yjd3P"
      },
      "outputs": [],
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Big_Animal_Net()\n",
        "prune_percent = 0.5\n",
        "\n",
        "print(\"Training a randomly initialized model\")\n",
        "val_acc, train_acc, _, trained_model ,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "##pruning a model\n",
        "print('Pruning and verifying and model:')\n",
        "prune_l1_unstructured(trained_model,prune_percent)\n",
        "\n",
        "#training the pruned model\n",
        "print(\"Training a pruned model\")\n",
        "val_acc_prune, train_acc_prune, _, pruned_model ,_ = main(args,trained_model.to('cpu'),train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "val_acc_prune = [val_acc_prune[0]]*args['epochs'] + val_acc_prune\n",
        "train_acc_prune = [train_acc_prune[0]]*args['epochs'] + train_acc_prune\n",
        "plt.plot(val_acc,label='Val',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc,label='Train',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_prune,label='Val Prune',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_prune,label='Train Prune',c='red',ls = 'solid')\n",
        "plt.title('Pruning')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXUlexnmjd3Q"
      },
      "source": [
        "Now change the prune_percent and report the percentage at which the model underfits. Let us say you create a new model with number of parameters equal to the number of parameters left after pruning. Do you think this model will work as well as the model which get after pruning the larger network? In the above pruning technique after pruning the network, how do you think the performance of the will change if we re-initialize the weights while maintaing the prune mask?\n",
        "\n",
        "### Pruning in Keras\n",
        "\n",
        "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIcXbR_pjd3Q"
      },
      "source": [
        "## Lottery Tickets\n",
        "\n",
        "\n",
        "The lottery ticket hypothesis claims that \" A dense randomly initialized NN contains a subnetwork that is initialzed such that when trained in isolation it can match the test accuracy of the original network after training for at most same number of iterations\" i.e. a pruned model when reinitialized with the same weights will can match the test accuracy of the denser model. If the initialization changes the accuracy match is no longer guaranteed.\n",
        "\n",
        "Here we train the following networks:\n",
        "\n",
        "An unregularized model with Xavier initialization of weights for 200 epochs\n",
        "A Pruned model with the weights reinitialized to the random values.\n",
        "A pruned model with weights initialized with same Xavier Initialization as unregularized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZuPqnY9jd3Q"
      },
      "outputs": [],
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 1e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "init_model = Big_Animal_Net()\n",
        "xavier_model = Big_Animal_Net()\n",
        "prune_percent = 0.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1bACSShjd3Q"
      },
      "outputs": [],
      "source": [
        "#Xavier Initilaization for one of the two models\n",
        "for name, module in xavier_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "print('Training the full model')\n",
        "val_acc, train_acc, _, trained_model ,_ = main(args,copy.deepcopy(xavier_model),train_loader,val_loader,img_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M7oIWR5jd3Q"
      },
      "outputs": [],
      "source": [
        "#prune the trained model\n",
        "prune_l1_unstructured(trained_model,prune_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G0MhjXUjd3Q"
      },
      "outputs": [],
      "source": [
        "#initialize masks for the initialzed model and xavier model\n",
        "for name, module in init_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        prune.identity(module, name='weight')\n",
        "        prune.identity(module, name='bias')\n",
        "\n",
        "for name, module in xavier_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        prune.identity(module, name='weight')\n",
        "        prune.identity(module, name='bias')\n",
        "\n",
        "init_modules = [[name,module] for name, module in init_model.named_modules()]\n",
        "xavier_modueles = [[name,module] for name, module in xavier_model.named_modules()]\n",
        "trained_modules = [[name,module] for name, module in trained_model.named_modules()]\n",
        "\n",
        "for i in range(len(init_modules)):\n",
        "    if isinstance(init_modules[i][1], torch.nn.Conv2d) or isinstance(init_modules[i][1], torch.nn.Linear):\n",
        "        init_modules[i][1].weight_mask = copy.deepcopy(trained_modules[i][1].weight_mask)\n",
        "        init_modules[i][1].bias_mask = copy.deepcopy(trained_modules[i][1].bias_mask)\n",
        "\n",
        "for i in range(len(xavier_modueles)):\n",
        "    if isinstance(xavier_modueles[i][1], torch.nn.Conv2d) or isinstance(xavier_modueles[i][1], torch.nn.Linear):\n",
        "        xavier_modueles[i][1].weight_mask = copy.deepcopy(trained_modules[i][1].weight_mask)\n",
        "        xavier_modueles[i][1].bias_mask = copy.deepcopy(trained_modules[i][1].bias_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1IwULj5jd3Q"
      },
      "outputs": [],
      "source": [
        "print('Training the pruned and Xavier model')\n",
        "val_acc_lottery_x, train_acc_lottery_x, _, pruned_model_x ,_ = main(args,xavier_model,train_loader,val_loader,img_test_dataset)\n",
        "print('Training the pruned Init model')\n",
        "val_acc_lottery, train_acc_lottery, _, pruned_model ,_ = main(args,init_model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "plt.plot(val_acc,c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc,label='Train - full model',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_lottery,c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_lottery,label='Train - Random',c='red',ls = 'solid')\n",
        "plt.plot(val_acc_lottery_x,c='green',ls = 'dashed')\n",
        "plt.plot(train_acc_lottery_x,label='Train - Xavier',c='green',ls = 'solid')\n",
        "plt.title('Lottery Tickets')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxxZJX98jd3R"
      },
      "source": [
        "### Lottery Ticket in Keras\n",
        "\n",
        "Keras Option: https://github.com/google-research/lottery-ticket-hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7orjoeIvjd3R"
      },
      "source": [
        "## Distillation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9-VVqRGjd3R"
      },
      "source": [
        "Bigger neural nets are better for model performance but require significant memory, while smaller networks tend be be less accurate but are easier to deploy and use.\n",
        "\n",
        "Distillation is a technique which allows us to train smaller networks such that they mimic the outputs of the bigger network. The bigger network is called the teacher network wheras the smaller one is the student network.\n",
        "\n",
        "Distillation begins by training a teacher network. It then trains the student network with both the original labels and \"soft\" labels--the output of the teacher model. This lets us train the student network on unlabelled datasets, using the labeled given by the teacher network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HUmI2J4jd3R"
      },
      "source": [
        "Let's begin by desiging a smaller network and training the parent model and also the small model using hard labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewWQjp0wjd3R"
      },
      "outputs": [],
      "source": [
        "class Small_Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(32)\n",
        "        super(Small_Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 32)\n",
        "        self.fc2 = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lavPU0-Ijd3R"
      },
      "outputs": [],
      "source": [
        "#Train the models\n",
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'lr' : 5e-3,\n",
        "        'cross_entropy':True\n",
        "        }\n",
        "\n",
        "Bmodel = Big_Animal_Net()\n",
        "Smodel = Small_Animal_Net()\n",
        "\n",
        "val_acc_big, train_acc_big, _, trained_big_model ,_ = main(args,Bmodel,train_loader,val_loader,img_test_dataset)\n",
        "val_acc_small, train_acc_small, _, _ ,_ = main(args,Smodel,train_loader,val_loader,img_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNWvR-F3jd3R"
      },
      "source": [
        "Loss Function:\n",
        "\n",
        "We use the same cross entropy loss as before but we use both hard and soft labels to calculate loss. We cannot directly use the CrossEntropy Loss in Pytorch to calculate loss for soft labels. Hence we will exploit the relation between Cross Entropy and KL Divergence.\n",
        "\n",
        "Cross Entropy loss and KL Divergence are both related by: H(p,q) = H(p) + KL(p,q) where H(p,q) calculates cross entropy loss between distributions p and q wheras KL represents KL divergence. Here p is the probability distribution of soft_outputs which are constant and hence we can omit from the loss function.\n",
        "\n",
        "        L = (1 - alpha)*CE(outputs,ground_truth) + alpha * (T**2) *CE(outputs,soft_targets)\n",
        "        L = (1 - alpha)*CE(outputs,ground_truth) + alpha * (T**2) *KL(outputs,soft_targets)\n",
        "\n",
        "Here alpha and temperature are hyper parameters where temperatures is used to smoothen the outputs of the parent network.\n",
        "\n",
        "[Click to learn more about the relation](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2S0vZGjjd3R"
      },
      "outputs": [],
      "source": [
        "def distillation_loss(args,soft_outputs, pred_logits, target):\n",
        "\n",
        "    alpha = args['alpha']\n",
        "    T = args['temperature']\n",
        "    dist_loss = (1. - alpha) * F.cross_entropy(pred_logits, target) + \\\n",
        "                    (alpha * (T ** 2)) * F.kl_div(F.log_softmax(pred_logits/T, dim=1),\n",
        "                             F.softmax(soft_outputs/T, dim=1))\n",
        "\n",
        "    return dist_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfE20WRYjd3R"
      },
      "outputs": [],
      "source": [
        "#Modified Train Functions\n",
        "def train_softmax_distillation(args, student_model,parent_model, device, train_loader, optimizer, epoch):\n",
        "\n",
        "    student_model.train()\n",
        "    parent_model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        soft_outputs = parent_model(data)\n",
        "        pred_logits = student_model(data)\n",
        "        loss = distillation_loss(args,soft_outputs.detach(), pred_logits, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Re32OLkjd3S"
      },
      "outputs": [],
      "source": [
        "#Modified Main Function\n",
        "def distilation_main(args, teacher_model,student_model,train_loader,val_loader):\n",
        "\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    student_model = student_model.to(device)\n",
        "    teacher_model = teacher_model.to(device)\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "    val_acc_list = []\n",
        "    train_acc_list = []\n",
        "    for epoch in tqdm(range(1, args['epochs'] + 1)):\n",
        "        train_softmax_distillation(args, student_model,teacher_model, device, train_loader, optimizer, epoch)\n",
        "        train_acc = test(student_model,device,train_loader,'Train')\n",
        "        val_acc = test(student_model,device,val_loader,'Val')\n",
        "        val_acc_list.append(val_acc)\n",
        "        train_acc_list.append(train_acc)\n",
        "\n",
        "    return val_acc_list, train_acc_list, student_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0kT2Hbjd3S"
      },
      "source": [
        "Now that we have everything ready let's train the student network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ox4jnEjd3S"
      },
      "outputs": [],
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'lr' : 5e-3,\n",
        "        'alpha': 1,\n",
        "        'temperature': 40\n",
        "        }\n",
        "\n",
        "student_model = Small_Animal_Net()\n",
        "\n",
        "val_acc_st, train_acc_st, _, = distilation_main(args,trained_big_model,student_model,train_loader,val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu4M6gFtjd3S"
      },
      "outputs": [],
      "source": [
        "# Plot the validation curves of student and the original small model\n",
        "plt.plot(val_acc_small,label='Val - Small Model',c='red',ls = 'dashed')\n",
        "plt.axhline(y = max(val_acc_small),c = 'red')\n",
        "plt.plot(val_acc_st,label='Val - Student Model',c='green',ls = 'dashed')\n",
        "plt.axhline(y = max(val_acc_st),c = 'green')\n",
        "plt.title('Distillation')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXpsnAf3jd3S"
      },
      "source": [
        "This method not only provides a way to train small and better networks but also gives us a chance to train small networks on more data using the soft labels from the heavily optimized parent networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A60pdgqijd3S"
      },
      "source": [
        "What other techniques can you use to reduce the dimensionality or size of the big network or the input data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWgJA5lWjd3S"
      },
      "source": [
        "Do you think regularization helps when you have infinite data available?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yttFMww1jd3V"
      },
      "source": [
        "Which regualarization technique from this week do you think had the biggest effect on the network and why do you think so? Can you apply all of them on the same network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH05Nge2jd3V"
      },
      "source": [
        "### Distillation in Keras\n",
        "\n",
        "https://keras.io/examples/vision/knowledge_distillation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtIiE3iBjd3V"
      },
      "source": [
        "# Homework Assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPbyj0U1jd3V"
      },
      "source": [
        "**1)** Build or reuse an older model you have built, and use 3 different Regularization techniques or methods on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD4jbkBWjd3V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwmkPCrsjd3W"
      },
      "source": [
        "**2)** Which regularisation technique worked best? How did each effect your model performance on out-of-sample test data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AqCP0w6jd3W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: LLM Fine-Tuning with LoRA and QLoRA\n",
        "\n",
        "**Summary: Fine-tuning neural networks involves taking a pre-trained model and adjusting its parameters allowing it to perform well on a specific task or dataset. This process sometimes involves (1) retraining all model weights, (2) unfreezing selected layers of the network and training them with a low learning rate on the new data, while keeping other layers frozen to preserve useful features learned during initial training, or (3) building adapators that translate the model weights through low-rank approaches, the purpose of this section. Fine-tuning is particularly valuable when working with limited data or computational resources, as it leverages knowledge transferred from the pre-trained model while allowing adaptation to the target task.**\n",
        "\n",
        "In this notebook, we'll explore low-rank adapter (LoRA) techniques for LLM fine-tuning, using GitHub repositories from:\n",
        "- [microsoft/LoRA](https://github.com/microsoft/LoRA)\n",
        "- [artidoro/qlora](https://github.com/artidoro/qlora)\n",
        "\n",
        "**Goals**:\n",
        "1. Understand low-rank adaptation methods (LoRA) and how they facilitate parameter-efficient fine-tuning of large models.\n",
        "2. Implement and test QLoRA, which also quantizes model weights (often to 4-bit precision) to reduce memory usage further.\n",
        "3. Experiment with training hyperparameters (learning rate, batch size) and regularization strategies (dropout, weight decay, etc.) to see how they affect downstream performance.\n",
        "4. Compare speed, memory footprint, and accuracy of standard full-parameter fine-tuning vs. LoRA vs. QLoRA.\n",
        "\n",
        "We'll show how to do this in PyTorch, with [Hugging Face Transformers](https://github.com/huggingface/transformers) for easy training loops on LLMs."
      ],
      "metadata": {
        "id": "2omYbPNl4z8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install & Imports\n",
        "First, we install any relevant libraries (if not already installed). This code will:\n",
        "1. Install Hugging Face Transformers (for LLM loading & training).\n",
        "2. Install bitsandbytes (if using QLoRA or 4-bit quantization).\n",
        "3. Install datasets or any other library you need."
      ],
      "metadata": {
        "id": "ZA9G55dF6ACP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# If you are using bitsandbytes-based 4-bit quantization:\n",
        "# import bitsandbytes as bnb\n",
        "\n",
        "# For demonstration, let's just pick a small-ish model to test\n",
        "# so we can see how LoRA/QLoRA steps look without massive compute.\n",
        "MODEL_NAME = \"facebook/galactica-125m\"  # for example, or \"EleutherAI/gpt-neo-125M\""
      ],
      "metadata": {
        "id": "zt2VOrH65aJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background: LoRA & QLoRA\n",
        "\n",
        "# **LoRA (Low-Rank Adaptation)**:\n",
        "- Paper: _Hu et al. (2022) “LoRA: Low-Rank Adaptation of Large Language Models.” ICLR._\n",
        "- Key idea: Freeze the original model weights, inject trainable low-rank matrices (the \"adapters\") into attention or MLP layers. Only the adapter weights are updated. This drastically reduces parameter count for fine-tuning, so we can fit training on a single GPU or smaller hardware.\n",
        "\n",
        "# **QLoRA**:\n",
        "- Paper: _Dettmers et al. (2023) “QLoRA: Efficient Finetuning of Quantized LLMs.”_\n",
        "- Key idea: Quantize model weights (e.g., 4-bit) for forward/backward pass, and then add LoRA adapters. The base model remains in quantized form (saving memory), while the LoRA adapters (in higher precision) get trained.\n",
        "\n",
        "LoRA typically wraps attention projection matrices, which will detail in future weeks (LLM parameters `W_q`, `W_k`, `W_v`, or `W_out`) with low-rank decomposition. QLoRA applies 4-bit quantization on those same weight matrices, but leaves LoRA adapters in FP16 or BF16."
      ],
      "metadata": {
        "id": "G1BiiEp76nZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# Important for causal LM tasks; we want to pad on left if doing batch inference, etc.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# If we do standard full-precision load:\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",  # or just \"cpu\" or \"cuda:0\" depending on environment\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "\n",
        "# If we want to do a 4-bit load using bitsandbytes:\n",
        "# base_model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "#     MODEL_NAME,\n",
        "#     load_in_4bit=True,\n",
        "#     device_map=\"auto\",\n",
        "#     quantization_config=bnb.QuantizationConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_compute_dtype=torch.float16,\n",
        "#         bnb_4bit_use_double_quant=True,\n",
        "#         bnb_4bit_quant_type='nf4'\n",
        "#     )\n",
        "# )\n",
        "\n",
        "print(\"Base model loaded!\")"
      ],
      "metadata": {
        "id": "QswXFnfk6AWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Inject LoRA Adapters\n",
        "We can wrap the above model with [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft) or with the original LoRA code.\n",
        "\n",
        "Below is a **PEFT**-style example (huggingface/peft) for simplicity. If you want the original LoRA or QLoRA repos, see their `train.py` scripts. The logic is mostly the same."
      ],
      "metadata": {
        "id": "UqMjR9BaBYm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model_lora = get_peft_model(base_model, lora_config)\n",
        "print(f\"LoRA Model params: {model_lora.print_trainable_parameters()}\")"
      ],
      "metadata": {
        "id": "5luOLp978ziQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QLoRA**:\n",
        "If using QLoRA with bitsandbytes 4-bit quantization, we could do something like:"
      ],
      "metadata": {
        "id": "rwcq-e9tBvM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"query_key_value\"],\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM\n",
        "# )\n",
        "\n",
        "# # base_model_4bit is the quantized model\n",
        "# model_lora_4bit = get_peft_model(base_model_4bit, lora_config)\n",
        "# print(model_lora_4bit.print_trainable_parameters())\n",
        "\n",
        "# Then proceed with training, but the base weights remain 4-bit in memory, and only the LoRA adapter is stored in full precision.\n"
      ],
      "metadata": {
        "id": "zuYC8z0gB6iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare a Dataset\n",
        "For a small demonstration, let's create a toy text dataset or use something from Hugging Face `datasets`."
      ],
      "metadata": {
        "id": "YUk6qFw-BLWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # small example\n",
        "print(raw_dataset)\n",
        "\n",
        "# Let's define a data collator for causal LM\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def tokenize_function(example):\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model_lora.resize_token_embeddings(len(tokenizer))\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # for causal LM\n",
        ")\n",
        "\n",
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "val_dataset   = tokenized_dataset[\"validation\"]\n",
        "test_dataset  = tokenized_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "6i_P02bn9fVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning with LoRA or QLoRA\n",
        "\n",
        "We'll use the Hugging Face `Trainer` to fine-tune.\n",
        "# Training Arguments\n",
        "Key parameters to watch for:\n",
        "- `learning_rate` — might be smaller for LLM fine-tuning (e.g., 1e-4, 2e-5).\n",
        "- `per_device_train_batch_size`, `per_device_eval_batch_size`.\n",
        "- `gradient_accumulation_steps` (especially for bigger LLMs).\n",
        "- `max_steps` or `num_train_epochs`.\n",
        "- `fp16` or `bf16` if available (and stable).\n",
        "\n",
        "If QLoRA, ensure `bitsandbytes` is installed and your GPU supports 4-bit."
      ],
      "metadata": {
        "id": "8DVd5yiyCMsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"lora-finetune-checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=50,\n",
        "    fp16=True,            # Enable mixed precision training if possible\n",
        "    report_to=\"none\",     # or \"wandb\", \"tensorboard\", etc.\n",
        "    max_grad_norm=1.0 # Clip gradients to prevent exploding gradients\n",
        ")\n",
        "\n",
        "# Set up the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_lora,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "XFZo0DTdCfEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Done Training LoRA model.\")"
      ],
      "metadata": {
        "id": "L3eC7_MBCnky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate & Compare\n",
        "Let's do a quick perplexity check on the validation (or test) set. We'll reuse the Trainer’s `evaluate()` method, which will compute the causal LM loss. Then perplexity = `exp(loss)`. If the perplexity is high the data is improbable; if the perplexity is low, then the data is expected and our models works well.\n"
      ],
      "metadata": {
        "id": "u9JMHnmbCtTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "val_loss = eval_results[\"eval_loss\"]\n",
        "val_ppl = np.exp(val_loss)\n",
        "print(f\"Validation Perplexity: {val_ppl:.2f}\")"
      ],
      "metadata": {
        "id": "rIG2fY7ACwnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework Assignments\n",
        "\n",
        "1. **Adapt an LLM** (e.g., Galactical used above) with LoRA to your text and evaluate its perplexity before and after adaptation.\n",
        "\n",
        "2. **QLoRA**: Load a 4-bit quantized model (via bitsandbytes), apply LoRA, and fine-tune. Compare GPU memory usage, speed, and resulting perplexity with the standard LoRA approach above.\n",
        "\n",
        "3. **Hyperparameter Tuning**: Adjust `r`, `lora_alpha`, `lora_dropout`, or the learning rate.\n",
        "\n",
        "4. **Regularization**: Introduce weight decay, dropout in LoRA layers, or regularization of adapter weights."
      ],
      "metadata": {
        "id": "LOIIBTHiuOS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yV8AmTDai6wc"
      }
    }
  ]
}